CloudFormation

creating an S3 bucket with versioning , copy html folder from devops essentials to bucket after modifying permissions of bucket.

mark everything as public  then into ->  CodePipeline going by stages{name,source,build,deploy,service role,review}

source = S3,GitHub,CodeCommit
build = CodeBuild
Deploy = CloudFormation,CodeDeploy,Elastic Bean Stalk,OpsWorks
Invokde = Lambda

add approval stage in codepipeline  & try disabling transition to block pipeline   devopsessentialsaws  

json2yaml site 

create codecommit  unique for region   

######################################################################################################################
53 is the TCP/UDP port where DNS services are running

buy your domain after checking the availabilty - partnered with Gandy - need to map the public ip of EC2 running web browser to domain -
choose hosted zones from dashboard - create a record set - type A (ipaddress of EC2 as value and TTL 300s) - click create 

But additionally it provides LB kind of features in the name of health checks

Amazon Route 53 allows DNS failover with active-active, active-passive, and mixed configurations to implement high availability of 
the applications. For instance, if there are many resources that perform the same function, then Route 53 can check the state (health)
of those resources and respond to DNS queries only the resources that are healthy.



Each EC2 instance has an elastic IP so that, after a stop/start of the instance, we will receive the same public IP address.

choose health checks - 
In the next window, you will need to fill in some details:

– Name—This is something that should be descriptive to you. You will reference this later.

– Protocol—This will be the protocol used for testing. Because we need to test if the HTTP server is up, we will use HTTP.

– IP Address—The IP address of the HTTP server.

– Port—The port used for availability checking.

– Request Interval—How often the health will be checked.

– Failure Threshold—After how many failures the health check will be considered as failure.

As you can see, we used pretty aggressive intervals/thresholds because we need to detect a possible failure as soon as possible.

Click on “Create” to create the health check.

Select “Hosted Zones” and then “Go to Record Sets”: Click on “Create Record Set” to create the first record set. The record set will be for www.vtep.net:


Everything is the same as explained in Part I of the series. In the value field, you will need to put the IP address of EC2 instance 
from US EAST region. The interesting stuff starts after this.

In the “Routing Policy” choose “Failover” and in the “Failure Record Type” choose “Primary” because this will be the primary server. 
On “Set ID,” fill in something that is meaningful for you. Further we need to Route 53 how to track the availability of this server.
Choose “Yes” from “Associate with Health Check” and then, from the drop down menu, we will choose the health check that we configured 
earlier. Then click on “Create” to create the record set.

Then we need to create another record set with the same name, www.vtep.net. Fill in the IP address of the EC2 instance from 
US EAST instance. In the “Routing Policy” choose “Failover” and in the “Failure Record Type” choose “Secondary” because this 
will be the secondary server. We don’t need to specify any health check because we don’t have a server that will be backup 
to this one. In case we would have, we need to configure the health check and then reference it. Click on “Create” to continue.



The health checks done by Route 53 are:

HTTP and HTTPS health checks—Route 53 must establish a TCP connection in less than four seconds. The endpoint should respond with a status code higher than 200 (including), but lower than 400 within 2 seconds.
TCP health checks—Route 53 must establish a TCP connection in less than 4 seconds

So we will start by looking at the latency routing. I have one EC2 instance running in EU CENTRAL (Frankfurt) region and one EC2 
instance in US EAST (N. Virginia). Each EC2 instance is running a web server and each one of them is returning a customized output 
to identify in which region is the EC2 instance running.

What we will demonstrate first is that we can route the user based on latency. For instance, we will see that users from Europe will
be served the webpage from the EC2 instance running in the EU CENTRAL region and the users from US will be served the webpage from 
the EC2 instance running in the US EAST region.

Let’s test it. I configured a proxy on my browser with an IP address from Switzerland. Obviously, Switzerland is closer to 
Frankfurt than to North Virginia:

It is important to understand that once you create the public hosted zone, a name server (NS) record and a start of authority (SOA) 
record are automatically created. The NS record is important here. It provides you with four name servers that you need to configure
with your registrar or DNS services, so all the queries related to your domain are routed to Amazon Route 53 name servers for 
resolution.

If we purchase a domain from Route 53, then the hosted zone is created automatically and we don't need to create one.
###############################################################################################################

AWS CloudWatch and CloudTrail: How do they work together?
AWS CloudWatch is integrated with a whole host of AWS services, including CloudTrail. Using CloudTrail’s ability to log a change to 
a resource, a CloudWatch Event rule can be created to recognize this and then trigger a Lambda function to open a ticket for 
investigation.


Classic Load balancer are used in times when there is simple traffic distribution between EC2 Instances

Application Load Balancer is used when there are container based or microservices based architecture which need to route the traffic
to different services or balance the traffic across different port on same EC2 Instance.





For serverless sundog uploaded to S3

 I was able to solve a concurrency problem when updating a file on S3. I am using lambda function to update a file on S3 using
 concurrency count as 1. This ensures that at a time only one ec2 instance has write access to the file.This is much cheaper 
 then using any managed DB on AWS for trivial usage

######################################################################################################################

CloudFormation   
--------------------------------
this is executed from console   programming the GUI console 

use update stack options after editting


{} encloses object   
{
	"name" : "siva",
	"salary" : "200000",
	"isManager" : true,
	"skills" : ["AWS","Python","GCP"],
	"address" : {"country" : "india" ,"state" : "karnataka"}  # object in object	
}


extension can be .json/.yaml

start in atom gives template  - basic skelton remains same

"Type" : "AWS::EC2::VPC"

Resources  logical-name cidr

==> console - CF - crete a new stack - choose file - give name to your stack - go with remaining defaults

Parameters : logical name descriptin type ( string) default value   allowed-values(t2-micro,m1.large)  NoEcho : true means passwords not shown   MinLength and MaxLength & add a resource as well ( like a VPC for parameters)   like-wise we can get AZs as parameters or anything else like volumes 

parameterizing CF : Cidr block is hard coded hence will parameterise it.Type: String Default: "10.23.0.0/16"===> then call it(~interpolation) using "Ref" : logical name

CF intrinsic functions (like Fn::split): adding 1 subnet using ref for VPC    

adding IGw & attaching to VPC again use ref & logical name of VPC to attach   then creating RT & attaching to this IGw (RT is a component of VPC) requires Gw id  then
associate subnets to newly created RT 

adding userdata : Fn::Base64 for encoding )  & Fn::Join    {scripts under user data are run as root}

metadata { cloudformation::init   -> gives better template structure for user addition,config files or commannds or services management 
findinmap  to define a map of AMIs as per regions   ELBs are PaaS therefore just http, no ssh 
in route53 ending dot is required   CFoutputs   outputs can be seen in tab of CF window

Cross stack access  as network  db  vpc subnets  & connect them using export and output  importvalue   Fn::Sub  

CF conditions    deploy zipped lambda fn from S3 bucket 

#######################################################################################################################

Invoke another lambda from lambda 

from boto3 import client as boto3_client
from datetime import datetime
import json

lambda_client = boto3_client('lambda')

def lambda_handler(event, context):
    msg = {"key":"new_invocation", "at": datetime.now()}
    invoke_response = lambda_client.invoke(FunctionName="another_lambda_",
                                           InvocationType='Event',
                                           Payload=json.dumps(msg))
    print(invoke_response)
Btw, you would need to add a policy like this to your lambda role as well

   {
        "Sid": "Stmt1234567890",
        "Effect": "Allow",
        "Action": [
            "lambda:InvokeFunction"
        ],
        "Resource": "*"
    }
    
    
    #########################################################################################
    
    To use requests module, you can simply import requests from botocore.vendored. For example:

from botocore.vendored import requests

def lambda_handler(event, context):
   response = requests.get("https://example.com/")
   print response.json()
   
   ########################################################################################################################
   For lambda zip
    The structure in my zip file was broken. It is important that the python script and the packed dependencies (as folders) are 
    in the root of the zip file. This solved my problem.
    
    
97

For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions.

{
    "Statement": [
        {
            "Action": [
                "logs:CreateLogGroup",
                 "logs:CreateLogStream",
                 "logs:PutLogEvents"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
} 

After you update your policy, it seems that you have to update your function's settings to refresh all job instances to read new policies.

So if you just click 'test' button from Lambda console after you update your role policy in IAM, the cached Lambda instances will still have old role permissions, so you will still see no logs being written to Cloudwatch logs.

Just change your timeout by a second and click on 'save and test' button, and you will start to see logs in Cloudwatch.
#############################################################################################################################

read will return bytes. At least for Python 3, if you want to return a string, you have to decode using the right encoding:

import boto3

s3 = boto3.resource('s3')

obj = s3.Object(bucket, key)
obj.get()['Body'].read().decode('utf-8') 

###########################################################################################################################

import boto3
import json

s3 = boto3.client('s3')

obj = s3.get_object(Bucket=bucket, Key=key)
j = json.loads(obj['Body'].read())
NOTE (for python 2.7): My object is all ascii, so I don't need .decode('utf-8')

NOTE (for python 3.6+): We moved to python 3.6 and discovered that read() now returns bytes so if you want to get a string out of it, you must use:

j = json.loads(obj['Body'].read().decode('utf-8'))

#################################################################################################################################

I just wanted to update the database from my lambda function. Is it possible to access RDS by specifiying IAM Role and access Policy?.

No you cannot. You need to provide DB url/username/password to connect. You may need to run Lambda in same VPC if it is in private subnet. See my pointers below.

I can connect to mysql databse using mysql client.but when i try on lambda i can't do that.

This is strict No , No! Your RDS should not be accessible from Internet unless you really need it. Try to run it in private subnet and configure other AWS services accordingly.

Two cents from my end if you are getting timeouts accessing resourced from Lambda-

By default Lambda has internet access and can access online resources.
Lambda cannot access services rurnning in private subnet of your VPC.
To connect to services in private subnet you need to run the lambda is private subnet. For this you need to go to Network section and configure your VPC, subnets and security group.
However note that when you do this you will loose Internet access. If you still need Internet access you will have to spin up a NAT gateway or NAT instance in public subnet and configure route from private subnet to this NAT.
I faced this when I was trying to connect to RDS in private subnet from my lambda. Since I used KMS to encrypt some environment variables and decryption part requires Internet access I had to use a NAT gateway.
More details - http://docs.aws.amazon.com/lambda/latest/dg/vpc.html#vpc-internet

How to connect to postgres RDS from AWS Lambda

#####################################################################################################################################
Basically, domain queries are automatically routed to the nearest DNS server to provide the quickest response possible. If you use a web hosting company like GoDaddy, it takes 30 minutes to 24 hours to remap a domain to a different IP, but by using Route 53 in AWS it takes only a few minutes.

Now, take a look at the benefits provided by Route 53.

Amazon Route 53 Benefits
Route 53 provides the user with several benefits.

They are:

Highly Available and Reliable
Flexible
Simple
Fast
Cost-effective
Designed to Integrate with Other AWS Services
Secure
Scalable

##########################################################################################################################################

import requests

print('Loading function')

s3 = boto3.client('s3')


def lambda_handler(event, context):
    #print("Received event: " + json.dumps(event, indent=2))

    # Get the object from the event and show its content type
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.unquote_plus(event['Records'][0]['s3']['object']['key']).decode('utf8')
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        s3.download_file(bucket,key, '/tmp/data.txt')
        lines = [line.rstrip('\n') for line in open('/tmp/data.txt')]
        for line in lines:
            col=line.split(',')
            print(col[5],col[6])
        print("CONTENT TYPE: " + response['ContentType'])
        return response['ContentType']
    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
        raise e
#############################################################################################################################

Amazon Lambda is designed to be used as an event-driven system that responds to events. The flow is:

Something happens somewhere that triggers Lambda (eg an upload to Amazon S3, data coming into an Amazon Kinesis stream, an application invoking the Lambda function directly)
The Lambda function is created, data from the trigger event is passed
The Lambda function runs
Lambda functions are limited to a maximum execution time of 15 minutes (this was recently increased from the original 5 minutes timeout). The actual limit is configured when the Lambda function is created. The limit is in place because Lambda functions are meant to be small and quick rather than being large applications.

Your error message says Task timed out after 15.00 seconds. This means that AWS intentionally stopped the task once it hit a run-time of 15 seconds. It has nothing to do with what the function was doing at the time, nor the file that was being processed.

To fix: Increase the timeout setting on the configuration page of your Lambda function.

###########################################################################################################################

My Lambda function runs every 15 minutes, it is triggered by Jenkins.My program gets the source code of the URL into the string,
then compares it with already existing file in /tmp directory (the source code from previous run), and if something has changed, 
uploads it to S3 bucket. So /tmp stores the latest source code file for comparison.


with NACL you may DENY specific IP which cant be done at SG level.

Reverse charging for EIPs    

You can use managed NAT GW from AWS or turn your EC2 into a NAT

NAT must be in public subnet while private subnet must have internet route via NAT   NAT must have EIP ( not under free tier)

by default EC2 will accept traffic only if destination is itself  but for NAT it has to accept everything hence disable src/dest check

choose a custom NAT ami from market place   in actions networking disable src/dest check

VPC peering through private IPs   no pulblic address involved    not transitive   a-b & b-c != a-c
private ec2s in Mumbai & NVirginia to communicate through private addresses

for on prem    VPGw - VPN tunnel - CustomerGW. One VPG can connect to multiple CGW  --> This is S2S over internet 

but you dont want tunnel over internet then direct-connect - dedicated nw   you might also go for brokers(middle amn) who already have 
lines and ready to lease

EC2 needs to connect to S3  you go for public internet & get charged but if you have both of them in same region  you can use 
VPC endpoint ie private connection.  ec2 - vpc endpoint - S3    OR ec2 - vpcept - dynamoDB

VPC flow log ~ wireshark  

1 central management VPC that'd connect to all the other VPCs   Chef,puppet masters be in central VPC

VPC endpoint for private subnet ec2 to connect to public bucket

####################################################################################################################

ec2 = boto3.client('ec2')


def start_ec2(event, context):
    ec2_instances = get_all_ec2_ids()
    response = ec2.start_instances(
        InstanceIds=ec2_instances,
        DryRun=False
    )
    return response


def stop_ec2(event, context):
    ec2_instances = get_all_ec2_ids()
    response = ec2.stop_instances(
        InstanceIds=ec2_instances,
        DryRun=False
    )
    return response


# get the list of all the ec2 instances
def get_all_ec2_ids():
    response = ec2.describe_instances(DryRun=False)
    instances = []
    for reservation in response["Reservations"]:
        for instance in reservation["Instances"]:
            # This sample print will output entire Dictionary object
            # This will print will output the value of the Dictionary key 'InstanceId'
            instances.append(instance["InstanceId"])
    return instances


####################################################################################################################

import boto3
import cStringIO
from PIL import Image, ImageOps
import os

s3 = boto3.client('s3')
size = int(os.environ['THUMBNAIL_SIZE'])


def s3_thumbnail_generator(event, context):
    # parse event
    print(event)
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    # only create a thumbnail on non thumbnail pictures
    if (not key.endswith("_thumbnail.png")):
        # get the image
        image = get_s3_image(bucket, key)
        # resize the image
        thumbnail = image_to_thumbnail(image)
        # get the new filename
        thumbnail_key = new_filename(key)
        # upload the file
        url = upload_to_s3(bucket, thumbnail_key, thumbnail)
        return url


def get_s3_image(bucket, key):
    response = s3.get_object(Bucket=bucket, Key=key)
    imagecontent = response['Body'].read()

    file = cStringIO.StringIO(imagecontent)
    img = Image.open(file)
    return img


def image_to_thumbnail(image):
    return ImageOps.fit(image, (size, size), Image.ANTIALIAS)


def new_filename(key):
    key_split = key.rsplit('.', 1)
    return key_split[0] + "_thumbnail.png"


def upload_to_s3(bucket, key, image):
    # We're saving the image into a cStringIO object to avoid writing to disk
    out_thumbnail = cStringIO.StringIO()
    # You MUST specify the file type because there is no file name to discern
    # it from
    image.save(out_thumbnail, 'PNG')
    out_thumbnail.seek(0)

    response = s3.put_object(
        ACL='public-read',
        Body=out_thumbnail,
        Bucket=bucket,
        ContentType='image/png',
        Key=key
    )
    print(response)

    url = '{}/{}/{}'.format(s3.meta.endpoint_url, bucket, key)
    return url

##########################################################################################################

this is one of the things I'd really like to see Lambda support, but currently it does not. One of the problems is that if there
were a lot of S3 PUT operations happening AWS would have to queue up all the Lambda invocations somehow, and there is currently 
no support for that.

If you built a locking mechanism into your Lambda function, what would you do with the requests you don't process due to a lock? 
Would you just throw those S3 notifications away?

The solution most people recommend is to have S3 send the notifications to an SQS queue, and then have your Lambda function scheduled 
to run periodically, like once a minute, and check if there is an item in the queue that needs to be processed.

Alternatively, have S3 send the notifications to SQS and just have a t2.nano EC2 instance with a single-threaded service polling
the queue.

Using messaging to serialize lambda executionie ie stop parallel executions
Have the S3 "Put events" cause a message to be placed on the queue (instead of involving a lambda function). The message should
contain a reference to the S3 object. Then SCHEDULE a lambda to "SHORT POLL the entire queue".




You can use an aws_cloudwatch_event_target resource to tie the scheduled event source (event rule) to your lambda function. You need to grant it permission to invoke your lambda function; you can use an aws_lambda_permission resource for this.

Example:

resource "aws_lambda_function" "check_foo" {
    filename = "check_foo.zip"
    function_name = "checkFoo"
    role = "arn:aws:iam::424242:role/something"
    handler = "index.handler"
}

resource "aws_cloudwatch_event_rule" "every_five_minutes" {
    name = "every-five-minutes"
    description = "Fires every five minutes"
    schedule_expression = "rate(5 minutes)"
}

resource "aws_cloudwatch_event_target" "check_foo_every_five_minutes" {
    rule = "${aws_cloudwatch_event_rule.every_five_minutes.name}"
    target_id = "check_foo"
    arn = "${aws_lambda_function.check_foo.arn}"
}

resource "aws_lambda_permission" "allow_cloudwatch_to_call_check_foo" {
    statement_id = "AllowExecutionFromCloudWatch"
    action = "lambda:InvokeFunction"
    function_name = "${aws_lambda_function.check_foo.function_name}"
    principal = "events.amazonaws.com"
    source_arn = "${aws_cloudwatch_event_rule.every_five_minutes.arn}"
}


##########################################################################################################################

AWS Lambda environment variables can be defined using the AWS Console, CLI, or SDKs. This is how you would define an AWS Lambda that uses an LD_LIBRARY_PATH environment variable using AWS CLI:

aws lambda create-function \
  --region us-east-1
  --function-name myTestFunction
  --zip-file fileb://path/package.zip
  --role role-arn
  --environment Variables={LD_LIBRARY_PATH=/usr/bin/test/lib64}
  --handler index.handler
  --runtime nodejs4.3
  --profile default
Once created, environment variables can be read using the support your language provides for accessing the environment, e.g. using process.env for Node.js. When using Python, you would need to import the os library, like in the following example:

...
import os
...
print("environment variable: " + os.environ['variable'])

#########################################################################################################################

def lambda_handler(event, context):
    # TODO implement
    import boto3

    s3 = boto3.client('s3')
    data = s3.get_object(Bucket='my_s3_bucket', Key='main.txt')
    contents = data['Body'].read()
    print(contents)
    
 ###################################################################################################################
 
 Amazon SNS is designed to distribute notifications. These can be received in a variety of formats, such as email, SMS, messages pushed
 to HTTP endpoints, mobile phone notifications and even triggering of AWS Lambda functions.

It is not designed as a fully-featured email system. It will only send text messages and appends an 'unsubscribe' footer at the 
bottom of the messages.

If you wish to send formatted emails, consider using Amazon Simple Email Service (SES), which improves email deliverability. 
Any content passed into Amazon SES is sent out to recipients, including HTML.

Amazon SNS is primarily about notification, rather than pretty content.

###############################################################################################################################
The AWS API Gateway is the only way to expose your lambda function over HTTP. The AWS lambda web console should create one
automatically for you if you use the microservice-http-endpoint blueprint when creating a new lambda function.

import boto3

def lambda_handler(event, context):

    client = boto3.client('dynamodb')

    for record in event['Records']:
        # your logic here...
        try:
            client.update_item(TableName='dynamo_table_name', Key={'hash_key':{'N':'value'}}, AttributeUpdates={"some_key":{"Action":"PUT","Value":{"N":'value'}}}) 
        except Exception, e:
            print (e)

#############################################################################################################################

Recently switched from GoDaddy (was a client for almost 5 years of their DNS and domain registration services) to Route 53.

Why GoDaddy was better for me than Route53:

they do not charge you for DNS queries;
they provide a free mail forwarding service.
That's it.

Now why Route53 is better than GoDaddy:

a better UI (GoDaddy's UI is really confusing);
API;
some domain zones are cheaper (e.g. "io" domain will cost you twice less per year);
provides a whois domain privacy for free (GoDaddy charges for that additionally).


Besides what's already been said about the quality of Amazon's infrastructure, the API is the killer feature of Route 53 
or competitors like Dynect. If your site does get large enough that you have a number of servers, you'll want to get into 
systems automation, and being able to automate your DNS changes can be quite nice.
################################################################################################################################

client = boto3.client('ec2')

resp = client.run_instances(ImageId='ami-467ca739',
                     InstanceType='t2.micro',
                     MinCount=1,
                     MaxCount=1)
for instance in resp['Instances']:
    print(instance['InstanceId'])
    
    
    
client = boto3.client('s3')
response = client.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])
    
    
client = boto3.client('s3')
response = client.delete_object(
    Bucket='123',
    Key='create_bucket.py'
)



file_reader = open('create_bucket.py').read()
response = client.put_object(
    ACL='private',
    Body=file_reader,
    Bucket='123',
    Key='create_bucket.py'
)


response = client.create_bucket(
    ACL='private',
    Bucket='javahomecloud123',
    CreateBucketConfiguration={
        'LocationConstraint': 'ap-south-1'
    }
)


dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('employees')

with table.batch_writer() as batch:
    for x in range(100):
        batch.put_item(
            Item={
                'emp_id': str(x),
                'name': 'Name-{}'.format(x)
            }
        )



dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('employees')
resp = table.get_item(
    Key={
        'emp_id': '2'
    }
)

print(resp['Item'])

table.delete_item(
    Key={
        'emp_id': '2'
    }
)



table.put_item(
    Item={
        'emp_id': '2',
        'name': 'kammana',
        'salary': 20000
    }
)



##########################
## Part-1 Create Images ##
##########################

source_region = 'ap-south-1'
ec2 = boto3.resource('ec2', region_name=source_region)


instances = ec2.instances.filter(InstanceIds=['i-0067eeaab6c8188fd'])

image_ids = []

for instance in instances:
    image = instance.create_image(Name='Demo Boto - '+instance.id, Description='Demo Boto'+instance.id)
    image_ids.append(image.id)

print("Images to be copied {} ".format(image_ids))


#############################################
## Part-2 Wait For Images to be available  ##
#############################################
# Get waiter for image_available

client = boto3.client('ec2', region_name=source_region)
waiter = client.get_waiter('image_available')

# Wait for Images to be ready
waiter.wait(Filters=[{
    'Name': 'image-id',
    'Values': image_ids
}])

##########################################
## Part-3 Copy Images to other regions  ##
##########################################

# Copy Images to the region, us-east-1

destination_region = 'us-east-1'
client = boto3.client('ec2', region_name=destination_region)
for image_id in image_ids:
    client.copy_image(Name='Boto3 Copy'+image_id, SourceImageId=image_id, SourceRegion='ap-south-1')




from datetime import datetime, timedelta, timezone

import boto3
ec2 = boto3.resource('ec2')

# List(ec2.Snapshot)
snapshots = ec2.snapshots.filter(OwnerIds=['self'])

for snapshot in snapshots:
    start_time = snapshot.start_time
    delete_time = datetime.now(tz=timezone.utc) - timedelta(days=15)
    if delete_time > start_time:
        snapshot.delete()
        print('Snapshot with Id = {} is deleted '.format(snapshot.snapshot_id))





client = boto3.client('ec2')

resp = client.describe_instances(Filters=[{
    'Name': 'tag:Env',
    'Values': ['Prod']
}])

for reservation in resp['Reservations']:
    for instance in reservation['Instances']:
        print("InstanceId is {} ".format(instance['InstanceId']))
	
	











