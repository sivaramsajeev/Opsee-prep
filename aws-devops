CloudFormation

creating an S3 bucket with versioning , copy html folder from devops essentials to bucket after modifying permissions of bucket.

mark everything as public  then into ->  CodePipeline going by stages{name,source,build,deploy,service role,review}

source = S3,GitHub,CodeCommit
build = CodeBuild
Deploy = CloudFormation,CodeDeploy,Elastic Bean Stalk,OpsWorks
Invokde = Lambda

add approval stage in codepipeline  & try disabling transition to block pipeline   devopsessentialsaws  

json2yaml site 

create codecommit  unique for region   

######################################################################################################################

For serverless sundog uploaded to S3

 I was able to solve a concurrency problem when updating a file on S3. I am using lambda function to update a file on S3 using
 concurrency count as 1. This ensures that at a time only one ec2 instance has write access to the file.This is much cheaper 
 then using any managed DB on AWS for trivial usage

######################################################################################################################

CloudFormation   
--------------------------------
this is executed from console   programming the GUI console 

use update stack options after editting


{} encloses object   
{
	"name" : "siva",
	"salary" : "200000",
	"isManager" : true,
	"skills" : ["AWS","Python","GCP"],
	"address" : {"country" : "india" ,"state" : "karnataka"}  # object in object	
}


extension can be .json/.yaml

start in atom gives template  - basic skelton remains same

"Type" : "AWS::EC2::VPC"

Resources  logical-name cidr

==> console - CF - crete a new stack - choose file - give name to your stack - go with remaining defaults

Parameters : logical name descriptin type ( string) default value   allowed-values(t2-micro,m1.large)  NoEcho : true means passwords not shown   MinLength and MaxLength & add a resource as well ( like a VPC for parameters)   like-wise we can get AZs as parameters or anything else like volumes 

parameterizing CF : Cidr block is hard coded hence will parameterise it.Type: String Default: "10.23.0.0/16"===> then call it(~interpolation) using "Ref" : logical name

CF intrinsic functions (like Fn::split): adding 1 subnet using ref for VPC    

adding IGw & attaching to VPC again use ref & logical name of VPC to attach   then creating RT & attaching to this IGw (RT is a component of VPC) requires Gw id  then
associate subnets to newly created RT 

adding userdata : Fn::Base64 for encoding )  & Fn::Join    {scripts under user data are run as root}

metadata { cloudformation::init   -> gives better template structure for user addition,config files or commannds or services management 
findinmap  to define a map of AMIs as per regions   ELBs are PaaS therefore just http, no ssh 
in route53 ending dot is required   CFoutputs   outputs can be seen in tab of CF window

Cross stack access  as network  db  vpc subnets  & connect them using export and output  importvalue   Fn::Sub  

CF conditions    deploy zipped lambda fn from S3 bucket 

#######################################################################################################################

Invoke another lambda from lambda 

from boto3 import client as boto3_client
from datetime import datetime
import json

lambda_client = boto3_client('lambda')

def lambda_handler(event, context):
    msg = {"key":"new_invocation", "at": datetime.now()}
    invoke_response = lambda_client.invoke(FunctionName="another_lambda_",
                                           InvocationType='Event',
                                           Payload=json.dumps(msg))
    print(invoke_response)
Btw, you would need to add a policy like this to your lambda role as well

   {
        "Sid": "Stmt1234567890",
        "Effect": "Allow",
        "Action": [
            "lambda:InvokeFunction"
        ],
        "Resource": "*"
    }
    
    
    #########################################################################################
    
    To use requests module, you can simply import requests from botocore.vendored. For example:

from botocore.vendored import requests

def lambda_handler(event, context):
   response = requests.get("https://example.com/")
   print response.json()
   
   ########################################################################################################################
   For lambda zip
    The structure in my zip file was broken. It is important that the python script and the packed dependencies (as folders) are 
    in the root of the zip file. This solved my problem.
    
    
97

For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions.

{
    "Statement": [
        {
            "Action": [
                "logs:CreateLogGroup",
                 "logs:CreateLogStream",
                 "logs:PutLogEvents"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
} 

After you update your policy, it seems that you have to update your function's settings to refresh all job instances to read new policies.

So if you just click 'test' button from Lambda console after you update your role policy in IAM, the cached Lambda instances will still have old role permissions, so you will still see no logs being written to Cloudwatch logs.

Just change your timeout by a second and click on 'save and test' button, and you will start to see logs in Cloudwatch.
#############################################################################################################################

read will return bytes. At least for Python 3, if you want to return a string, you have to decode using the right encoding:

import boto3

s3 = boto3.resource('s3')

obj = s3.Object(bucket, key)
obj.get()['Body'].read().decode('utf-8') 

###########################################################################################################################

import boto3
import json

s3 = boto3.client('s3')

obj = s3.get_object(Bucket=bucket, Key=key)
j = json.loads(obj['Body'].read())
NOTE (for python 2.7): My object is all ascii, so I don't need .decode('utf-8')

NOTE (for python 3.6+): We moved to python 3.6 and discovered that read() now returns bytes so if you want to get a string out of it, you must use:

j = json.loads(obj['Body'].read().decode('utf-8'))

#################################################################################################################################

I just wanted to update the database from my lambda function. Is it possible to access RDS by specifiying IAM Role and access Policy?.

No you cannot. You need to provide DB url/username/password to connect. You may need to run Lambda in same VPC if it is in private subnet. See my pointers below.

I can connect to mysql databse using mysql client.but when i try on lambda i can't do that.

This is strict No , No! Your RDS should not be accessible from Internet unless you really need it. Try to run it in private subnet and configure other AWS services accordingly.

Two cents from my end if you are getting timeouts accessing resourced from Lambda-

By default Lambda has internet access and can access online resources.
Lambda cannot access services rurnning in private subnet of your VPC.
To connect to services in private subnet you need to run the lambda is private subnet. For this you need to go to Network section and configure your VPC, subnets and security group.
However note that when you do this you will loose Internet access. If you still need Internet access you will have to spin up a NAT gateway or NAT instance in public subnet and configure route from private subnet to this NAT.
I faced this when I was trying to connect to RDS in private subnet from my lambda. Since I used KMS to encrypt some environment variables and decryption part requires Internet access I had to use a NAT gateway.
More details - http://docs.aws.amazon.com/lambda/latest/dg/vpc.html#vpc-internet

How to connect to postgres RDS from AWS Lambda

#####################################################################################################################################
Basically, domain queries are automatically routed to the nearest DNS server to provide the quickest response possible. If you use a web hosting company like GoDaddy, it takes 30 minutes to 24 hours to remap a domain to a different IP, but by using Route 53 in AWS it takes only a few minutes.

Now, take a look at the benefits provided by Route 53.

Amazon Route 53 Benefits
Route 53 provides the user with several benefits.

They are:

Highly Available and Reliable
Flexible
Simple
Fast
Cost-effective
Designed to Integrate with Other AWS Services
Secure
Scalable

##########################################################################################################################################

import requests

print('Loading function')

s3 = boto3.client('s3')


def lambda_handler(event, context):
    #print("Received event: " + json.dumps(event, indent=2))

    # Get the object from the event and show its content type
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.unquote_plus(event['Records'][0]['s3']['object']['key']).decode('utf8')
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        s3.download_file(bucket,key, '/tmp/data.txt')
        lines = [line.rstrip('\n') for line in open('/tmp/data.txt')]
        for line in lines:
            col=line.split(',')
            print(col[5],col[6])
        print("CONTENT TYPE: " + response['ContentType'])
        return response['ContentType']
    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
        raise e
#############################################################################################################################

Amazon Lambda is designed to be used as an event-driven system that responds to events. The flow is:

Something happens somewhere that triggers Lambda (eg an upload to Amazon S3, data coming into an Amazon Kinesis stream, an application invoking the Lambda function directly)
The Lambda function is created, data from the trigger event is passed
The Lambda function runs
Lambda functions are limited to a maximum execution time of 15 minutes (this was recently increased from the original 5 minutes timeout). The actual limit is configured when the Lambda function is created. The limit is in place because Lambda functions are meant to be small and quick rather than being large applications.

Your error message says Task timed out after 15.00 seconds. This means that AWS intentionally stopped the task once it hit a run-time of 15 seconds. It has nothing to do with what the function was doing at the time, nor the file that was being processed.

To fix: Increase the timeout setting on the configuration page of your Lambda function.

###########################################################################################################################

My Lambda function runs every 15 minutes, it is triggered by Jenkins.My program gets the source code of the URL into the string,
then compares it with already existing file in /tmp directory (the source code from previous run), and if something has changed, 
uploads it to S3 bucket. So /tmp stores the latest source code file for comparison.


with NACL you may DENY specific IP which cant be done at SG level.

Reverse charging for EIPs    

You can use managed NAT GW from AWS or turn your EC2 into a NAT

NAT must be in public subnet while private subnet must have internet route via NAT   NAT must have EIP ( not under free tier)

by default EC2 will accept traffic only if destination is itself  but for NAT it has to accept everything hence disable src/dest check

choose a custom NAT ami from market place   in actions networking disable src/dest check

VPC peering through private IPs   no pulblic address involved    not transitive   a-b & b-c != a-c
private ec2s in Mumbai & NVirginia to communicate through private addresses

for on prem    VPGw - VPN tunnel - CustomerGW. One VPG can connect to multiple CGW  --> This is S2S over internet 

but you dont want tunnel over internet then direct-connect - dedicated nw   you might also go for brokers(middle amn) who already have 
lines and ready to lease

EC2 needs to connect to S3  you go for public internet & get charged but if you have both of them in same region  you can use 
VPC endpoint ie private connection.  ec2 - vpc endpoint - S3    OR ec2 - vpcept - dynamoDB

VPC flow log ~ wireshark  

1 central management VPC that'd connect to all the other VPCs   Chef,puppet masters be in central VPC

VPC endpoint for private subnet ec2 to connect to public bucket

####################################################################################################################

ec2 = boto3.client('ec2')


def start_ec2(event, context):
    ec2_instances = get_all_ec2_ids()
    response = ec2.start_instances(
        InstanceIds=ec2_instances,
        DryRun=False
    )
    return response


def stop_ec2(event, context):
    ec2_instances = get_all_ec2_ids()
    response = ec2.stop_instances(
        InstanceIds=ec2_instances,
        DryRun=False
    )
    return response


# get the list of all the ec2 instances
def get_all_ec2_ids():
    response = ec2.describe_instances(DryRun=False)
    instances = []
    for reservation in response["Reservations"]:
        for instance in reservation["Instances"]:
            # This sample print will output entire Dictionary object
            # This will print will output the value of the Dictionary key 'InstanceId'
            instances.append(instance["InstanceId"])
    return instances


####################################################################################################################

import boto3
import cStringIO
from PIL import Image, ImageOps
import os

s3 = boto3.client('s3')
size = int(os.environ['THUMBNAIL_SIZE'])


def s3_thumbnail_generator(event, context):
    # parse event
    print(event)
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    # only create a thumbnail on non thumbnail pictures
    if (not key.endswith("_thumbnail.png")):
        # get the image
        image = get_s3_image(bucket, key)
        # resize the image
        thumbnail = image_to_thumbnail(image)
        # get the new filename
        thumbnail_key = new_filename(key)
        # upload the file
        url = upload_to_s3(bucket, thumbnail_key, thumbnail)
        return url


def get_s3_image(bucket, key):
    response = s3.get_object(Bucket=bucket, Key=key)
    imagecontent = response['Body'].read()

    file = cStringIO.StringIO(imagecontent)
    img = Image.open(file)
    return img


def image_to_thumbnail(image):
    return ImageOps.fit(image, (size, size), Image.ANTIALIAS)


def new_filename(key):
    key_split = key.rsplit('.', 1)
    return key_split[0] + "_thumbnail.png"


def upload_to_s3(bucket, key, image):
    # We're saving the image into a cStringIO object to avoid writing to disk
    out_thumbnail = cStringIO.StringIO()
    # You MUST specify the file type because there is no file name to discern
    # it from
    image.save(out_thumbnail, 'PNG')
    out_thumbnail.seek(0)

    response = s3.put_object(
        ACL='public-read',
        Body=out_thumbnail,
        Bucket=bucket,
        ContentType='image/png',
        Key=key
    )
    print(response)

    url = '{}/{}/{}'.format(s3.meta.endpoint_url, bucket, key)
    return url

##########################################################################################################

this is one of the things I'd really like to see Lambda support, but currently it does not. One of the problems is that if there
were a lot of S3 PUT operations happening AWS would have to queue up all the Lambda invocations somehow, and there is currently 
no support for that.

If you built a locking mechanism into your Lambda function, what would you do with the requests you don't process due to a lock? 
Would you just throw those S3 notifications away?

The solution most people recommend is to have S3 send the notifications to an SQS queue, and then have your Lambda function scheduled 
to run periodically, like once a minute, and check if there is an item in the queue that needs to be processed.

Alternatively, have S3 send the notifications to SQS and just have a t2.nano EC2 instance with a single-threaded service polling
the queue.

Using messaging to serialize lambda executionie ie stop parallel executions
Have the S3 "Put events" cause a message to be placed on the queue (instead of involving a lambda function). The message should
contain a reference to the S3 object. Then SCHEDULE a lambda to "SHORT POLL the entire queue".




You can use an aws_cloudwatch_event_target resource to tie the scheduled event source (event rule) to your lambda function. You need to grant it permission to invoke your lambda function; you can use an aws_lambda_permission resource for this.

Example:

resource "aws_lambda_function" "check_foo" {
    filename = "check_foo.zip"
    function_name = "checkFoo"
    role = "arn:aws:iam::424242:role/something"
    handler = "index.handler"
}

resource "aws_cloudwatch_event_rule" "every_five_minutes" {
    name = "every-five-minutes"
    description = "Fires every five minutes"
    schedule_expression = "rate(5 minutes)"
}

resource "aws_cloudwatch_event_target" "check_foo_every_five_minutes" {
    rule = "${aws_cloudwatch_event_rule.every_five_minutes.name}"
    target_id = "check_foo"
    arn = "${aws_lambda_function.check_foo.arn}"
}

resource "aws_lambda_permission" "allow_cloudwatch_to_call_check_foo" {
    statement_id = "AllowExecutionFromCloudWatch"
    action = "lambda:InvokeFunction"
    function_name = "${aws_lambda_function.check_foo.function_name}"
    principal = "events.amazonaws.com"
    source_arn = "${aws_cloudwatch_event_rule.every_five_minutes.arn}"
}


##########################################################################################################################

AWS Lambda environment variables can be defined using the AWS Console, CLI, or SDKs. This is how you would define an AWS Lambda that uses an LD_LIBRARY_PATH environment variable using AWS CLI:

aws lambda create-function \
  --region us-east-1
  --function-name myTestFunction
  --zip-file fileb://path/package.zip
  --role role-arn
  --environment Variables={LD_LIBRARY_PATH=/usr/bin/test/lib64}
  --handler index.handler
  --runtime nodejs4.3
  --profile default
Once created, environment variables can be read using the support your language provides for accessing the environment, e.g. using process.env for Node.js. When using Python, you would need to import the os library, like in the following example:

...
import os
...
print("environment variable: " + os.environ['variable'])

#########################################################################################################################

def lambda_handler(event, context):
    # TODO implement
    import boto3

    s3 = boto3.client('s3')
    data = s3.get_object(Bucket='my_s3_bucket', Key='main.txt')
    contents = data['Body'].read()
    print(contents)
    
 ###################################################################################################################
 
 Amazon SNS is designed to distribute notifications. These can be received in a variety of formats, such as email, SMS, messages pushed
 to HTTP endpoints, mobile phone notifications and even triggering of AWS Lambda functions.

It is not designed as a fully-featured email system. It will only send text messages and appends an 'unsubscribe' footer at the 
bottom of the messages.

If you wish to send formatted emails, consider using Amazon Simple Email Service (SES), which improves email deliverability. 
Any content passed into Amazon SES is sent out to recipients, including HTML.

Amazon SNS is primarily about notification, rather than pretty content.

###############################################################################################################################
The AWS API Gateway is the only way to expose your lambda function over HTTP. The AWS lambda web console should create one
automatically for you if you use the microservice-http-endpoint blueprint when creating a new lambda function.

import boto3

def lambda_handler(event, context):

    client = boto3.client('dynamodb')

    for record in event['Records']:
        # your logic here...
        try:
            client.update_item(TableName='dynamo_table_name', Key={'hash_key':{'N':'value'}}, AttributeUpdates={"some_key":{"Action":"PUT","Value":{"N":'value'}}}) 
        except Exception, e:
            print (e)

#############################################################################################################################

Recently switched from GoDaddy (was a client for almost 5 years of their DNS and domain registration services) to Route 53.

Why GoDaddy was better for me than Route53:

they do not charge you for DNS queries;
they provide a free mail forwarding service.
That's it.

Now why Route53 is better than GoDaddy:

a better UI (GoDaddy's UI is really confusing);
API;
some domain zones are cheaper (e.g. "io" domain will cost you twice less per year);
provides a whois domain privacy for free (GoDaddy charges for that additionally).


Besides what's already been said about the quality of Amazon's infrastructure, the API is the killer feature of Route 53 
or competitors like Dynect. If your site does get large enough that you have a number of servers, you'll want to get into 
systems automation, and being able to automate your DNS changes can be quite nice.
################################################################################################################################








