CloudFormation

creating an S3 bucket with versioning , copy html folder from devops essentials to bucket after modifying permissions of bucket.

mark everything as public  then into ->  CodePipeline going by stages{name,source,build,deploy,service role,review}

source = S3,GitHub,CodeCommit
build = CodeBuild
Deploy = CloudFormation,CodeDeploy,Elastic Bean Stalk,OpsWorks
Invokde = Lambda

add approval stage in codepipeline  & try disabling transition to block pipeline   devopsessentialsaws  

json2yaml site 

create codecommit  unique for region   

######################################################################################################################

RT issues

backups – AMI or snapshots  + AWS backups(service) as a  feature itself is there using that VM backup can be taken. But depends on the kind of app you are trying to backup like three tier app  leverage lambda itself if none of the above works. By default aws takes incremental backup.

Containers on AWS – ECS/fargate/EKS/K8s on EC2  --> fargate mode is favoured   
monitoring solution -> Attach driver to send it to cloudwatch or config containers to send it to ELK/Dynatrace/newrelic

best libraries for K8s – kubectl vs kops --> depends on team   kubectl has so many APIs already

application monitoring – like heap size or mem fail in java or application errors --> cloudwatch agent can be configured to send those to cloudwatch & create custom metrics and alarms in cloudwatch & config lambda to reset heap or splunk based agents

application migration – steps    1)assessment 2)analysis 3)plan  + aws offers database migration service
ie from onpremise mysql or oracle   or synchronize from them to rds + schema conversion tool to check if there are compatibility issues like oracle to aurora

upload data to a snowball sent to your data centre + but if amount of data is small you can make use of vpn 

aws is coming up with outpost to deploy in onprem to integrate with aws sdk & apis

use simply monthly pricing calculator to get the idea of cost of a service 

aws has high throughput instances to avoid network throttling/congestion

try and remove technical debt while migrating ie replace old windows server from on from with the latest version

unless there is join query or long running qeries with dynamo db(no sql )   for special requirements go with rds

fargate is ideal for cost reduction than ecs + asg

aws cdk is easier to write than CF   with cdk taking care of json yaml syntax . Cdk better than tf 

migrating acroos regions – leverage global resource + ami/snapshot can be replicated across regions

scaling db possible with read-replicas  horizontally

multi-cloud is in experimental phase with most clients

IAM access key rotation should be done with lambda. But for secrets use secrets manager

Shankar – IBM/AIX engineer – Linux – Storage – AWS (2015) – devops  then did POCs before joining as devops engineer . Currently more focussed in devops than aws. 
Working in git/bitbucket – jenkins – docker – k8s – openshift/openstack – application side nginx 

mobaxterm over putty  saves a lot of time – lot of ansible with visual studio than pycharm 

challenges – if you now solution its simple 

certification is more important for the client to showcase
aws security is a booming area    you might have to pay 1000s of $ as penalty to customer

migrated apps from ubuntu to rhel using ansible playbooks + postbuild activities installation of packages,users,file systems + updating nginx app config

kubernetes admin – infrastructure admin + docker/k8s admin – writing yaml manifests   

docker resource issues + monitoring

business logic is taken care by app server but for scalability & performance you need to bring in nosql db. Then you bring in db caches like redis or memcache. And media files can be stored in flat storage like s3 . But content filter should be there in the media.

Spark/hadoop + dwh is required for data analytics , advertisng, business intelligence,strategy planning.

CDN is required for caching as per location.

SMS,Email & messaging queing services are also required for an fb like application.

Relational rds, nosql dynamodb, memcache   elasticcache

content filter – rekognition , mp4-> mp3 etc format change and all lambda can do

kinesis for click stream analysys – emr for spark/hadoop – glue is for etl operations – redshift is the dwh – quicksight/athena is the BI tool for visualization & insight

Managed API gateway is where you put all your REST APIs. Web & mobile user management is done by incognito. WAF are web app firewall like preventing xss & mitm. Inspector is for checking cve & patching stuff .

Codecommit – code build(maven/ant) – codedeploy   also look into codepipeline  (necessary roles need to be created and attached accordingly)

codestar is the service tool to integrate with Jira and other bug tracking services.

Codedeploy agent needs to be installed in ec2 machine ie need to pull it down frm s3 bucket hence the ec2 machine needs to have appropriate role assigned to it.

Make sure that tagging is done on EC2 and same is given to codedeploy as well like labels in gitlab runner

aws provides cloudwatch agent that needs to be installed on EC2 instance based on the OS version
sudo yum install -u awslogs

then make config  changes in 
/etc/awslogs/awscli.conf   & /etc/awslogs/awslogs.conf

sudo service awslogsd start 

Useful contents:

 1. IAM Policy { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "logs:CreateLogGroup", "logs:CreateLogStream", "logs:PutLogEvents", "logs:DescribeLogStreams" ], "Resource": [ "arn:aws:logs:*:*:*" ] } ] } 

2. awslogs.conf file contents:
[general] state_file = /var/lib/awslogs/agent-state [application_logs] region = ap-south-1 datetime_format = %b %d %H:%M:%S file = /var/log/application.log buffer_duration = 5000 log_stream_name = {instance_id} initial_position = start_of_file log_group_name = application_logs


once that is done logs will appear in cloudwatch mentioned log groups and the next step is to create filters  - select the log group and create metric filter – Filter pattern : ERROR – assign metric & create filter – create an alarm – whenver > 0 action to send an email 


Setting up cross account access – Get the account id from support center (top right corner)
going to account A – and creating an IAM role – select type of trusted entity – choose another AWS account (instead of the usual aws services) – provide account ID of B – permissions – Ec2readonlyaccess + s3readonlyaccess – tags: role for account B – then give a name and descriptin in the review section as role_for_Account_B – create a role – thats it from account A – copy the created role’s arn – Goto account B – IAM – either create a new user or use the existing user – add inline policy / add permissions (he is doing with inline policy json) - 

Setup the AWS cross account access for IAM users AssumeRole JSON Policy: { "Version": "2012-10-17", "Statement": { "Effect": "Allow", "Action": "sts:AssumeRole", "Resource": "arn:aws:iam::ACCOUNT-A-ID:role/Role_for_B" } }

review the policy – name it (assume_role_for_account_A) – create policy – then logout from account B – log back in as the new user – top right hand corner ‘switch role’ option – give account A ID & role name – once you are done switch back to usual account 

DNS failover across regions – create 2 ec2 with httpd and sample index files in place – goto route53 dashboard – goto hosted zone (your domain will show up here if pre-configured) – first create a health check – give a name – protocol: http + public ip + port + /index.html – create health check – then create another health check for second regions server – then going back to hosted zones – say abc.com is your domain   lets create a subdomain ie create a record set   test.abc.com   type:A  then give value as the ip address of primary webserver – and routing policy is failover – failover record type is : primary – associate health check as yes – provide the created health check name and create  - then create record set again (but this time its secondary) – give value (ip ) and routing policy(failover) as before BUT faiover type is SECONDARY this time – associate health check as well.

Dbs will be like one read-cluster replica other as write-cluster 

S2S VPN connection ==>      AWS – VPG <- VPN -> CGW(cisco,juniper) – onprem 
he is simulating onprem with another VPC and installing openswan on an EC2 machine making it a S/w defined CGW 

Useful Information: 
1. Help/Commands for Installation of Openswan
i. Change to root user: $ sudo su
ii. Install openswan: $ yum install openswan -y
iii. In /etc/ipsec.conf uncomment following line if not already uncommented: include /etc/ipsec.d/*.conf
iv. Update /etc/sysctl.conf to have following
net.ipv4.ip_forward = 1 
net.ipv4.conf.all.accept_redirects = 0 
net.ipv4.conf.all.send_redirects = 0 
v. Restart network service: $ service network restart

2. Contents for /etc/ipsec.d/aws-vpn.conf
conn Tunnel1 authby=secret auto=start left=%defaultroute leftid=Customer end VPN public IP right=AWS VPN Tunnel 1 public IP type=tunnel ikelifetime=8h keylife=1h phase2alg=aes128-sha1;modp1024 ike=aes128-sha1;modp1024 keyingtries=%forever keyexchange=ike leftsubnet=Customer end VPN CIDR rightsubnet=AWS end VPN CIDR dpddelay=10 dpdtimeout=30 dpdaction=restart_by_peer

 3. Contents for /etc/ipsec.d/aws-vpn.secrets customer_public_ip aws_vgw_public_ip: PSK "shared secret" 

4. Commands to enable/start ipsec service
$ chkconfig ipsec on 
$ service ipsec start
 $ service ipsec status


disable source destination check in the cgw ec2

 
then create a virtual private gateway – then create CGW mentionig the public ip of openswan ec2 – then go to s2s vpn
connection & give the VPG and CGW details to it – mention routing as static routing – give the ip cidr of other 
side – update RT accordingly – updating RT is different   you need to enable route propogation  - then need to download 
the configuration and update at the other end of the tunnel – vendor (mention as generic as its software defined cgw) - 
the data needs to be pasted to  /etc/ipsec.d/aws-vpn.conf in the CGW Ec2  - leftside = customer   rightside = aws vpc  - 
then the authentication file as given above  /etc/ipsec.d/aws-vpn.secrets

- start & enable ipsec  - validate by pinging over private ip addresses – by now RT will have an entry for VPG as well
since we had enabled route propogation

ELB with 2 WS (App LB L7 http) – spin up 2 instances with httpd running – create a target group  - http 80 /index.html 
as usual – then add the 2 target WS to the group – create a LB – link with the target groups hence ip details of targets 
are autopopulated – next assignment is putting the WS in ASG instead of just putting them in target group – create LB –
choose new target group and no need to attach target during creation since later we will attach ASG to it – goto ASG –
create ASG – create launch config (just like ec2 + SG can be just for ELB subnet) – configure scaling policies based
on avg.cpu – then link it with LB

App LB & Path based routing – say u want requests to abc.com/a to goto WS1 & abc.com/b to WS2







#####################################################################################################################
53 is the TCP/UDP port where DNS services are running

buy your domain after checking the availabilty - partnered with Gandy - need to map the public ip of EC2 running web browser to domain -
choose hosted zones from dashboard - create a record set - type A (ipaddress of EC2 as value and TTL 300s) - click create 

But additionally it provides LB kind of features in the name of health checks

Amazon Route 53 allows DNS failover with active-active, active-passive, and mixed configurations to implement high availability of 
the applications. For instance, if there are many resources that perform the same function, then Route 53 can check the state (health)
of those resources and respond to DNS queries only the resources that are healthy.



Each EC2 instance has an elastic IP so that, after a stop/start of the instance, we will receive the same public IP address.

choose health checks - 
In the next window, you will need to fill in some details:

– Name—This is something that should be descriptive to you. You will reference this later.

– Protocol—This will be the protocol used for testing. Because we need to test if the HTTP server is up, we will use HTTP.

– IP Address—The IP address of the HTTP server.

– Port—The port used for availability checking.

– Request Interval—How often the health will be checked.

– Failure Threshold—After how many failures the health check will be considered as failure.

As you can see, we used pretty aggressive intervals/thresholds because we need to detect a possible failure as soon as possible.

Click on “Create” to create the health check.

Select “Hosted Zones” and then “Go to Record Sets”: Click on “Create Record Set” to create the first record set. The record set will be for www.vtep.net:


Everything is the same as explained in Part I of the series. In the value field, you will need to put the IP address of EC2 instance 
from US EAST region. The interesting stuff starts after this.

In the “Routing Policy” choose “Failover” and in the “Failure Record Type” choose “Primary” because this will be the primary server. 
On “Set ID,” fill in something that is meaningful for you. Further we need to Route 53 how to track the availability of this server.
Choose “Yes” from “Associate with Health Check” and then, from the drop down menu, we will choose the health check that we configured 
earlier. Then click on “Create” to create the record set.

Then we need to create another record set with the same name, www.vtep.net. Fill in the IP address of the EC2 instance from 
US EAST instance. In the “Routing Policy” choose “Failover” and in the “Failure Record Type” choose “Secondary” because this 
will be the secondary server. We don’t need to specify any health check because we don’t have a server that will be backup 
to this one. In case we would have, we need to configure the health check and then reference it. Click on “Create” to continue.



The health checks done by Route 53 are:

HTTP and HTTPS health checks—Route 53 must establish a TCP connection in less than four seconds. The endpoint should respond with a status code higher than 200 (including), but lower than 400 within 2 seconds.
TCP health checks—Route 53 must establish a TCP connection in less than 4 seconds

So we will start by looking at the latency routing. I have one EC2 instance running in EU CENTRAL (Frankfurt) region and one EC2 
instance in US EAST (N. Virginia). Each EC2 instance is running a web server and each one of them is returning a customized output 
to identify in which region is the EC2 instance running.

What we will demonstrate first is that we can route the user based on latency. For instance, we will see that users from Europe will
be served the webpage from the EC2 instance running in the EU CENTRAL region and the users from US will be served the webpage from 
the EC2 instance running in the US EAST region.

Let’s test it. I configured a proxy on my browser with an IP address from Switzerland. Obviously, Switzerland is closer to 
Frankfurt than to North Virginia:

It is important to understand that once you create the public hosted zone, a name server (NS) record and a start of authority (SOA) 
record are automatically created. The NS record is important here. It provides you with four name servers that you need to configure
with your registrar or DNS services, so all the queries related to your domain are routed to Amazon Route 53 name servers for 
resolution.

If we purchase a domain from Route 53, then the hosted zone is created automatically and we don't need to create one.
###############################################################################################################

AWS CloudWatch and CloudTrail: How do they work together?
AWS CloudWatch is integrated with a whole host of AWS services, including CloudTrail. Using CloudTrail’s ability to log a change to 
a resource, a CloudWatch Event rule can be created to recognize this and then trigger a Lambda function to open a ticket for 
investigation.


Classic Load balancer are used in times when there is simple traffic distribution between EC2 Instances

Application Load Balancer is used when there are container based or microservices based architecture which need to route the traffic
to different services or balance the traffic across different port on same EC2 Instance.





For serverless sundog uploaded to S3

 I was able to solve a concurrency problem when updating a file on S3. I am using lambda function to update a file on S3 using
 concurrency count as 1. This ensures that at a time only one ec2 instance has write access to the file.This is much cheaper 
 then using any managed DB on AWS for trivial usage

######################################################################################################################

CloudFormation   
--------------------------------
this is executed from console   programming the GUI console 

use update stack options after editting


{} encloses object   
{
	"name" : "siva",
	"salary" : "200000",
	"isManager" : true,
	"skills" : ["AWS","Python","GCP"],
	"address" : {"country" : "india" ,"state" : "karnataka"}  # object in object	
}


extension can be .json/.yaml

start in atom gives template  - basic skelton remains same

"Type" : "AWS::EC2::VPC"

Resources  logical-name cidr

==> console - CF - crete a new stack - choose file - give name to your stack - go with remaining defaults

Parameters : logical name descriptin type ( string) default value   allowed-values(t2-micro,m1.large)  NoEcho : true means passwords not shown   MinLength and MaxLength & add a resource as well ( like a VPC for parameters)   like-wise we can get AZs as parameters or anything else like volumes 

parameterizing CF : Cidr block is hard coded hence will parameterise it.Type: String Default: "10.23.0.0/16"===> then call it(~interpolation) using "Ref" : logical name

CF intrinsic functions (like Fn::split): adding 1 subnet using ref for VPC    

adding IGw & attaching to VPC again use ref & logical name of VPC to attach   then creating RT & attaching to this IGw (RT is a component of VPC) requires Gw id  then
associate subnets to newly created RT 

adding userdata : Fn::Base64 for encoding )  & Fn::Join    {scripts under user data are run as root}

metadata { cloudformation::init   -> gives better template structure for user addition,config files or commannds or services management 
findinmap  to define a map of AMIs as per regions   ELBs are PaaS therefore just http, no ssh 
in route53 ending dot is required   CFoutputs   outputs can be seen in tab of CF window

Cross stack access  as network  db  vpc subnets  & connect them using export and output  importvalue   Fn::Sub  

CF conditions    deploy zipped lambda fn from S3 bucket 

#######################################################################################################################

Invoke another lambda from lambda 

from boto3 import client as boto3_client
from datetime import datetime
import json

lambda_client = boto3_client('lambda')

def lambda_handler(event, context):
    msg = {"key":"new_invocation", "at": datetime.now()}
    invoke_response = lambda_client.invoke(FunctionName="another_lambda_",
                                           InvocationType='Event',
                                           Payload=json.dumps(msg))
    print(invoke_response)
Btw, you would need to add a policy like this to your lambda role as well

   {
        "Sid": "Stmt1234567890",
        "Effect": "Allow",
        "Action": [
            "lambda:InvokeFunction"
        ],
        "Resource": "*"
    }
    
    
    #########################################################################################
    
    To use requests module, you can simply import requests from botocore.vendored. For example:

from botocore.vendored import requests

def lambda_handler(event, context):
   response = requests.get("https://example.com/")
   print response.json()
   
   ########################################################################################################################
   For lambda zip
    The structure in my zip file was broken. It is important that the python script and the packed dependencies (as folders) are 
    in the root of the zip file. This solved my problem.
    
    
97

For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions.

{
    "Statement": [
        {
            "Action": [
                "logs:CreateLogGroup",
                 "logs:CreateLogStream",
                 "logs:PutLogEvents"
            ],
            "Effect": "Allow",
            "Resource": "arn:aws:logs:*:*:*"
        }
    ]
} 

After you update your policy, it seems that you have to update your function's settings to refresh all job instances to read new policies.

So if you just click 'test' button from Lambda console after you update your role policy in IAM, the cached Lambda instances will still have old role permissions, so you will still see no logs being written to Cloudwatch logs.

Just change your timeout by a second and click on 'save and test' button, and you will start to see logs in Cloudwatch.
#############################################################################################################################

read will return bytes. At least for Python 3, if you want to return a string, you have to decode using the right encoding:

import boto3

s3 = boto3.resource('s3')

obj = s3.Object(bucket, key)
obj.get()['Body'].read().decode('utf-8') 

###########################################################################################################################

import boto3
import json

s3 = boto3.client('s3')

obj = s3.get_object(Bucket=bucket, Key=key)
j = json.loads(obj['Body'].read())
NOTE (for python 2.7): My object is all ascii, so I don't need .decode('utf-8')

NOTE (for python 3.6+): We moved to python 3.6 and discovered that read() now returns bytes so if you want to get a string out of it, you must use:

j = json.loads(obj['Body'].read().decode('utf-8'))

#################################################################################################################################

I just wanted to update the database from my lambda function. Is it possible to access RDS by specifiying IAM Role and access Policy?.

No you cannot. You need to provide DB url/username/password to connect. You may need to run Lambda in same VPC if it is in private subnet. See my pointers below.

I can connect to mysql databse using mysql client.but when i try on lambda i can't do that.

This is strict No , No! Your RDS should not be accessible from Internet unless you really need it. Try to run it in private subnet and configure other AWS services accordingly.

Two cents from my end if you are getting timeouts accessing resourced from Lambda-

By default Lambda has internet access and can access online resources.
Lambda cannot access services rurnning in private subnet of your VPC.
To connect to services in private subnet you need to run the lambda is private subnet. For this you need to go to Network section and configure your VPC, subnets and security group.
However note that when you do this you will loose Internet access. If you still need Internet access you will have to spin up a NAT gateway or NAT instance in public subnet and configure route from private subnet to this NAT.
I faced this when I was trying to connect to RDS in private subnet from my lambda. Since I used KMS to encrypt some environment variables and decryption part requires Internet access I had to use a NAT gateway.
More details - http://docs.aws.amazon.com/lambda/latest/dg/vpc.html#vpc-internet

How to connect to postgres RDS from AWS Lambda

#####################################################################################################################################
Basically, domain queries are automatically routed to the nearest DNS server to provide the quickest response possible. If you use a web hosting company like GoDaddy, it takes 30 minutes to 24 hours to remap a domain to a different IP, but by using Route 53 in AWS it takes only a few minutes.

Now, take a look at the benefits provided by Route 53.

Amazon Route 53 Benefits
Route 53 provides the user with several benefits.

They are:

Highly Available and Reliable
Flexible
Simple
Fast
Cost-effective
Designed to Integrate with Other AWS Services
Secure
Scalable

##########################################################################################################################################

import requests

print('Loading function')

s3 = boto3.client('s3')


def lambda_handler(event, context):
    #print("Received event: " + json.dumps(event, indent=2))

    # Get the object from the event and show its content type
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.unquote_plus(event['Records'][0]['s3']['object']['key']).decode('utf8')
    try:
        response = s3.get_object(Bucket=bucket, Key=key)
        s3.download_file(bucket,key, '/tmp/data.txt')
        lines = [line.rstrip('\n') for line in open('/tmp/data.txt')]
        for line in lines:
            col=line.split(',')
            print(col[5],col[6])
        print("CONTENT TYPE: " + response['ContentType'])
        return response['ContentType']
    except Exception as e:
        print(e)
        print('Error getting object {} from bucket {}. Make sure they exist and your bucket is in the same region as this function.'.format(key, bucket))
        raise e
#############################################################################################################################

Amazon Lambda is designed to be used as an event-driven system that responds to events. The flow is:

Something happens somewhere that triggers Lambda (eg an upload to Amazon S3, data coming into an Amazon Kinesis stream, an application invoking the Lambda function directly)
The Lambda function is created, data from the trigger event is passed
The Lambda function runs
Lambda functions are limited to a maximum execution time of 15 minutes (this was recently increased from the original 5 minutes timeout). The actual limit is configured when the Lambda function is created. The limit is in place because Lambda functions are meant to be small and quick rather than being large applications.

Your error message says Task timed out after 15.00 seconds. This means that AWS intentionally stopped the task once it hit a run-time of 15 seconds. It has nothing to do with what the function was doing at the time, nor the file that was being processed.

To fix: Increase the timeout setting on the configuration page of your Lambda function.

###########################################################################################################################

My Lambda function runs every 15 minutes, it is triggered by Jenkins.My program gets the source code of the URL into the string,
then compares it with already existing file in /tmp directory (the source code from previous run), and if something has changed, 
uploads it to S3 bucket. So /tmp stores the latest source code file for comparison.


with NACL you may DENY specific IP which cant be done at SG level.

Reverse charging for EIPs    

You can use managed NAT GW from AWS or turn your EC2 into a NAT

NAT must be in public subnet while private subnet must have internet route via NAT   NAT must have EIP ( not under free tier)

by default EC2 will accept traffic only if destination is itself  but for NAT it has to accept everything hence disable src/dest check

choose a custom NAT ami from market place   in actions networking disable src/dest check

VPC peering through private IPs   no pulblic address involved    not transitive   a-b & b-c != a-c
private ec2s in Mumbai & NVirginia to communicate through private addresses

for on prem    VPGw - VPN tunnel - CustomerGW. One VPG can connect to multiple CGW  --> This is S2S over internet 

but you dont want tunnel over internet then direct-connect - dedicated nw   you might also go for brokers(middle amn) who already have 
lines and ready to lease

EC2 needs to connect to S3  you go for public internet & get charged but if you have both of them in same region  you can use 
VPC endpoint ie private connection.  ec2 - vpc endpoint - S3    OR ec2 - vpcept - dynamoDB

VPC flow log ~ wireshark  

1 central management VPC that'd connect to all the other VPCs   Chef,puppet masters be in central VPC

VPC endpoint for private subnet ec2 to connect to public bucket

####################################################################################################################

ec2 = boto3.client('ec2')


def start_ec2(event, context):
    ec2_instances = get_all_ec2_ids()
    response = ec2.start_instances(
        InstanceIds=ec2_instances,
        DryRun=False
    )
    return response


def stop_ec2(event, context):
    ec2_instances = get_all_ec2_ids()
    response = ec2.stop_instances(
        InstanceIds=ec2_instances,
        DryRun=False
    )
    return response


# get the list of all the ec2 instances
def get_all_ec2_ids():
    response = ec2.describe_instances(DryRun=False)
    instances = []
    for reservation in response["Reservations"]:
        for instance in reservation["Instances"]:
            # This sample print will output entire Dictionary object
            # This will print will output the value of the Dictionary key 'InstanceId'
            instances.append(instance["InstanceId"])
    return instances


####################################################################################################################

import boto3
import cStringIO
from PIL import Image, ImageOps
import os

s3 = boto3.client('s3')
size = int(os.environ['THUMBNAIL_SIZE'])


def s3_thumbnail_generator(event, context):
    # parse event
    print(event)
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    # only create a thumbnail on non thumbnail pictures
    if (not key.endswith("_thumbnail.png")):
        # get the image
        image = get_s3_image(bucket, key)
        # resize the image
        thumbnail = image_to_thumbnail(image)
        # get the new filename
        thumbnail_key = new_filename(key)
        # upload the file
        url = upload_to_s3(bucket, thumbnail_key, thumbnail)
        return url


def get_s3_image(bucket, key):
    response = s3.get_object(Bucket=bucket, Key=key)
    imagecontent = response['Body'].read()

    file = cStringIO.StringIO(imagecontent)
    img = Image.open(file)
    return img


def image_to_thumbnail(image):
    return ImageOps.fit(image, (size, size), Image.ANTIALIAS)


def new_filename(key):
    key_split = key.rsplit('.', 1)
    return key_split[0] + "_thumbnail.png"


def upload_to_s3(bucket, key, image):
    # We're saving the image into a cStringIO object to avoid writing to disk
    out_thumbnail = cStringIO.StringIO()
    # You MUST specify the file type because there is no file name to discern
    # it from
    image.save(out_thumbnail, 'PNG')
    out_thumbnail.seek(0)

    response = s3.put_object(
        ACL='public-read',
        Body=out_thumbnail,
        Bucket=bucket,
        ContentType='image/png',
        Key=key
    )
    print(response)

    url = '{}/{}/{}'.format(s3.meta.endpoint_url, bucket, key)
    return url

##########################################################################################################

this is one of the things I'd really like to see Lambda support, but currently it does not. One of the problems is that if there
were a lot of S3 PUT operations happening AWS would have to queue up all the Lambda invocations somehow, and there is currently 
no support for that.

If you built a locking mechanism into your Lambda function, what would you do with the requests you don't process due to a lock? 
Would you just throw those S3 notifications away?

The solution most people recommend is to have S3 send the notifications to an SQS queue, and then have your Lambda function scheduled 
to run periodically, like once a minute, and check if there is an item in the queue that needs to be processed.

Alternatively, have S3 send the notifications to SQS and just have a t2.nano EC2 instance with a single-threaded service polling
the queue.

Using messaging to serialize lambda executionie ie stop parallel executions
Have the S3 "Put events" cause a message to be placed on the queue (instead of involving a lambda function). The message should
contain a reference to the S3 object. Then SCHEDULE a lambda to "SHORT POLL the entire queue".




You can use an aws_cloudwatch_event_target resource to tie the scheduled event source (event rule) to your lambda function. You need to grant it permission to invoke your lambda function; you can use an aws_lambda_permission resource for this.

Example:

resource "aws_lambda_function" "check_foo" {
    filename = "check_foo.zip"
    function_name = "checkFoo"
    role = "arn:aws:iam::424242:role/something"
    handler = "index.handler"
}

resource "aws_cloudwatch_event_rule" "every_five_minutes" {
    name = "every-five-minutes"
    description = "Fires every five minutes"
    schedule_expression = "rate(5 minutes)"
}

resource "aws_cloudwatch_event_target" "check_foo_every_five_minutes" {
    rule = "${aws_cloudwatch_event_rule.every_five_minutes.name}"
    target_id = "check_foo"
    arn = "${aws_lambda_function.check_foo.arn}"
}

resource "aws_lambda_permission" "allow_cloudwatch_to_call_check_foo" {
    statement_id = "AllowExecutionFromCloudWatch"
    action = "lambda:InvokeFunction"
    function_name = "${aws_lambda_function.check_foo.function_name}"
    principal = "events.amazonaws.com"
    source_arn = "${aws_cloudwatch_event_rule.every_five_minutes.arn}"
}


##########################################################################################################################

AWS Lambda environment variables can be defined using the AWS Console, CLI, or SDKs. This is how you would define an AWS Lambda that uses an LD_LIBRARY_PATH environment variable using AWS CLI:

aws lambda create-function \
  --region us-east-1
  --function-name myTestFunction
  --zip-file fileb://path/package.zip
  --role role-arn
  --environment Variables={LD_LIBRARY_PATH=/usr/bin/test/lib64}
  --handler index.handler
  --runtime nodejs4.3
  --profile default
Once created, environment variables can be read using the support your language provides for accessing the environment, e.g. using process.env for Node.js. When using Python, you would need to import the os library, like in the following example:

...
import os
...
print("environment variable: " + os.environ['variable'])

#########################################################################################################################

def lambda_handler(event, context):
    # TODO implement
    import boto3

    s3 = boto3.client('s3')
    data = s3.get_object(Bucket='my_s3_bucket', Key='main.txt')
    contents = data['Body'].read()
    print(contents)
    
 ###################################################################################################################
 
 Amazon SNS is designed to distribute notifications. These can be received in a variety of formats, such as email, SMS, messages pushed
 to HTTP endpoints, mobile phone notifications and even triggering of AWS Lambda functions.

It is not designed as a fully-featured email system. It will only send text messages and appends an 'unsubscribe' footer at the 
bottom of the messages.

If you wish to send formatted emails, consider using Amazon Simple Email Service (SES), which improves email deliverability. 
Any content passed into Amazon SES is sent out to recipients, including HTML.

Amazon SNS is primarily about notification, rather than pretty content.

###############################################################################################################################
The AWS API Gateway is the only way to expose your lambda function over HTTP. The AWS lambda web console should create one
automatically for you if you use the microservice-http-endpoint blueprint when creating a new lambda function.

import boto3

def lambda_handler(event, context):

    client = boto3.client('dynamodb')

    for record in event['Records']:
        # your logic here...
        try:
            client.update_item(TableName='dynamo_table_name', Key={'hash_key':{'N':'value'}}, AttributeUpdates={"some_key":{"Action":"PUT","Value":{"N":'value'}}}) 
        except Exception, e:
            print (e)

#############################################################################################################################

Recently switched from GoDaddy (was a client for almost 5 years of their DNS and domain registration services) to Route 53.

Why GoDaddy was better for me than Route53:

they do not charge you for DNS queries;
they provide a free mail forwarding service.
That's it.

Now why Route53 is better than GoDaddy:

a better UI (GoDaddy's UI is really confusing);
API;
some domain zones are cheaper (e.g. "io" domain will cost you twice less per year);
provides a whois domain privacy for free (GoDaddy charges for that additionally).


Besides what's already been said about the quality of Amazon's infrastructure, the API is the killer feature of Route 53 
or competitors like Dynect. If your site does get large enough that you have a number of servers, you'll want to get into 
systems automation, and being able to automate your DNS changes can be quite nice.
################################################################################################################################

client = boto3.client('ec2')

resp = client.run_instances(ImageId='ami-467ca739',
                     InstanceType='t2.micro',
                     MinCount=1,
                     MaxCount=1)
for instance in resp['Instances']:
    print(instance['InstanceId'])
    
    
    
client = boto3.client('s3')
response = client.list_buckets()
for bucket in response['Buckets']:
    print(bucket['Name'])
    
    
client = boto3.client('s3')
response = client.delete_object(
    Bucket='123',
    Key='create_bucket.py'
)



file_reader = open('create_bucket.py').read()
response = client.put_object(
    ACL='private',
    Body=file_reader,
    Bucket='123',
    Key='create_bucket.py'
)


response = client.create_bucket(
    ACL='private',
    Bucket='javahomecloud123',
    CreateBucketConfiguration={
        'LocationConstraint': 'ap-south-1'
    }
)


dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('employees')

with table.batch_writer() as batch:
    for x in range(100):
        batch.put_item(
            Item={
                'emp_id': str(x),
                'name': 'Name-{}'.format(x)
            }
        )



dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('employees')
resp = table.get_item(
    Key={
        'emp_id': '2'
    }
)

print(resp['Item'])

table.delete_item(
    Key={
        'emp_id': '2'
    }
)



table.put_item(
    Item={
        'emp_id': '2',
        'name': 'kammana',
        'salary': 20000
    }
)



##########################
## Part-1 Create Images ##
##########################

source_region = 'ap-south-1'
ec2 = boto3.resource('ec2', region_name=source_region)


instances = ec2.instances.filter(InstanceIds=['i-0067eeaab6c8188fd'])

image_ids = []

for instance in instances:
    image = instance.create_image(Name='Demo Boto - '+instance.id, Description='Demo Boto'+instance.id)
    image_ids.append(image.id)

print("Images to be copied {} ".format(image_ids))


#############################################
## Part-2 Wait For Images to be available  ##
#############################################
# Get waiter for image_available

client = boto3.client('ec2', region_name=source_region)
waiter = client.get_waiter('image_available')

# Wait for Images to be ready
waiter.wait(Filters=[{
    'Name': 'image-id',
    'Values': image_ids
}])

##########################################
## Part-3 Copy Images to other regions  ##
##########################################

# Copy Images to the region, us-east-1

destination_region = 'us-east-1'
client = boto3.client('ec2', region_name=destination_region)
for image_id in image_ids:
    client.copy_image(Name='Boto3 Copy'+image_id, SourceImageId=image_id, SourceRegion='ap-south-1')




from datetime import datetime, timedelta, timezone

import boto3
ec2 = boto3.resource('ec2')

# List(ec2.Snapshot)
snapshots = ec2.snapshots.filter(OwnerIds=['self'])

for snapshot in snapshots:
    start_time = snapshot.start_time
    delete_time = datetime.now(tz=timezone.utc) - timedelta(days=15)
    if delete_time > start_time:
        snapshot.delete()
        print('Snapshot with Id = {} is deleted '.format(snapshot.snapshot_id))





client = boto3.client('ec2')

resp = client.describe_instances(Filters=[{
    'Name': 'tag:Env',
    'Values': ['Prod']
}])

for reservation in resp['Reservations']:
    for instance in reservation['Instances']:
        print("InstanceId is {} ".format(instance['InstanceId']))
	
	











