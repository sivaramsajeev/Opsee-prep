

df -i .   ---> find . -xdev -empty -print |wc -l


By default, you can always list other users processes in Linux.

To change that, you need to mount proc in /etc/fstab with hidepid=2:

proc            /proc           proc    defaults,hidepid=2
This functionality is supported from the kernel v3.2 onwards. It hides /proc and consequentially ps activity from all users except root.


To check if Pdftk is already installed

sudo apt list | grep pdftk 

If output contains '[installed]' tag with pdftk then you can skip step1 i.e if the output is like this

qpdf -password=<your-password> -decrypt /path/to/secured.pdf out.pdf
###########################################################################################################
VIRT stands for the virtual size of a process, which is the sum of memory it is actually using, memory it has mapped into itself (for instance the video
 card’s RAM for the X server), files on disk that have been mapped into it (most notably shared libraries), and memory shared with other processes. 
 VIRT represents how much memory the program is able to access at the present moment.

RES stands for the resident size, which is an accurate representation of how much actual physical memory a process is consuming. (This also corresponds 
directly to the %MEM column.) This will virtually always be less than the VIRT size, since most programs depend on the C library.

SHR indicates how much of the VIRT size is actually sharable (memory or libraries). In the case of libraries, it does not necessarily mean that the entire 
library is resident. For example, if a program only uses a few functions in a library, the whole library is mapped and will be counted in VIRT and SHR, 
but only the parts of the library file containing the functions being used will actually be loaded in and be counted under RES.



I would perhaps only re-word "VIRT represents how much memory the program is able to access at the present moment." to something like "VIRT represents the
 size of the program's entire addressable space at the present moment." OK, that could still use polish. But the point is, "how much memory" can still give 
 the impression we're discussing RAM, when VIRT has nothing to do with RAM space. In fact, large programs will often have a VIRT size that's several MULTIPLEs
 of the total system RAM size — because that VIRT is almost entirely file-backed address regions (AKA "disk not RAM").
 
 
 You have to create device nodes into /dev with mknod. The device nodes in dev have a type (block, character and so on), a major number and a minor number.
 You can find out the type and the major number by doing ls -l /dev/loop0:

user@foo:/sys# ls -l /dev/loop0
brw-rw---- 1 root disk 7, 0 Oct  8 08:12 /dev/loop0
This means loop device nodes should have the block type and major number of 7. The minor numbers increment by one for each device node, starting from 0, so
 loop0 is simply 0 and loop7 is 7.

To create loop8 you run, as root, command mknod -m 0660 /dev/loop8 b 7 8. This will create the device node /dev/loop8 with permissions 
specified along the -m switch (that's not necessary as you're probably running a desktop system, but it's a good idea not to let 
everyone read and write your device nodes).

When you run it as root, losetup -f will automatically create loop devices as needed if there aren't any free ones available.

So rather than doing it yourself with mknod, the easiest way to create a new loop device is with sudo losetup -f. 
That approach will give you a free existing loop device if one exists, or automatically create a new one if needed.



########################################################################################################
Create a file
1. First step is to create a file of desired size. The following command will create a file that is 1 GB in size:

# dd if=/dev/zero of=loopbackfile.img bs=100M count=10

Create the loop device
1. Next step is to create a loop device with the file. Use the command “losetup” to create a loop device “loop0”

# losetup -fP loopbackfile.img
Here,
-f – find the first unused loop device. If a file argument is present, use this device. Otherwise, print its name.
-P – force kernel to scan partition table on newly created loop device.
2. To print the loop device generated using the above command use “losetup -a”.

# losetup -a
/dev/loop0: [64769]:4199216 (/root/loopbackfile.img)

1. Now lets create a ext4 filesystem on the loopback device.

# mkfs.ext4 /root/loopbackfile.img 

Mount the loopback filesystem
1. We can now mount the loopback filesystem onto a directory. The “-o loop” additional option is used to mount loopback filesystems.

# mkdir /loopfs
# mount -o loop /dev/loop0 /loopfs
2. Verify the size of the new mount point and type of filesystem using below commands.

# df -hP /loopfs/
Filesystem      Size  Used Avail Use% Mounted on
/dev/loop1      969M  2.5M  900M   1% /loopfs


Removing loop device
If you want remove the new filesystem, use the following steps:
1. Umount and delete the directory /loopfs

umount /loopfs
rmdir /loopfs
2. Delete the loopback device “loop0” created using the “losetup -d” command.

# losetup -d /dev/loop0
3. Finally remove the file “/root/loopbackfile.img” used to create the loop device.

# rm /root/loopbackfile.img



###############################################################################################################


Out of Memory (OOM) refers to a computing state where all available memory, including swap space, has been allocated. Normally this will cause the system 
to panic and stop functioning as expected. There is a switch that controls OOM behavior in /proc/sys/vm/panic_on_oom. When set to 1 the kernel will panic 
on OOM. A setting of 0 instructs the kernel to call a function named oom_killer on an OOM. Usually, oom_killer can kill rogue processes and the system will 
survive. The easiest way to change this is to echo the new value to /proc/sys/vm/panic_on_oom.

# cat /proc/sys/vm/panic_on_oom 1

# echo 0 > /proc/sys/vm/panic_on_oom

# cat /proc/sys/vm/panic_on_oom 0

The RHEL5 kernel includes 2 files for each process to control when that process will be considered for termination when the system must start OOM killing.

/proc/<pid>/oom_adj - Adjust the oom-killer score.

This file can be used to adjust the score used to select which processes shall be killed in an out-of-memory situation. Giving a process a high score, 
increase the likelihood of this process being killed by the oom-killer. Valid values are in the range [-16:15], plus the special value -17, which disables
 oom-killing that process altogether.
 

######################################################################################################################
The /sys filesystem (sysfs) contains files that provide information about devices: whether it's powered on, the vendor name and model, what bus the device is plugged into, etc. It's of interest to applications that manage devices.

The /dev filesystem contains files that allow programs to access the devices themselves: write data to a serial port, read a hard disk, etc. It's of interest to applications that access devices.

A metaphor is that /sys provides access to the packaging, while /dev provides access to the content of the box.

In addition to /proc, the kernel also exports information to another virtual file system called sysfs. sysfs is used by programs 
such as udev to access device and device driver information. The creation of sysfs helped clean up the proc file system because 
much of the hardware information has been moved from proc to sysfs.

The sysfs file system is mounted on /sys. The top-level directories are shown. Following is a brief description of some of these 
directories:

/sys/block
This directory contains entries for each block device in the system. Symbolic links point to the physical device that the device
maps to in the physical device tree. For example, attributes for the sda disks reside in the following directory:

Use udev (and or define and publish some major & minor device numbers, like for mknod). See makedev(3)

Application programs want to access physical devices in /dev/ (not in /sys/). Data to/from a device go usually thru /dev/ char or 
block devices. Metadata and configuration can go thru sysfs

'/dev' contains a list of device files (pseudo-devices, block devices, device nodes, queues, etc). The whole directory is nowadays 
managed automatically by a system resource called 'udev'. Everything under /dev communicates directly with the kernel.

'/sys' (part of sysfs) contains a directory tree of the devices grouped by category as well as an interface to a variety of device
properties such as bus settings, bus speeds, device IDs, device names, firmware info, manufacturer data, etc.

The important thing is that '/dev' is a real filesystem (disk based) whereas '/sys' is a virtual filesystem (RAM based).

'/sys/class' is used to group all devices that are similar (by class) so you'll find all memory stuff under '/sys/class/mem',
sensor stuff under '/sys/class/thermal', network stuff under '/sys/class/net' and so on.


#################################################################################################################

All process when spawned, they are assigned a priority based on a numeric value called as “nice value“. The priority of a process 
denotes how much processor time allocated to that process. There are 40 niceness values, with –20 being the highest and +19 the 
lowest. Most system-started processes use the default niceness of 0. If the niceness value is high number like 19 the task will be
set to the lowest priority and the CPU will process it whenever it gets a chance. The default nice value is zero. A child process
inherits the niceness of its calling process in calculating its priority.


# ps -elf
At this point you are probably wondering how you can set your own priority levels on processes. To change the priority when issuing a new command you do

# nice -n [nice value] [command]
For example to run the yum update command with nice value of +10 which gives it less priority over other processes. This makes sure that yum update does not load the system more.

# nice -n 10 yum update
Setting Priority of currently running process
To change the priority of an existing process use the renice command :

# renice [nice value] -p [process id]
For Example to change the priority of the currently running process (with pid 390) to 15.

# renice 15 -p 390
390: old priority 0, new priority 15

For example, you can have below entries for user and group respectively.

# vi /etc/security/limits.conf
user01 hard priority -10
@group01 hard priority -10
This would add priority to all the applications running under user ‘user01’ or group ‘group01’ priority set to ‘-10’

Understanding the Linux Kernel Scheduler
A kernel scheduler is a unit of the kernel that determines the most suitable process out of all runnable processes to execute next;
it allocates processor time between the runnable processes on a system. A runnable process is one which is waiting only for CPU time, 
it’s ready to be executed.

The scheduler forms the core of multitasking in Linux, using a priority-based scheduling algorithm to choose between the runnable
processes in the system. It ranks processes based on the most deserving as well as the need for CPU time.

ps -eo pid,ppid,ni,comm

109

Nice value is a user-space and priority PR is the process's actual priority that use by Linux kernel. In linux system priorities are
0 to 139 in which 0 to 99 for real time and 100 to 139 for users. nice value range is -20 to +19 where -20 is highest, 0 default 
and +19 is lowest. relation between nice value and priority is :

PR = 20 + NI
so , the value of PR = 20 + (-20 to +19) is 0 to 39 that maps 100 to 139.

According to top manual:

PR -- Priority The scheduling priority of the task. If you see 'rt' in this field, it means the task is running under 'real time' 
scheduling priority.

NI is nice value of task.

NI -- Nice Value The nice value of the task. A negative nice value means higher priority, whereas a positive nice value means lower
priority.Zero in this field simply means priority will not be adjusted in determining a task's dispatch-ability


############################################################################################################

ulimit -v, it's a shell builtin, but it should do what you want.

I use that in init scripts sometimes:

ulimit -v 128k
command
ulimit -v unlimited
It seems however, that you want ways of manipulating the maximum allocatable memory while the program is running


If you need to change the limit for a running process, there's no utility for that. You have to get the process to execute the setrlimit system call. This can often be done with a debugger, although it doesn't always work reliably. Here's how you might do this with gdb (untested; 9 is the value of RLIMIT_AS on Linux):

gdb -n -pid $pid -batch -x /dev/stdin <<EOF
call setrlimit(9, {409600, -1})
detach
quit
EOF


If you're using systemd, you can set some additional options in a .service file   MemoryLimit=50M


Vss: called VSZ in the ps command and VIRT in top, is the total amount of memory mapped by a process. It is the sum of all the regions
shown in /proc/<PID>/map. This number is of limited interest, since only part of the virtual memory is committed to physical memory 
at any one time.

Rss: called RSS in ps and RES in top, is the sum of memory that is mapped to physical pages of memory. This gets closer to the actual
memory budget of the process, but there is a problem, if you add up the Rss of all the processes, you will get an overestimate the
memory in use because some pages will be shared.


Using smem to check memory usage per process
In 2009, Matt Mackall began looking at the problem of accounting for shared pages in process memory measurement and added two new 
metrics called the unique set size or Uss, and the proportional set size or Pss

Uss: This is the amount of memory that is committed to physical memory and is unique to a process; it is not shared with any other.
It is the amount of memory that would be freed if the process were to terminate.
Pss: This splits the accounting of shared pages that are committed to physical memory between all the processes that have them mapped.
For example, if an area of library code is 12 pages long and is shared by six processes, each will accumulate two pages in Pss.
Thus, if you add the Pss numbers for all processes, you will get the actual amount of memory being used by those processes. 
In other words, Pss is the number we have been looking for.
 

The information is available in /proc/<PID>/smaps, which contains additional information for each of the mappings shown
in /proc/<PID>/maps. Here is one section from such a file which provides information about the mapping for the libc code segment:


# cat /proc/31768/smaps | grep -i pss |  awk '{Total+=$2} END {print Total/1024" MB"}'
56.4102 MB         --> number we are looking for 

# cat /proc/31768/smaps | grep -i rss |  awk '{Total+=$2} END {print Total/1024" MB"}'
58.7109 MB




##################################################################################################################



systemctl -t help   shows service/mount/socket/swap etc

[Unit]
description=
after=
before=

[Service/socker/mount]
ExecStart=
ExecStop=

[Install]
WantedBy=multi-user.target


systemctl show sshd   to show all the configurable parameters for the service

systemctl list-dependencies NetworkManager.service --reverse

conflicts
mount vs umount
network vs NetworkManager
iptables vs firewall
ntp vs chrony

Need to mask to ensure conflicting services are not started together

systemctl --type=target --all   but the ones with AllowIsolate=yes in target file  set can only be isolated

yum group list   then yum group install "server with gui"

kernel = heart and allowing users to interact with hardware  while initramfs = mini fs mounted during boot with kernel 
modules required for the rest of the boot process  eg LVM & SCSI which are not supported by kernel by default  

ls -l /boot/grub2/i386-pc   shows the modules that are available to grub2     they determine what you can do from grub2 
boot loader   can see them in insmod section of grub.cfg     remove rhgb and quiet for verbose logging


/run for the run time config while /etc for mainual and /usr/lib for system 



Just use /sys.

Example. I want to find the driver for my Ethernet card:

$ sudo lspci
...
02:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168B PCI Express Gigabit Ethernet controller (rev 01)
$ find /sys | grep drivers.*02:00
/sys/bus/pci/drivers/r8169/0000:02:00.0

That is r8169.

udevadm monitor    systemd-udevd    /etc/udev/rules.d/  
lspci -k  to look for just bus and no modules   need to check with vendor for open source support before tainting kernel

each phase of hardware probing is completed with creation of file in /sys


#####################################################################################################

[root@server ~]# df -h /mnt1
Filesystem      Size  Used Avail Use% Mounted on
/dev/sdb1       248M  199M   38M  85% /mnt1
[root@server ~]# mount | grep -i /mnt1
/dev/sdb1 on /mnt1 type ext2 (rw,relatime,seclabel,stripe=8192)

[root@server ~]# umount /dev/sdb1
[root@server ~]# tune2fs -j /dev/sdb1
tune2fs 1.42.9 (28-Dec-2013)
Creating journal inode: done
[root@server ~]# mount /dev/sdb1 /mnt1
[root@server ~]# mount | grep /mnt1
/dev/sdb1 on /mnt1 type ext3 (rw,relatime,seclabel,stripe=8192,data=ordered)
------------------------------------------------------------------------------------------------------------------
[root@server ~]# umount /dev/sdb1
[root@server ~]# tune2fs -O ^has_journal /dev/sdb1
tune2fs 1.42.9 (28-Dec-2013)
Creating journal inode: done
[root@server ~]# mount /dev/sdb1 /mnt1
[root@server ~]# mount | grep /mnt1
/dev/sdb1 on /mnt1 type ext2 (rw,relatime,seclabel,stripe=8192)

-----------------------------------------------------------------------------------------------------------------------------

[root@server ~]# umount /dev/sdb1
[root@server ~]# tune2fs -O extents,uninit_bg,dir_index /dev/sdb1
tune2fs 1.42.9 (28-Dec-2013)
Creating journal inode: done
[root@server ~]# mount /dev/sdb1 /mnt1
[root@server ~]# mount | grep /mnt1
/dev/sdb1 on /mnt1 type ext4 (rw,relatime,seclabel,stripe=8192)


Formatting disk for DBA dd if=/dev/zero of=/dev/sdm bs=1024 count=100

While a forward proxy proxies in behalf of clients ( or requesting hosts ), a reverse proxy proxies in behalf of servers
In effect, whereas a forward proxy hides the identities of clients, a reverse proxy hides the identities of servers


As the day proceeds there comes a lot of work. I am listing a few, it may differ from organization to organization.

    Commissioning and decommissioning of resources as per the need.
    Disk management
    user and access management
    build and release management
    configuration management
    troubleshooting applications issues
    scanning through endless lines of log
    having project meetings
    keep an eye on server and application health
    keeping in check the various components of your infrastructure ensuring the maximum uptime.
    
    
    etup Linux OS Virtual/on-premises Server as per the requirement from Dev team.

    Set repository and Install packages and update to its stable version without losing any data.
    Create, Delete and modify user and groups
    Must be comfortable with CLI, and perform a major task using CLI.
    Manage file system permissions for users and groups and apply system policy.
    Must be aware of all system configuration file and keep a backup of it.
    File sharing for Windows and Linux ( samba, NFS )
    Should be able to work remotely without GUI and understating of SSH.
    Must have good knowledge of shell scripts, without shell scripting you can’t be a good system admin.
    Keep a record of changes and able to solve the problem quickly.
    
    ######################################################################################################
    
    
Hard Mount Vs Soft Mount:


A hard mount using some kind of network file system (nfs or fuse) can (sometimes) block forever while trying to re-establish a broken connection. This means, 
every process trying to access that mount goes into disk sleep (D) until the device is available again or the system is rebooted.

Disk sleep can not be interrupted or killed. Its like the zombie of zombie processes.

In short, do not use hard mounts for network file systems, ever. You want the file system to fail (immediately, to processes using syscalls) if I/O is 
not possible. Otherwise, the memory that they claim may as well be leaked if the FS fails.

==========================================================================================================================================================================
  
Using NFS protocol, the NFS client can mount the filesystem existing on a NFS server, just like a local filesystem. For example, you will be able to mount
 “/home” directory of host.server.com to your client machine as follows: 

# mount host.server.com:/home /mymountpoint 

The directory “/mymountpoint” should be created in your machine to hold the NFS partition. 
Hard mount or Soft mount options define how the NFS client should handle NFS server crash/failure.
Hard Mount:
A Hard mount is generally used for block resources like a local disk or SAN. When a NFS filesystem mount is a Hard mount, an NFS request affecting any part 
of the mounted resource is issued repeatedly until the request is satisfied (for example, the server crashes and comes back up later). Once the server is back 
online, the program will continue to execute undisturbed from the state where it was during server crash. We can use the mount option “intr” which allows
 NFS requests to be interrupted if the server goes down or cannot be reached. Hence the recommended settings are hard and intr options.
Advantages:
In case of connection loss all NFS clients freeze until NFS server comes back online. Hence no loss of data.
Data integrity and messaging is assured.
Disadvantage:
There could be a performance impact with the constant connection.
Command to hard mount the directory /home from the remote machine host.server.com on the mount-point /mymountpoint. rw – for resource mounted to be
 read-write and intr – for keyboard interrupt enabled.  

mount -o rw,hard,intr host.server.com/home /mymountpoint
Soft Mount:
A Soft mount is usually used for network file protocols like NFS or CIFS. When a NFS filesystem mount is a soft mount, a program or application requests a 
file from the NFS filesystem, NFS client daemons will try to retrieve the data from the NFS server. NFS tries repeatedly to contact the server until either:
A connection is established
The NFS retry threshold is met
The nfstimeout value is reached
Control returns to the calling program if one of these events occur. 
But, if it doesn’t get any response from the NFS server (due to any crash, time out or failure of NFS server), the NFS client will report an error to the 
process on the client machine requesting the file access, then quits.
Advantage:
The advantage of this mechanism is “fast responsiveness” as it doesn’t wait for the NFS server to respond.
If the NFS server is unavailable, the kernel will time out the I/O operation after a pre-configured period of time.
Disadvantage:
The disadvantage is that if your NFS driver caches data and the soft mount times out, your application may not know which writes to the NFS volumes were 
actually committed to disk.
Data corruption or loss of data.
Command to soft mount from remote machine host.server.com on the mount-point /mymountpoint 

mount -o rw,soft host.server.com/home /mymountpoint 

To check what kind of a mount is present on the system currently: 
[usero1@Linux01 ~]$ nfsstat -m 

/home from vrouter:/home
Flags: rw,relatime,vers=4.1,rsize=262144,wsize=262144,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr= 10.0.0.1,local_lock=none,addr=10.0.0.2 
/mnt/test from svm-data-lif1:/vol_unix
Flags: rw,relatime,vers=4.0,rsize=65536,wsize=65536,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr= 10.0.0.1,local_lock=none,addr=10.0.0.2 
 

hard mounts and "intr" (interruptible) is a good compromise (for kernels before 2.6.25, see comment by Ryan Horrisberger) . The application is not fooled 
about successful writes, yet you can kill them if something clogs up the tubes.


#########################################################################################################################################

ps -eo pid,state,comm | sort -k2 | head
20675 R ps
 6433 S acpid
  549 S aio/0
  550 S aio/1

  
  ==========================================================================================================================
  
  Getting Rid of Zombie Processes
You can’t kill zombie processes as you can kill normal processes with the SIGKILL signal — zombie processes are already dead. Bear in mind that you don’t 
need to get rid of zombie processes unless you have a large amount on your system – a few zombies are harmless. However, there are a few ways you can get
 rid of zombie processes.

One way is by sending the SIGCHLD signal to the parent process. This signal tells the parent process to execute the wait() system call and clean up 
its zombie children. Send the signal with the kill command, replacing pid in the command below with the parent process’s PID:

kill -s SIGCHLD pid

However, if the parent process isn’t programmed properly and is ignoring SIGCHLD signals, this won’t help. You’ll have to kill or close the zombies’ 
parent process. When the process that created the zombies ends, init inherits the zombie processes and becomes their new parent. 
(init is the first process started on Linux at boot and is assigned PID 1.) init periodically executes the wait() system call to clean up its zombie children,
 so init will make short work of the zombies. You can restart the parent process after closing it.

If a parent process continues to create zombies, it should be fixed so that it properly calls wait() to reap its zombie children. 
File a bug report if a program on your system keeps creating zombies.

top command lists total count of all these states in its output header
Tasks: 325 total,   1 running, 323 sleeping,   1 stopped,   0 zombie



Process state : Running

Most healthy state of all. It indicates process is active and serving its requests. Process is properly getting system resources (especially CPU) to perform
 its operations. Running process is process which is being served by CPU currently. It can be identified by state flag R in ps or top output.

Runnable state is when process has got all the system resources to perform its operation except CPU. This means process is ready to go once CPU is free.
 Runnable processes are also flagged with state flag R

Process state : Sleeping

Sleeping process is the one who awaits for resources to run. Since its on waiting stand, it gives up CPU and goes to sleep mode. Once its required resource 
is free, it gets placed in scheduler queue for CPU to execute. There are two types of sleep modes : Interruptible and Uninterruptible

Interruptible sleep mode

In this mode process awaits for particular time slot or specific event to occur. If those conditions occur, then process will come out of sleep mode.
 These processes are shown with state S in ps or top output.

Uninterruptible sleep mode

Process in this sleep mode gets its timeout value before going to sleep. Once the timeout sets off, it awakes. Or it awakes when waited-upon resources 
becomes available for it. It can be identified by state D in outputs.

Process state : Stopped

Process ends or terminates when they receive kill signal or they enter exit status. At this moment, process gives up all the occupied resources but does 
not release entry in process table. Instead it sends signal about termination to its parent process. This helps parent process to decide if child is exited 
successfully or not. Once SIGCHLD received by parent process, it takes action and release child process entry in process table.

Process state : Zombie

As explained above, while exiting process sends SIGCHLD to parent. During the time between sending signal to parent and then parent clearing out process slot 
in process table, process enters zombie mode. Process can stay in zombie mode if its parent died before it release child process’s slot in process table. 
It can be identified with Z in outputs.


So complete life cycle of process can be circle as –
•Spawn
•Waiting
•Runnable
•Running
•Stopped
•Zombie
•Removed from process table

######################################################################################################################################


Yes there are several overheads involved in installing a program in a computer Like the following.

Is the program compatible with my computer architecture.
Is that program compatible with my operating system version.
Does all the programs and libraries required to run a certain program there in the system
Will the newly installed program conflict with an already installed program
 

An installer or program manager, must handle those overhead by itself by not harassing the user
------------------------------------------------------------------------------------------------------------------------------------------------------

yum history
yum history list all
yum history info httpd
yum history summary httpd
yum history info 15
# yum history package-list httpd
OR
# yum history package-info httpd

This is how the sub-commands above work: If we have 5 transactions: V, W, X, Y and Z, where packages where installed respectively.
# yum history undo 2    #will remove package W
# yum history redo 2    #will  reinstall package W
# yum history rollback 2    #will remove packages from X, Y, and Z.

yum remove firefox
yum list openssh kernel
yum search vsftpd --> regex match searching 
yum info firefox
yum list | less --> to list all packages
yum provides /etc/httpd/conf/httpd.conf   --> package providing specific file
yum check-update  --> how many of the INSTALLED packs have update
yum groupinstall 'MySQL Database'
yum groupupdate 'DNS Name Server'
yum repolist --> to list all ENABLED repos
yum repolist all  --> to list all ENABLED & DISABLED repos
yum --enablerepo=epel install phpmyadmin --> to install form a particular repo 
yum clean all    --> clean yum cache


In Yum 3.2.22, which come with Red Hat Enterprise Linux 5.4, there is a downgrade option. This will downgrade a package to the previously highest version 
or you can specify the whole version and release number for the package to downgrade. For example:
yum downgrade vsftpd-2.0.5-12.el5 

yum deplist httpd | grep dependency

yum deplist bind | awk '/provider:/ {print $2}' | sort -u | xargs yum -y install

-------------------------------------------------------
[ian@attic-f21 ~]$ rpm -q gcc-gfortran
gcc-gfortran-4.9.2-6.fc21.x86_64
[ian@attic-f21 ~]$ yum deplist $(rpm -q gcc-gfortran)
Loaded plugins: langpacks
package: gcc-gfortran.x86_64 4.9.2-6.fc21
  dependency: /bin/sh
   provider: bash.x86_64 4.3.39-1.fc21
  dependency: /sbin/install-info
   provider: info.x86_64 5.2-5.fc21
  dependency: gcc = 4.9.2-6.fc21
   provider: gcc.x86_64 4.9.2-6.fc21
  dependency: ld-linux-x86-64.so.2()(64bit)
   provider: glibc.x86_64 2.20-8.fc21
   
   -----------------------------------------------------------------------
  
yum list installed    --> to list INSTALLED
yum list available    --> to list all AVAILABLE
yum whatprovides '/usr/bin/whois'
which repoquery
yum whatprovides /usr/bin/repoquery
repoquery -a --installed 
rpm -q --fileprovide mplayer
rpm -q --requires mplayer

rpm -qR gcc-gfortran     ---> R for requires 

user@my-pc:~$ rpm -qp --requires proj1-1.0-1.x86_64.rpm
libtest1.so()(64bit)

user@my-pc:~$ rpm -qp --provides libtest1-1.0-1.x86_64.rpm
libtest1.so()(64bit)

rpm -vK /usr/local/depot/adm/linux/epel-release-latest-5.noarch.rpm   ------> to check the key status of rpm 


$rpm -V python-inotify-0.9.4-4.el7.noarch        ----> to check whether any change has occured sice install. 
No o/p => fine 
~$rpm -vV python-inotify-0.9.4-4.el7.noarch
.........    /usr/bin/pyinotify
.........    /usr/lib/python2.7/site-packages/pyinotify-0.9.4-py2.7.egg-info
.........    /usr/lib/python2.7/site-packages/pyinotify.py
.........    /usr/lib/python2.7/site-packages/pyinotify.pyc
.........    /usr/lib/python2.7/site-packages/pyinotify.pyo
.........    /usr/share/doc/python-inotify-0.9.4
.........  d /usr/share/doc/python-inotify-0.9.4/ACKS
.........  d /usr/share/doc/python-inotify-0.9.4/COPYING
.........  d /usr/share/doc/python-inotify-0.9.4/README.md



=============================================================================================================================================================


So if RPM is already there, why was YUM made?
RPM and YUM are completely two different things. RPM is the package manager tool which installs the package. YUM is a repository 
management tool 
which will fetch the appropriate package for your particular version of Linux(along with all other required packages).

Repositories is an organized collection of packages that YUM uses. YUM can use these repositories to fetch the correct and exact
version of a particular
 package compatible for your system. Previously before YUM(or before the existence of such repository management tools), the user 
 had to fetch the rpm package 
 for installation, and if a dependency problem arises, the user had to fetch those dependencies from internet or some other sources.

YUM will contain the URL's(Uniform Resource Locators) of different repositories in its configuration files. You can in fact update 
all the installed 
applications on your system, with the help of a single YUM command(yum will fetch different packages from appropriate different 
repositories.)

deplist	Produces a list of all dependencies and what packages provide those dependencies for the given packages. As of 3.2.30 it 
now just shows the latest 
version of each package that matches (this can be changed by using --showduplicates) and it only shows the newest providers 
(which can be changed by using --verbose).


You can block kernel updated by adding:
exclude=kernel,kernel-devel,kernel-firmware,kernel-headers
to /etc/yum.conf. Comment out the line when you want to get an update


==========================================================================================================================================================

yum --showduplicates --disableexcludes=all list \*gitlab-ce\*

If you just downloaded a package and want the same kind of information, you can get this using the -p option (for package file)
on your query along
 with specifying the package file name (as used for installing the package).

[ian@attic-f21 ~]$ #Query vim packages
[ian@attic-f21 ~]$ rpm -qp .rpm
vim-common-7.4.475-2.fc21.x86_64
vim-enhanced-7.4.475-2.fc21.x86_64
[ian@attic-f21 ~]$ #Query vim configuration files
[ian@attic-f21 ~]$ rpm -qpc .rpm
/etc/vimrc
/etc/profile.d/vim.csh
/etc/profile.d/vim.sh



============================================================================================================================================================================

Verifying an installed package
Like checking the integrity of an rpm, you can also check the integrity of your installed files using rpm -V. This step makes 
sure that the files
 haven’t been modified since they were installed from the rpm. As shown in Listing 21, there is no output from this command if
 the package is still good,
 but you can add the -v option to get much more detailed output.

Listing 21. Verifying the installed vim-common package

[ian@attic-f21 ~]$ rpm -V vim-common

Let’s become root and corrupt our vim-common installation by deleting /usr/bin/xxd and replacing 
/usr/share/vim/vim74/syntax/bindzone.vim with 
/bin/bash. Let’s try the verification again. The results are shown in Listing 22.

Listing 22. Tampering with the vim-common package

root@attic-f21 ~rpm -qf /usr/bin/xxd /usr/share/vim/vim74/syntax/bindzone.vim
vim-common-7.4.475-2.fc21.x86_64
vim-common-7.4.475-2.fc21.x86_64
root@attic-f21 ~rm /usr/bin/xxd
rm: remove regular file ‘/usr/bin/xxd’? y
root@attic-f21 ~cp /bin/bash /usr/share/vim/vim74/syntax/bindzone.vim
cp: overwrite ‘/usr/share/vim/vim74/syntax/bindzone.vim’? y
root@attic-f21 ~rpm -V vim-common
missing     /usr/bin/xxd
S.5....T.    /usr/share/vim/vim74/syntax/bindzone.vim

This output shows us that the /usr/share/vim/vim74/syntax/bindzone.vim file fails MD5 sum, file size, and mtime tests. One way to 
solve the problem would 
be to remove the package and then reinstall it, but there are other packages that depend on vim-common and that are installed and 
still OK.
 The solution is to forcibly reinstall it using the --force option of rpm, or the reinstall function of yum. Listing 23 shows how 
 to reinstall with yum,
 and then verify that the package is now okay and the deleted file has been restored.

Listing 23. Reinstalling the vim-common package

root@attic-f21 ~yum reinstall vim-common
Loaded plugins: langpacks
Resolving Dependencies
--> Running transaction check
---> Package vim-common.x86_64 2:7.4.475-2.fc21 will be reinstalled
--> Finished Dependency Resolution

Dependencies Resolved

================================================================================
 Package           Arch          Version                    Repository     Size
================================================================================
Reinstalling:
 vim-common        x86_64        2:7.4.475-2.fc21           fedora        5.9 M

Transaction Summary
================================================================================
Reinstall  1 Package

Total download size: 5.9 M
Installed size: 21 M
Is this ok [y/d/N]: y
Downloading packages:
vim-common-7.4.475-2.fc21.x86_64.rpm                        | 5.9 MB  00:03     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction (shutdown inhibited)
  Installing : 2:vim-common-7.4.475-2.fc21.x86_64                           1/1 
  Verifying  : 2:vim-common-7.4.475-2.fc21.x86_64                           1/1 

Installed:
  vim-common.x86_64 2:7.4.475-2.fc21                                            

Complete!
root@attic-f21 ~rpm -V vim-common
root@attic-f21 ~ls /usr/bin/xxd
/usr/bin/xxd
Show more
If you need more force
Usually the package management system keeps your packages in order. However, if you manage to delete some file that is an important 
part of a package—and 
reinstalling the package without removing does not fix the problem—then you may need to remove the package before reinstalling. For 
such a case,
 you probably want to delete the existing copy and reinstall it, without needing to uninstall and reinstall all the packages that 
 depend on it. 
 For this, you can use the rpm command’s --nodeps option to bypass dependency checking when you remove a package. Listing 24 shows 
 how this might work if 
 you accidentally removed the /usr/bin/xxd file, which is part of the vim-common package, as we did earlier.

Listing 24. Updating packages with rpm

root@attic-f21 ~rm /usr/bin/xxd
rm: remove regular file ‘/usr/bin/xxd’? y
root@attic-f21 ~#Oops! we needed that file
root@attic-f21 ~rpm -Fvh vim-common-7.4.475-2.fc21.x86_64.rpm 
root@attic-f21 ~ls /usr/bin/xxd
ls: cannot access /usr/bin/xxd: No such file or directory
root@attic-f21 ~#Oh! Freshening the package didn't replace the missing file
root@attic-f21 ~rpm -e vim-common
error: Failed dependencies:
    vim-common = 2:7.4.475-2.fc21 is needed by (installed) vim-enhanced-2:7.4.475-2.fc21.x86_64
root@attic-f21 ~#Can't remove vim-common because vim-enhanced needs it
root@attic-f21 ~rpm -e --nodeps vim-common
warning: file /usr/bin/xxd: remove failed: No such file or directory
root@attic-f21 ~#Bypassing the dependency check allowed removal
root@attic-f21 ~#No surprise that /usr/bin/xxd was not found
root@attic-f21 ~#Update (or install) vim-common again
root@attic-f21 ~rpm -Uvh vim-common-7.4.475-2.fc21.x86_64.rpm
Preparing...                          #################################100%Updating / installing...
   1:vim-common-2:7.4.475-2.fc21      #################################100%root@attic-f21 ~ls /usr/bin/xxd
/usr/bin/xxd
root@attic-f21 ~#And /usr/bin/xxd is back
Show more
So now you have some approaches to updating or repairing if accidents happen and the ordinary update process fails. Note that you 
can also bypass 
dependency checking when installing an RPM, but this not usually a good idea.

=================================================================================================================================================


Downloading RPMs from repositories
Although yum will automatically retrieve packages from repositories, you may want to download RPMs and save them, perhaps to install
them on a 
non-networked system, or to examine their contents, or for some other reason. You can use the yumdownloader command to do this as
shown in Listing 25. 
In our case, the gcc-gfortran.x86_64 package is already installed, so there are no additional packages to download.

Listing 25. Downloading the gcc-gfortran package

[ian@attic-f21 ~]$ yumdownloader --resolve gcc-gfortran.x86_64
Loaded plugins: langpacks
--> Running transaction check
---> Package gcc-gfortran.x86_64 0:4.9.2-6.fc21 will be reinstalled
--> Finished Dependency Resolution
gcc-gfortran-4.9.2-6.fc21.x86_64.rpm                       | 7.7 MB  00:04     

The --resolve option of yumdownloader will cause other required packages to be downloaded, too. To illustrate this I 686-download 
also shows the files 
downloaded using the --resolve option when we download gcc-gfortran. Note that we did not specify an architecture (x86_64 or i686), 
so the default download 
is for the i686 version.


================================================================================================================================================================

If you can’t find a particular RPM through your system tools, a good Internet resource for locating RPMs is the Rpmfind.Net server.


###########################################################################################################################################
ip aliasing_research
---------------------------------------------------------------------------------------------------------------

Resolution
There are two ways to add another IP address to an interface:

The old way creates a new virtual interface named in the style of ethX:Y where X and Y are numbers, for instance, eth0:1. Each interface has one IP address.
 It appears in ifconfig output as an ordinary interface and in ip output with a label attached.

The new way adds a secondary address to the main interface. So, instead of having one interface per IP address, it is possible to add many addresses to the 
real interface. However, ifconfig tool is too old and can't see the additional IP addresses, so in this case the ip tool must be used instead. This is the
 preferred way nowadays.



Now adding IP address 172.31.33.1/255.255.255.0 to that file

Raw
# cat /etc/sysconfig/network-scripts/ifcfg-eth0 
TYPE=Ethernet
BOOTPROTO=none
IPADDR=192.168.122.2
PREFIX=24
DNS1=192.168.122.1
DOMAIN=lan
DEFROUTE=yes
IPV4_FAILURE_FATAL=yes
IPV6INIT=no
NAME=eth0
UUID=8dc6deb4-4868-46a1-bc3b-0a8fb55fxxxx
ONBOOT=yes
LAST_CONNECT=1380032766
IPADDR2=172.31.33.1
NETMASK2=255.255.255.0


Then bring down and up the interface to make the changes take effect:

Raw
# ifdown eth0; ifup eth0
Verifying:

Raw
# ip address list dev eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:XX:XX:XX brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.2/24 brd 192.168.122.255 scope global eth0
    inet 172.31.33.1/24 brd 172.31.33.255 scope global eth0
    inet6 fe80::5054:ff:fexx:xxxx/64 scope link 
       valid_lft forever preferred_lft forever

    
    
    #########################################################################################################
    
    Steps to install the s3fs on Centos/RHEL/Amazon Linux
Login to EC2 Linux Server via SSH

Install required dependencies for Centos/RHEL/Amazon Linux
$ sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel
1
	
$ sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel

#For Ubuntu Systems

$ sudo apt-get install build-essential libcurl4-openssl-dev libxml2-dev mime-support
1
	
$ sudo apt-get install build-essential libcurl4-openssl-dev libxml2-dev mime-support

Now compile s3fs and install it with below command:

# git clone https://github.com/s3fs-fuse/s3fs-fuse.git
# cd s3fs-fuse
# ./autogen.sh
# ./configure
# make
# sudo make install
1
2
3
4
5
6
	
# git clone https://github.com/s3fs-fuse/s3fs-fuse.git
# cd s3fs-fuse
# ./autogen.sh
# ./configure
# make
# sudo make install

Step to mount S3 Bucket to Linux File System

You need root privileges, so login with root user or switch to root user:
$ sudo su
1
	
$ sudo su

Create IAM user you need access-key and secret key for s3fs, store key details in /etc/passwd-s3fs

#echo <access-key-id>:<secret-access-key> > /etc/passwd-s3fs

#chmod 600 /etc/passwd-s3fs
1
2
3
	
#echo <access-key-id>:<secret-access-key> > /etc/passwd-s3fs
 
#chmod 600 /etc/passwd-s3fs

(Replace <access-key-id> and <secret-access-key> with the actual IAM user keys)
Create Dir to mount s3bucket:

For example:
#mkdir /mnt/<test-bucket>
Add entry to fstab to mount the bucket:
echo s3fs#<s3-bucket> /mnt/<test-bucket> fuse _netdev,rw,nosuid,nodev,allow_other,nonempty 0 0 >> /etc/fstab
1
2
3
	
#mkdir /mnt/<test-bucket>
Add entry to fstab to mount the bucket:
echo s3fs#<s3-bucket> /mnt/<test-bucket> fuse _netdev,rw,nosuid,nodev,allow_other,nonempty 0 0 >> /etc/fstab

(Replace the leading <s3-bucket> with your AWS s3 bucket name and the /mnt/<test-bucket> with the mount point which you have created)

Use-mention command to mount the partition which has entered in fstab and here we just now added AWS S3 bucket details to mount on Linux:
# mount -a
1
	
# mount -a

Verify the S3 bucket mounted on Linux server

# df -h
1
	
# df -h

In the command output, you can see the bucket name which you have to add an entry in /etc/fstab.



When a child exits, some process must wait on it to get its exit code. That exit code is stored in the process table until this happens.
The act of reading that exit code is called "reaping" the child. Between the time a child exits and is reaped, it is called a zombie. 
(The whole nomenclature is a bit gruesome when you think about it; I recommend not thinking about it too much.)

Zombies only occupy space in the process table. They take no memory or CPU. However, the process table is a finite resource, and
excessive zombies can fill it, meaning that no other processes can launch. Beyond that, they are bothersome clutter, and should be
strongly avoided.

If a process exits with children still running (and doesn't kill its children; the metaphor continues to be bizarre), those 
children are orphans. Orphaned children are immediately "adopted" by init (actually, I think most people call this "reparenting,"
but "adoption" seems to carry the metaphor better). An orphan is just a process. It will use whatever resources it uses. It is 
reasonable to say that it is not an "orphan" at all since it has a parent, but I've heard them called that often.

init automatically reaps its children (adopted or otherwise). So if you exit without cleaning up your children, then they will not
become zombies (at least not for more than a moment).

But long-lived zombies exist. What are they? They're the former children of an existing process that hasn't reaped them. The
process may be hung. Or it may be poorly written and forgets to reap its children. Or maybe it's overloaded and hasn't gotten 
around to it. Or whatever. But for some reason, the parent process continues to exist (so they aren't orphans), and they haven't 
been waited on, so they live on as zombies in the process table.

So if you see zombies for longer than a moment, then it means that there is something wrong with the parent process, and something 
should be done to improve that program



################################################################################################################

The zombie (child died - parent alive)isn't occupying any significant memory or resources, it's (effectively) only an exit status 
waiting to be delivered.
An orphan is a live, running process just like any other -- it just has a peculiar name (parent died - child alive)


If no parent waiting (did not invoke wait()) process is a zombie

If parent terminated without invoking wait , process is an orphan



  ####################################################################################################################
  
  netstat -nr | egrep -iv '^kernel|^destination' | awk '{print $1 "\t\t" $2 "\t\t" $3 "\t\t" $8}' | sort -n -k1
ifconfig -a | awk -F ':' '/inet/ {print $2 ":"} /Link/ {print $1}' | awk '{ print $1}' | egrep -iv 'lo|127.0.0.1'
df -Ph | awk '{print $1 "\t\t" $6}' | egrep -iv '^filesystem|tmpfs' | sort -k1
pvs | awk '{print $1 "\t" $2 "\t" $3}' | grep -iv 'pv' | sort -k1
vgs | awk '{print $1 "\t" $2 "\t" $3 "\t" $4}'| sort -k1
lvs | awk '{print $1 "\t" $2}'| sort -k1
cat /etc/redhat-release
cat /etc/hosts
cat /etc/resolv.conf
sysctl -p
cat /etc/fstab

#################################################################################################################

/sbin/iptables -A INPUT -i eth0 -p tcp --dport 80 -j ACCEPT
/sbin/iptables -A INPUT -i eth0 -p tcp --dport 8080 -j ACCEPT
/sbin/iptables -A PREROUTING -t nat -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080
iptables -L
iptables-save



/etc/security/limits.conf
# Oracle-Grid Preinstall setting for nproc hard limit is 16384
oracle   hard   nproc    16384
grid   hard   nproc    16384


#############################################################################################################################


full_manual_sox_report_generation

Create a file named /tmp/man_sosreport.sh, copy the script given below and save it in the /tmp/man_sosreport.sh file.

Raw
#!/bin/bash
export LANG=C

# If this script hangs, un-comment the below two entries and note the command that the script hangs on.  Then comment out that command and re-run the script.
# set -x
# set -o verbose

[[ -d /tmp/sosreport ]] && rm -rf /tmp/sosreport
mkdir /tmp/sosreport && cd /tmp/sosreport && mkdir -p  var/log etc/lvm etc/sysconfig network storage sos_commands/networking

echo -e "Gathering system information..."
uname -n &> hostname  
cp -a /etc/redhat-release  ./etc/ 2>> error_log
uptime &> uptime 

echo -e "Gathering application information..."
chkconfig --list &> chkconfig
top -bn1 &> top_bn1
service --status-all &> service_status_all
date &> date
ps auxww &> ps_auxww
ps -elf &> ps_-elf
rpm -qa --last &> rpm-qa
echo -e "Running 'rpm -Va'. This may take a moment."
rpm -Va &> rpm-Va

echo -e "Gathering memory information..."
free -m &> free  
vmstat 1 10 &> vmstat

echo -e "Gathering network information..."
ifconfig &> ./network/ifconfig  
netstat -s &>./network/netstat_-s
netstat -agn &> ./network/netstat_-agn
netstat -neopa &> ./network/netstat_-neopa
route -n &> ./network/route_-n
for i in $(ls /etc/sysconfig/network-scripts/{ifcfg,route,rule}-*) ; do echo -e "$i\n----------------------------------"; cat $i;echo " ";  done &> ./sos_commands/networking/ifcfg-files    
for i in $(ifconfig | grep "^[a-z]" | cut -f 1 -d " "); do echo -e "$i\n-------------------------" ; ethtool $i; ethtool -k $i; ethtool -S $i; ethtool -i $i;echo -e "\n" ; done &> ./sos_commands/networking/ethtool.out
cp /etc/sysconfig/network ./sos_commands/networking/ 2>> error_log
cp /etc/sysconfig/network-scripts/ifcfg-* ./sos_commands/networking/ 2>> error_log
cp /etc/sysconfig/network-scripts/route-* ./sos_commands/networking/ 2>> error_log
cat /proc/net/bonding/bond* &> ./sos_commands/networking/proc-net-bonding-bond 2>> error_log
iptables --list --line-numbers &> ./sos_commands/networking/iptables_--list_--line-numbers
ip route show table all &> ./sos_commands/networking/ip_route_show_table_all
ip link &> ./sos_commands/networking/ip_link

echo -e "Gathering Storage/Filesystem information..."
df -l &> df
fdisk -l &> fdisk
parted -l &> parted
cp -a /etc/fstab  ./etc/ 2>> error_log
cp -a /etc/lvm/lvm.conf ./etc/lvm/ 2>> error_log
cp -a /etc/lvm/backup/ ./etc/lvm/ 2>> error_log
cp -a /etc/lvm/archive/ ./etc/lvm/ 2>> error_log
cp -a /etc/multipath.conf ./etc/ 2>> error_log
cat /proc/mounts &> mount  
iostat -tkx 1 10 &> iostat_-tkx_1_10
parted -l &> storage/parted_-l
vgdisplay -v &> storage/vgdisplay
lvdisplay &> storage/lvdisplay
pvdisplay &> storage/pvdisplay
pvs -a -v &> storage/pvs
vgs -v &> storage/vgs
lvs -o +devices &> storage/lvs
multipath -v4 -ll &> storage/multipath_ll
pvscan -vvvv &> storage/pvscan
vgscan -vvvv &> storage/vgscan
lvscan -vvvv &> storage/lvscan
lsblk &> storage/lsblk
lsblk -t &> storage/lsblk_t
dmsetup info -C &> storage/dmsetup_info_c
dmsetup status &>  storage/dmsetup_status 
dmsetup table &>  storage/dmsetup_table
ls -lahR /dev &> storage/dev

echo -e "Gathering kernel information..."
cp -a /etc/security/limits.conf ./etc/ 2>> error_log
cp -a /etc/sysctl.conf ./etc/ 2>> error_log
ulimit -a &> ulimit
cat /proc/slabinfo &> slabinfo
cat /proc/interrupts &> interrupts 
cat /proc/iomem &> iomem
cat /proc/ioports &> ioports
slabtop -o &> slabtop_-o
uname -a &> uname
sysctl -a &> sysctl_-a
lsmod &> lsmod
cp -a /etc/modprobe.conf ./etc/ 2>> error_log
cp -a  /etc/sysconfig/* ./etc/sysconfig/ 2>> error_log
for MOD in `lsmod | grep -v "Used by"| awk '{ print $1 }'`; do modinfo  $MOD 2>&1 >> modinfo; done;
ipcs -a &> ipcs_-a
ipcs -s | awk '/^0x/ {print $2}' | while read semid; do ipcs -s -i $semid; done &> ipcs_-s_verbose
sar -A &> sar_-A
cp -a /var/log/dmesg dmesg 2>> error_log
dmesg &> dmesg_now

echo -e "Gathering hardware information..."
dmidecode &> dmidecode
lspci -vvv &> lspci_-vvv
lspci &> lspci
cat /proc/meminfo &> meminfo  
cat /proc/cpuinfo &> cpuinfo

echo -e "Gathering kump information..."
cp -a /etc/kdump.conf ./etc/ 2>> error_log
ls -laR /var/crash &> ls-lar-var-crash
ls -1 /var/crash | while read n; do mkdir -p var/crash/${n}; cp -a /var/crash/${n}/vmcore-dmesg* var/crash/${n}/ 2>> error_log; done

echo -e "Gathering container related information..."
mkdir container
rpm -q podman || alias podman="docker"
podman ps &> container/ps
podman image list &> container/image_list
podman ps | awk '$1!="CONTAINER" {print $1}' | while read id; do podman inspect $id &> container/inspect_${id}; done

echo -e "Gathering logs..."
cp -a /var/log/{containers*,message*,secure*,boot*,cron*,yum*,Xorg*,sa,rhsm,audit,dmesg} ./var/log/ 2>> error_log
cp -a /etc/*syslog.conf ./etc/ 2>> error_log

echo -e "Compressing files..."
tar -cjf /tmp/sosreport.tar.bz2 ./

echo -e "Script complete."


#############################################################################################################################

Re-reading partition table

All the following commands did not make kernel reread partition :

partprobe /dev/sda (warning : kernel failed to reread ....)
hdparm -z /dev/sda (BLKRRPART failed : device or resource busy)
blockdev -rereadpt /dev/sda (BLKRRPART failed : device or resource busy)
sfdisk -R /dev/sda (BLKRRPART failed : device or resource busy)


blockdev --rereadpt /dev/sdX       


Rehat Suggested Fix

Issue
Added a new partition to a disk that have some existing partitions already mounted. Can the new partition be used without reboot in RHEL6?
New partition created with fdisk command is not visible in the OS.
Resolution
partprobe was commonly used in RHEL 5 to inform the OS of partition table changes on the disk. In RHEL 6, it will only trigger the OS 
to update the partitions on a disk that none of its partitions are in use (e.g. mounted). If any partition on a disk is in use, partprobe 
will not trigger the OS to update partitions in the system because it is considered unsafe in some situations.

So in general we would suggest:

Unmount all the partitions of the disk before modifying the partition table on the disk, and then run partprobe to update the partitions in system.
If this is not possible (e.g. the mounted partition is a system partition), reboot the system after modifying the partition table. 
The partitions information will be re-read after reboot.
If a new partition was added and none of the existing partitions were modified, consider using the partx command to update the system
 partition table. Do note that the partx command does not do much checking between the new and the existing partition table in 
 the system and assumes the user knows what they are doing. So it can corrupt the data on disk if the existing partitions are modified or
 the partition table is not set correctly. So use at one's own risk.

For example, a partition #1 is an existing partition and a new partition #2 is already added in /dev/sdb by fdisk. Here we 
use partx -v -a /dev/sdb to add the new partition to the system:

Raw
# ls /dev/sdb*  
/dev/sdb  /dev/sdb1  
List the partition table of disk:

Raw
# partx -l /dev/sdb
# 1:        63-   505007 (   504945 sectors,    258 MB)  
# 2:    505008-  1010015 (   505008 sectors,    258 MB)  
# 3:         0-       -1 (        0 sectors,      0 MB)  
# 4:         0-       -1 (        0 sectors,      0 MB)  
Read disk and try to add all partitions to the system:

Raw
# partx -v -a /dev/sdb                                         
device /dev/sdb: start 0 size 2097152  
gpt: 0 slices  
dos: 4 slices  
# 1:        63-   505007 (   504945 sectors,    258 MB)  
# 2:    505008-  1010015 (   505008 sectors,    258 MB)  
# 3:         0-       -1 (        0 sectors,      0 MB)  
# 4:         0-       -1 (        0 sectors,      0 MB)  
BLKPG: Device or resource busy
error adding partition 1
(These last 2 lines are normal in this case because partition 1 is already added in the system before partition 2 is added)

Check that we have device nodes for /dev/sdb itself and the partitions on it:

Raw
# ls /dev/sdb*  
/dev/sdb  /dev/sdb1  /dev/sdb2

#################################################################################################################################

In the sosreport, I'm not seeing an audit rule that is monitoring the /etc/resolv.conf file. 

Could you please add the following rule in the /etc/audit/rules.d/audit.rules file? 

-w /etc/resolv.conf -p wa -k resolv_alert

Once you add the rule, please restart the service with the following command. 

 #service auditd restart

Then, please reboot the server and see if the resolv.conf gets replaced. 

We will need a fresh sosreport once you notice the issue again.

###############################################################################################################################

Configure X windows in Linux server
Before Login into GUI of any linux server we need to install basic package into the specific server ,Those package are:
X Windows System
Desktop
code to install
yum groupinstall "X Windows System" "Desktop" -y
Edit the below conf file
vi /etc/gdm/custom.conf
place this entry on xdmcp block “Enable=true”

Change the ID in the inittab /etc/inittab as “5”
Do
gdm &
Do
init 5
Use MoboXterm to login to GUI of Linux server configured
select using server name ,write the server name,hit connect .



############################################################################################################################

When should I start to worry?
A healthy Linux system with more than enough memory will, after running for a while, show the following expected and harmless behavior:

free memory is close to 0
used memory is close to total
available memory (or "free + buffers/cache") has enough room (let's say, 20%+ of total)
swap used does not change
Warning signs of a genuine low memory situation that you may want to look into:

available memory (or "free + buffers/cache") is close to zero
swap used increases or fluctuates
dmesg | grep oom-killer shows the OutOfMemory-killer at work


#################################################################################################################################


Background
The routing table contains information used by the network stack to decide where each packet should be sent next. This decision may result in the packet 
being delivered locally, or forwarded to another machine, or rejected as unroutable. For packets that are forwarded the routing table is concerned only 
with selection of the next ‘hop’, at which point another routing decision will be made.

Within the routing table is a list of routes. Each route specifies an address range (expressed as a network address and a netmask), the interface to which
 packets matching that address range should be sent, and (optionally) the address of a gateway machine.

The route that is used for a given packet is the most specific one with an address range that matches the ultimate destination address. If the route has a 
gateway then the packet is forwarded to that gateway, otherwise it is forwarded directly to its final destination.

If the network address and netmask of a route are both zero then it will match packets with any destination address, but only if there is no other route 
that matches. Most routing tables have such an entry, which is known as the ‘default route’.

There are a number of different ways in which routes may be created:

Whenever you bring up an interface using ifup, a route is automatically created for the address range that is directly reachable through that interface.
You can specify further static routes to be created when an interface is brought up.
If an interface uses an automatic configuration protocol such as DHCP and is able to obtain a suitable gateway address then it may create a default route 
to that gateway.
If an interface uses a point-to-point protocol such as PPP then it may automatically create a default route to the far end of the link.
If your network is sufficiently complex to make use of a routing protocol such as BGP or RIP then these can automatically add any number of dynamic routes 
to the table.
Symptoms
A routing table error may result in:

traffic being forwarded to the wrong interface, or
traffic not being forwarded when it should have been.
Note that either of these issues could equally indicate a problem with iptables, and a failure to forward could indicate that forwarding has not been enabled.

In the unlikely event that you are using the NAT capabilities of iproute2 (as opposed to NAT in iptables) then a configuration error could additionally 
cause traffic to be given the wrong source or destination address. You should also be aware that iproute2 has the ability to support multiple 
routing tables, therefore the route taken by a packet may depend on more than just its destination address.

Scenario
Suppose that a machine has been configured to act as a boundary router between a local area network (connected to interface eth0 with the
 address 192.168.0.1/24) and the public Internet (connected to interface ppp0 with the address 203.0.113.144/32). The default gateway is 203.0.113.1. 
 Because the local area network uses a private address range, iptables on the boundary router has been configured to SNAT them to its public IP address.

In order to test this configuration you have attempted to ping a machine on the public Internet (198.51.100.1) from a machine on the local area 
network (192.168.0.2), but this has failed.

Investigation
Strategy
There are two possibilities to consider:

that the routing table has not been populated with the routes that you intended, or
that the table has been populated as intended but the routes do not have the desired effect.
It is usually worth quickly checking the first point before launching into any detailed investigation of the second.

Remember that routes created using the route and ip route commands are not persistent. Even if you are certain that the routing table was correct at some 
point in the past, that configuration will not necessarily survive a reboot.

Inspecting the routing table
The traditional method for viewing the content of the routing table is by means of the route command, however this predates iproute2 and the information 
it displays may be incomplete. The preferred method is to use the ip route command:

ip route show
The response is a list of routes, which for the scenario described above should be similar to:

203.0.113.1 dev ppp0  proto kernel  scope link  src 203.0.113.144
192.168.0.0/24 dev eth0  proto kernel  scope link  src 192.168.0.1
default via 203.0.113.1 dev ppp0
These routes specify that:

traffic to 203.0.113.1 (the gateway to the public Internet) should be forwarded directly to its destination through ppp0;
traffic to anywhere within 192.168.0.0/24 (the local area network) should be forwarded directly to its destination through eth0; and
traffic to anywhere else should be forwarded through ppp0 to the gateway at 203.0.113.1.
There is little benefit in attempting to predict the effect of the table on individual packets because this can be done more quickly and reliably using 
the automated method below. However you should aim to ensure that there are no spurious or missing routes, and that the destination addresses and device 
names are correct.

Testing the routing table
It is possible to test the effect of the routing table on a particular packet using the command ip route get. You need to know where the packet came from 
(IP address and interface name) and where it is supposed to go to (IP address).

For example, the outbound test packet in the scenario above originates from 192.168.0.2 and is received by the router on eth0. It is destined for 198.51.100.1.
 You can inquire how such packets are routed using the command:

ip route get to 198.51.100.1 from 192.168.0.2 iif eth0
which should give a response of the form:

198.51.100.1 from 192.168.0.2 via 203.0.113.1 dev ppp0  src 192.168.0.1
    cache <src-direct>  mtu 1500 advmss 1460 hoplimit 64 iif eth0
The main points to check are that the IP address of the next hop is correct (203.0.113.1 in this case) and that the packet is being sent onwards through 
the appropriate interface (ppp0).

You should also check the reverse path, because you will get no response to a ping unless it can be routed in both directions:

ip route get to 192.168.0.2 from 198.51.100.1 iif ppp0
This should give a response of the form:

192.168.0.2 from 198.51.100.1 dev eth0  src 203.0.113.144
    cache  mtu 1500 advmss 1460 hoplimit 64 iif ppp0
Points to note:

The ip route get command is sensitive to whether forwarding has been enabled, so if it returns the error ‘No route to host’ then that may be the reason.
Routing is performed before any change to the source address by the iptables SNAT target. You should therefore use the real address from which traffic 
originates, not the address from which it purports to originate after NATting.
Routing is performed after any change to the destination address by the iptables DNAT target. You should therefore use the address to which the traffic
 is redirected after NATting, not the address to which it was originally sent.
Variations
The route command
Though deprecated, the route command it adequate for inspecting most of the configurations that are likely to be encountered in practice:

route -n
The -n option causes IP addresses to be displayed numerically. This is usually preferable to converting them into names.

For the scenario described above the output should look similar to the following:

Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
203.0.113.1     0.0.0.0         255.255.255.255 UH    0      0        0 ppp0
192.168.0.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0
0.0.0.0         203.0.113.1     0.0.0.0         UG    0      0        0 ppp0



=====================================================================================================================

To find the port or universal addresses of a remote program, the client sends an RPC to well-known port 111 of the server’s host. If the server 
listening on port 111 (rpcbind or portmapper) has an entry for the remote program, it provides the port number or universal addresses in a 
return RPC. The client then contacts the remote program by sending an RPC to the port number or universal addresses provided.

#############################################################################################################################################
#######################################
Process flow : client--> rpc to 111 of server --> if server has entry for reqeusted port ---> port/universal address to client --> 
client sends rpc --> provided port on server 

Either portmap/rpcbind runs not both of them simultaneously

netstat -tulpn | grep -w 111
tcp        0      0 0.0.0.0:111                 0.0.0.0:*                   LISTEN      4929/portmap
udp        0      0 0.0.0.0:111                 0.0.0.0:*                               4929/portmap


The rpcbind[1] utility maps RPC services to the ports on which they listen. RPC processes notify rpcbind when they start, registering the ports 
they are listening on and the RPC program numbers they expect to serve. The client system then contacts rpcbind on the server with a particular 
RPC program number. The rpcbind service redirects the client to the proper port number so it can communicate with the requested service.
Because RPC-based services rely on rpcbind to make all connections with incoming client requests, rpcbind must be available before any of these 
services start.


Because rpcbind[1] provides coordination between RPC services and the port numbers used to communicate with them, it is useful to view the status
 of current RPC services using rpcbind when troubleshooting. The rpcinfo command shows each RPC-based service with port numbers, an RPC program 
 number, a version number, and an IP protocol type (TCP or UDP).

rpcinfo -p 

If one of the NFS services does not start up correctly, rpcbind will be unable to map RPC requests from clients for that service to the correct 
port. In many cases, if NFS is not present in rpcinfo output, restarting NFS causes the service to correctly register with rpcbind and begin working.

#############################################################################################################################

sar Command examples
In addition, you can use various command options with sar to display additional information or control the program’s operations. A few of the more common 
options are listed in the following table.

Option	Displayed statstics
-A	All
-b	I/O
-B	swap
-d	I/O for each block device on the system
-n ALL	All network statistics. Instead of ALL, you can use DEV (device), NFS (network file system), SOCK (sockets), and additional options to
 display subsets of network data.
-q	Processor queue (cache)
-r	Memory and swap
-u	CPU (the default when no options are specified)
-v	Kernel
-W	Simplified swap statistics (pages swapped in and out per second only)


Raw
# sar -r -s 10:00:00 -e 14:00:00 -f /var/log/sa07 -o /tmp/mem.txt



#################################################################################################################################

Statically linked executables: Contain all the library functions that they need to execute; all library functions are linked into the executable. 
They are complete programs that do not depend on external libraries to run. One advantage of statically linked programs is that they work without your 
needing to install prerequisites.
Dynamically linked executables: Much smaller programs; they are incomplete in the sense that they require functions from external shared
 libraries to run. Besides being smaller, dynamic linking permits a package to specify prerequisite libraries without needing to include the 
 libraries in the package. By using dynamic linking, many running programs can share one copy of a library rather than occupying memory with
 many copies of the same code. For these reasons, most programs today use dynamic linking.

 ldd /usr/bin/ln
        linux-vdso.so.1 =>  (0x00007fff7e1db000)
        libc.so.6 => /lib64/libc.so.6 (0x00007feb5db8e000)
        /lib64/ld-linux-x86-64.so.2 (0x00007feb5df5b000)

		
 ldd /sbin/sln
        not a dynamic executable


Apart from knowing that a statically linked program is likely to be large, how can you tell whether a program is statically linked? And if it 
is dynamically linked, how do you know what libraries it needs? The ldd command can answer both questions. If you are running a system such as 
Debian or Ubuntu, you probably don’t have the sln executable, so you might also want to check the /sbin/ldconfig executable.



strace -f -tt -s 10000 -o /var/tmp/mhl.out -p 29738 -p 30096 -p 30099 -p 30102

When you start a service, it creates a "lock" file to indicate that the service is running. This helps avoid multiple instances of the service. 
When you stop a service, this lock file is removed.

When a running service crashes, the lock file exists but the process no longer exists. Thus, the message.

Look at the two areas /var/run/*.pid and /var/lock/subsys/*. These are expected to agree with each other. That is, if the (emtpy file) 
lockfile /var/lock/subsys/crond exists, then the first line of the file /var/run/crond.pid is expected to contain the PID of the process 
running for this service. If no such process is running, then something is wrong. If a process is indeed running (as you see) but it is not that PID, 
then something is probably confused.

#################################################################################################################################


What is  Sticky Bit
The sticky bit works on directories only. If a user wants to create or delete a file/directory in some directory, he needs write permission on 
that directory. The write permission on a directory gives a user the privilege to create a file as well as the privilege to remove it.

The /tmp directory is the directory for temporary files/directories. This directory has all the rights on all the three levels because all the 
users need to create/delete their temporary files. But as the users have write permission on this directory, they can delete any file in this directory.
The permissions of that file do not have any effect on deletion.

But with sticky bit set on a directory, anyone can create a file/directory in it, but can delete his own files only. Files owned by other users cannot
 be deleted.


 
How to view and set stickybit
You could notice t tag added to /tmp directory and it means bit is set for this directory.

$ ls -ld /tmp/
drwxrwxrwt 4 root root 4096 Aug 19 02:29 /tmp/
In linux sticky bit can be set with chmod command. You can use +t tag to add and -t tag to delete sticky bit.

$ chmod o-t dir1
$ ls -l
total 8
drwxr-xr-x 2 root root 4096 Aug 19 03:08 dir1
$ chmod o+t dir1
$ ls -l
total 8
drwxr-xr-t 2 root root 4096 Aug 19 03:08 dir1
Alternatively,

$ chmod 1777 dir1/
$ ls -l
total 8
drwxrwxrwt 2 root root 4096 Aug 19 03:08 dir1
Note: In Unix flavored OS, sticky bit has a different purpose but we are not discussing it here.

What is SUID Bit and How to set it
When an executable file runs, it runs under the ownership of the user who has executed it. It means that when student user runs ls command, 
then the corresponding process will run under the ownership of student. The SUID bit, also known as Set User ID bit, overwrites this behavior.
 If SUID bit is set on a program, then that program will run as the owner of that file, irrespective of who is executing it.

The passwd command in Linux has SUID bit set.

$ ls -l /usr/bin/passwd
-rwsr-xr-x 1 root root 23420 Aug 3 2010 /usr/bin/passwd
This can be seen in the third field of permissions. The 's' in place of 'x' indicates that SUID bit is set. With SUID bit set, 
when a normal user (say student) runs the passwd command, the command runs with the ownership of 'root', and not as student, 
because root is the owner of this file. This behavior is required because the passwords are stored in the /etc/shadow file, 
which has no permission on group or other level.

$ ls -l /etc/shadow
-r-------- 1 root root 1027 Jul 13 21:56 /etc/shadow
You need to understand that all users cannot be given read or write permission on this file for security reasons; otherwise, 
they will read/change the passwords of other users. So this causes a problem that if the users don't have permission on this file,
 then how will they change their own passwords? So SUID bit solves the problem. The passwd command has SUID bit set, so when normal 
 users execute this command, they run it with the ownership of root, i.e. the owner of passwd command.

How to Set and unset SUID bit
This is to be noted that SUID bit works on files only. To set the SUID bit on a file, use the chmod command as follows

$ ls -l
total 8
-rwxr--r-- 1 root root 104 Aug 19 01:26 hello.sh
$ chmod u+s hello.sh
$ ls -l
total 8
-rwsr--r-- 1 root root 104 Aug 19 01:26 hello.sh
The numeric method for changing permissions can also be used. Suppose if the normal permissions for a file are 744, then with SUID bit set, these will become 4744. SUID bit has value 4.

$ ls -l
total 8
-rwxr--r-- 1 root root 104 Aug 19 01:26 hello.sh
$ chmod 4744 hello.sh
$ ls -l
total 8
-rwsr--r-- 1 root root 104 Aug 19 01:26 hello.sh
How SGID Bit work on file and directory
Unlike SUID bit, SGID bit works on both files and directories, but it has a different meaning in both cases.

On files:

For file, it has similar meaning as the SUID bit, i.e. when any user executes a file with SGID bit set on it, it will always be executed with the group 
ownership of that file, irrespective of who is running it. For example, the file /sbin/netreport has SGID bit set, which can be seen in the 
's' instead of 'x' in group permissions.

$ ls -l /sbin/netreport
-rwxr-sr-x 1 root root 6020 Oct 13 2010 /sbin/netreport
This file has group ownership of root group. So when a user (say student) executes it, then the corresponding process will not have group ownership of
 student, but that of root group.

On directories:

Now let’s talk about SGID on directories. SGID on directories is used for creating collaborative directories. To understand SGID bit on directories, 
consider the following scenario:

Suppose three users jack, jones and jenny are working together on some project. All of them belong to a group named javaproject. 
For the course of the project, they need to share all the files related to the project. All of them must be able to see each other's file. 
This can be done simply by providing read permission on group level. Further, suppose that the directory used for the project is "/javaproject".

Here, a problem arises that when a file is created, it belongs to the primary group of the user who created the file. So, when different users 
create their files in this directory, those files will not have group ownership of javaproject group.

What we do for our problem is that we set the group of /javaproject directory to be javaproject group, 
and set the SGID bit set on it. When SGID bit is set on a directory, all the files and directory created within it has the group ownership of the
 group associated with that directory. It means that after setting SGID bit on /javaproject directory, all the files and directories being 
 created in this directory will have the group ownership of "javaproject" group. Moreover, this behavior is recursive, 
 i.e. the directories created in this directory will have SGID bit set as well. The permissions for the new directory will also be same as that of
 /javaproject directory.

The SGID bit can be set with chmod command as follows:

$ ls -ld /javaproject
drwxrwxr-x 2 root javaproject 4096 Aug 19 02:33 /javaproject
$ chmod g+s /javaproject
$ ls -ld /javaproject
drwxrwsr-x 2 root javaproject 4096 Aug 19 02:33 /javaproject
Now when jones user creates a file in this directory, it is created under the group ownership of javaproject group.

$ touch /javaproject/jones1.txt
$ mkdir /javaproject/jones1dir
$ ls -l /javaproject/
total 12
drwxrwsr-x 2 jones javaproject 4096 Aug 19 02:38 jones1dir
-rw-rw-r-- 1 jones javaproject 0 Aug 19 02:37 jones1.txt
The numeric value corresponding to SGID bit is 2. So to add SGID bit numerically, use the following command:

$ ls -ld /shared/
drwxrwxr-x 2 root adm 4096 Aug 19 02:47 /shared/
$ chmod 2775 /shared/
$ ls -ld /shared/
drwxrwsr-x 2 root adm 4096 Aug 19 02:47 /shared/

######################################################################################################################################

When you want to improve the performance or the characteristics of your server, you need to set the kernel runtime parameters.

In order to do this, you’ve got three ways:

through the /proc filesystem,
with the sysctl command,
through the /etc/sysctl.conf file.
The /proc Filesystem
To get the value of a kernel runtime parameter (here /proc/sys/net/ipv4/ip_forward used for allowing a host to act as an router), type:

# cat /proc/sys/net/ipv4/ip_forward
To set the value of the same parameter, type:

# echo 1 > /proc/sys/net/ipv4/ip_forward
Note: 1 is used for On and 0 for off.

This change is instantaneously active but doesn’t persist a reboot. You have to write it into the /etc/rc.d/rc.local file to get it re-applied 
at each boot. See below for a better solution.

The sysctl Command
With the sysctl command, you can get all the available kernel runtime parameters with their current value.

# sysctl -a | grep vm.swappiness
vm.swappiness = 30
But you can also set a kernel runtime parameter with the -w option.

# sysctl -w vm.swappiness=20
vm.swappiness = 20
Still like the previous method, this change is instantaneously active but doesn’t persist a reboot. You have to write it into the 
/etc/rc.d/rc.local file to get it re-applied at each boot. See below for a better solution.

The /etc/sysctl.conf File
To permanently store kernel runtime parameters, you need to write them into the /etc/sysctl.conf file.

For example, edit the /etc/sysctl.conf file and paste the following line:

# allow IPv4 forwarding
net.ipv4.ip_forward = 1
Caution: Comments are only allowed on a separate line and not at the end of a line!
Note: It is not a coincidence if the net.ipv4.ip_forward kernel runtime parameter name matches the /proc/sys/net/ipv4/ip_forward path name.

Note: There is also a directory called /etc/sysctl.d. You can create files with .conf extension inside that will be read at boot.

Then, you need to apply the change:

# sysctl -p
Caution: Only changes in the /etc/sysctl.conf file will be applied. If you created some files in the /etc/sysctl.d directory, 
you will need either to type sysctl -p /etc/sysctl.d/file.conf (if file.conf is the file where kernel runtime parameters are stored)
 or sysctl –system to get the associated changes applied.

Many kernel runtime parameters can be set this way. Here are only a few examples:

# don't respond to a ping
net.ipv4.icmp_echo_ignore_all = 1
# don't respond to a ping to the broadcast address
net.ipv4.icmp_echo_ignore_broadcasts = 1
# disable IPv6 for all network interfaces
net.ipv6.conf.all.disable_ipv6 = 1
Note: As seen before, the sysctl -a command gets all the kernel runtime parameters with their current value. By redirecting the output to a file
, this is also a good way to back up your configuration before any change.

Default kernel runtime configuration is located in the /usr/lib/sysctl.d directory.

To know the order the files are read and apply the various settings, type: # sysctl –system.

Caution: Kernel runtime parameters set in the /etc/sysctl.conf file can be overrided by the application of a tuned profile (see this example).


#######################################################################################################################################

IP address change Solaris
-----------------------------------------------------

u can edit the /etc/hostname.interfacename and place the ip address . Also u need to change the ip address in /etc/hosts file . /etc/inet/ipnodes all also sometimes play a tricky part if you are using a IPV6. 

Without rebooting u can use like this, 
ifconfig e1000g0 plumb 
ifconfig e100g0 192.168.1.1 netmask 255.255.255.0 up



Changing the IP Address in Solaris 10 U3
18 Dec 2006 · Filed in Explanation
Changing the IP address of a system running Solaris (Solaris 10, specifically) is different than a lot of other operating systems out there. Really, all you have to do is just edit a few files and then take the interface down and back up again. However, there seems to be a “gotcha” with Solaris 10. (I don’t know how far back this procedure goes—it is unclear to me if this is new to Solaris 10, or if it extends back to Solaris 8 or 9.)

Most of the sites out there I found indicated that you only needed to edit the /etc/hosts file (which is actually just a symlink to /etc/inet/hosts) and place the new IP address of the server in that file. Since I wasn’t changing the hostname or default gateway, there was no need to edit /etc/hostname.pcn0 (the hostname file for the only interface in the system), /etc/nodename, or /etc/defaultrouter. So I edited the /etc/inet/hosts file, rebooted the server, and expected to see the new IP address show up on the network.

It didn’t work. A bit more research indicates that in Solaris 10, the operating system uses /etc/inet/ipnodes over /etc/inet/hosts. This is a bit odd since ipnodes is only supposed to be used for IPv6, and I know that I specifically disabled IPv6 in this installation. Some additional targeted searches I performed, however, showed that this was indeed the case even if IPv6 is disabled.

Upon editing /etc/inet/ipnodes and rebooting the server, the IP address change took effect.

So, if you need to change the IP address of a server running Solaris 10, change the following files:

/etc/inet/hosts
/etc/inet/ipnodes
Upon a reboot, the server will now have the new IP address.

http://www.machine-unix.com/changing-the-ip-address-of-a-solaris-10-host-without-a-reboot/







Changing the IP address of a Solaris 10 host without a reboot
23 Mar, 2009 Solaris 0
It is possible to change the IP address of a solaris 10 host by modifying couple of files.  Here is how:

The ip address of the system is placed under /etc/hosts. This file actually is symbolically linked to /etc/inet/hosts in Solaris 10. In my system, I have the following in my /etc/hosts

# cat /etc/hosts

::1     localhost
127.0.0.1       localhost
192.168.1.122   opensolaris     loghost

So I am going to change IP address from 192.168.1.122 to 192.168.1.123. Edit the /etc/hosts file with a text editor of your liking:

#vi /etc/hosts

::1     localhost
127.0.0.1       localhost
192.168.1.123   opensolaris     loghost

Since this file is sym linked to /etc/inet/hosts, you should be able to see the change:

# cat /etc/inet/hosts

::1     localhost
127.0.0.1       localhost
192.168.1.123   opensolaris     loghost

At this point you should also change the IP address in /etc/inet/ipnodes .

Now use ifconfig command to make the change effect immediately.

# ifconfig pcn0 192.168.1.123 netmask 255.255.255.0

# ifconfig -a

lo0: flags=2001000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv4,VIRTUAL> mtu 8232 index 1

inet 127.0.0.1 netmask ff000000

pcn0: flags=201000843<UP,BROADCAST,RUNNING,MULTICAST,IPv4,CoS> mtu 1500 index 2

inet 192.168.1.123 netmask ffffff00 broadcast 192.168.1.255

If you want to have changes take effect across reboots, you can restart the network/physical service:

# svcadm restart network/physical

That’s it…Now you’ve changed the actual IP address without even rebooting….



===================================================================================================================


tcpdump -i eth0 src 192.168.0.2


 tcpdump -i eth0 dst 50.116.66.139
 
 tcpdump -i eth0 host 10.10.1.1
 
 Capture packets for particular destination IP and Port
The packets will have source and destination IP and port numbers. Using tcpdump we can apply filters on source or destination IP and port number.
 The following command captures packets flows in eth0, with a particular destination ip and port number 22.


$ tcpdump -w xpackets.pcap -i eth0 dst 10.181.140.216 and port 22



Capture TCP communication packets between two hosts
If two different process from two different machines are communicating through tcp protocol, we can capture those packets using tcpdump as shown below.

$tcpdump -w comm.pcap -i eth0 dst 16.181.170.246 and port 22


tcpdump src 10.170.101.64 and dst host 10.44.43.181 -w /tmp/tcpdump_314to627_new.log


########################################################################################################################


TCP/IP network model
Before we get into routing, it helps to understand a little bit about how packets find their way to the correct host on a network. The TCP/IP network
 model defines a five layer stack that describes the mechanisms necessary to move data packets from one host to another, whether that host is on the 
 local network or halfway around the world. Each of the layers in the following description of this model is numbered and also contain the names of the 
 data units that are handled by that layer.

5. Application layer: Message This layer consists of the connection protocols required for various network applications to communicate,
 such as HTTP, DHCP, SSH, FTP, SMTP, IMAP, and others. When you request a web page from a remote web site, a connection request is sent to 
 the web server and the response is sent back to your host at this layer and then your browser displays the web page in its window.

4. Transport layer: TCP segment. The transport layer provides end-to-end data transport and flow management services that are independent of 
the data and types of protocols being transported. It uses ports such as 80 for HTTP and 25 for SMTP to make connections between the sending host 
and the remote host.

3. Internet layer: Packet. Packet routing is performed on the Internet layer. This layer is responsible for routing packets across two or more 
different networks in order to reach their final destination. This layer uses IP Addresses and the routing table to determine which device to send 
the packets to next. If sent to a router, each router is responsible for sending the data packets only to the next router in the series and not for 
mapping out the entire route from the local host to the target host. The Internet layer is mostly about routers talking to routers in order to determine
 the next router in the chain.

2. Data Link layer: Frame. The Link layer manages the direct connections between hardware hosts on a single, local, logical, physical network.
 This layer uses the Media Access Control (MAC) addresses embedded in the Network Interface Cards (NICs) to identify the physical devices attached 
 to the local network. This layer cannot access hosts that are not on the local network.

1. Physical layer: Bits. This is the hardware layer and consists of the NICs and the physical Ethernet cable as well as the hardware level protocols 
used to transmit individual bits that make up the data frames between any two hosts or other network nodes that are locally connected.

A simple example
So what does that look like when a host is actually sending data on the network using the TCP/IP network model? Here is my own made-up description
 of how data are moved from one network to another. In this example, my computer is sending a request to a remote server for a web page.

On the application layer, the browser initiates an HTTP connection request message to the remote host, www.example.com, to send back the data
 comprising the contents of a web page. This is the message, and it includes only the IP Address of the remote web server.

The transport layer encapsulates the message containing the web page request in a TCP datagram with the IP address of the remote web
 server as the destination. Along with the original request packet, this packet now includes the source port from which the request will originate, 
 usually a very high number random port, so that the return data knows which port the browser is listening on; and the destination port on the remote 
 host, port 80 in this case.

The Internet layer encapsulates the TCP datagram in a packet that also contains both the source and destination IP addresses.

The data Link layer uses the Address Resolution Protocol (ARP) to identify the physical MAC address of the default router and encapsulates the 
Internet packet in a frame that includes both the source and destination MAC addresses.

The frame is sent over the wire, usually CAT5 or CAT6, from the NIC on the local host to the NIC on the default router.

The default router opens the datagram and determines the destination IP address. The router uses its own routing table to identify the 
IP address of the next router that will take the frame onto the next step of its journey. The router then re-encapsulates the frame in a new datagram 
that contains its own MAC as the source and the MAC address of the next router and then sends it on through the appropriate interface. 
The router performs its routing task at layer 3, the Internet layer.

Note that switches are invisible to all protocols at layers two and above, so they do not affect the transmission of data in any logical manner. 
The function of switches is merely to provide a simple means to connect multiple hosts into a single physical network via lengths of Ethernet cable.

You can use the arp [-n] command to view all of the MAC addresses that your host has stored in its arp table. These are always hosts on the local network.

##################################################################################################################

Wire Me a Virtual Nic, Would Ya?
21 Aug, 2011 Solaris 0
The idea of having a Virtual Nic in a Solaris environment is actually really cool. Virtual Nics have really nice features. It will let you have multiple IP addresses in only 1 NIC thus preventing you to use different physical NICs wired up if it is not necessary. You can find some FAQ’s related to Virtual NICs here. So why would one want to use a Virtual NIC? I am going to demonstrate one of the benefits of using a virtual nic in the following scenario:

I want to drink a Mocha and have my Networking too

So imagine that you are in your favorite coffee shop and you just ordered your Mocha. You are a happy person because you have your Mac and you installed a Virtualization software such as Virtual Box or Vmware Fusion, so that you can have multiple OS’es running in the same laptop. Among your other Linux distributions,  you have your beloved Solaris 10/11 installed and it is ready to be booted up. So you do the inevitable and boot your Solaris 10 VM, and soon you realize that you are not going to be able to reach to your system through your local host and especially if you are running your Solaris 10 in headless-mode. You can boot the OS, you can login to the OS and do interesting things through the GUI but you just can’t ssh into it because you configured your Solaris 10 system at your home with your home network information. So what do you do? Enter the virtual NIC.

Then you decide to configure a Virtual NIC so that you can access your OS in headless-mode by just playing with the network a bit. First thing you will do is to figure out what your network is, so at your coffee shop, in your MAC you just do ifconfig ( adding only relevant part here, and edited )

$ ifconfig -a

en1: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
ether 10:60:1b:50:f7:fa
inet6 6033::fe81:f4bf:f41e:bff7%en1 prefixlen 64 scopeid 0x5
inet 192.168.11.88 netmask 0xffffff00 broadcast 192.168.11.255
media: autoselect
status: active

Ok, so your IP is 192.168.11.88 and you are in 192.168.11.x network. Next thing you do through your VM GUI is to give a VirtualNIC to your Solaris system. You also do an ifconfig in your solaris VM ( Again adding the relevant parts with edits):

root@solu9# ifconfig -a

e1000g0: flags=1000843<UP,BROADCAST,RUNNING,MULTICAST,IPv4> mtu 1500 index 2
inet 192.168.1.120 netmask ffffff00 broadcast 192.168.1.255
ether 8:0:27:dc:14:ff

So your system is in 192.168.1.X network instead of 192.168.11.X network. Also notice that your physical Nic is e1000g0. Here is how you configure your VirtualNic and give it an IP address:

First you create a /etc/hostname.e1000g0:1 file and add a hostname information there so that it will be persistent across reboots.

$ cat /etc/hostname.e1000g0:1

solu9local

e1000g0:1 is a virtualnic and you also want to add this information with the IP address of your choice to your /etc/hosts file:

$ cat /etc/hosts

###########################################################################################################

> /etc/udev/rules.d/70-persistent-net.rules

cpulimit --pid ${PROCESSID} --limit 40 &

create file with fallocate/dd/head
---------------------------------------------------------------------------------

Here's how to create a 12GB ext4 disk image which you can then mount automatically upon boot:

Create the image file: fallocate -l 12G /path/to/image.img
Create the filesystem in the file: mkfs.ext4 /path/to/image.img
Mount the filesystem automatically: echo "/path/to/image.img /srv/data ext4 defaults,auto,loop 0 0" >>/etc/fstab

=========================================================================================================================================
truncate -s 5M ostechnix.txt

The another command to create a particular size file is fallocate. Please note that you can only specify file sizes in bytes using fallocate command. 
Now let us create a file of size 5MB using command:
$ fallocate -l 5242880 ostechnix.txt
$ fallocate -l $((5*1024*1024)) ostechnix.txt
fallocate -l 10G /tmp-file

$ head -c 5MB /dev/urandom > ostechnix.txt
The above command will create 5MB size file filled with random data. You can also create the file with 0s as shown below.
$ head -c 5MB /dev/zero > ostechnix.txt

$ dd if=/dev/urandom of=ostechnix.txt bs=5MB count=1
$ dd if=/dev/zero of=ostechnix.txt bs=5MB count=1
dd if=/dev/zero of=/tmp-file bs=1 count=0 seek=10G

===================================================================================================================================


This is a common question -- especially in today's environment of virtual environments. Unfortunately, the answer is not as 
straight-forward as one might assume.

dd is the obvious first choice, but dd is essentially a copy and that forces you to write every block of data (thus, initializing the 
file contents)...
 And that initialization is what takes up so much I/O time. (Want to make it take even longer? Use /dev/random instead of /dev/zero! 
 Then you'll use CPU
 as well as I/O time!) In the end though, dd is a poor choice (though essentially the default used by the VM "create" GUIs). E.g:

dd if=/dev/zero of=./gentoo_root.img bs=4k iflag=fullblock,count_bytes count=10G
truncate is another choice -- and is likely the fastest... But that is because it creates a "sparse file". Essentially, a 
sparse file is a section of disk  that has a lot of the same data, and the underlying filesystem "cheats" by not really storing all of the data, but just "pretending" 
that it's all there. Thus, when you use truncate to create a 20 GB drive for your VM, the filesystem doesn't actually allocate 20 GB, 
 but it cheats and says that there are 20 GB of zeros there, even though as little as one track on the disk may actually (really) be in use. E.g.:

 truncate -s 10G gentoo_root.img
fallocate is the final -- and best -- choice for use with VM disk allocation, because it essentially "reserves" (or "allocates" all 
of the space you're  seeking, but it doesn't bother to write anything. So, when you use fallocate to create a 20 GB virtual drive 
space, you really do get a 20 GB file  (not a "sparse file", and you won't have bothered to write anything to it -- which means 
virtually anything could be in there  -- kind of like a brand  new disk!) E.g.:

fallocate -l 10G gentoo_root.img

find /proc/*/fd -ls | grep  '(deleted)'

#####################################################################################################################################

[VMware]
# cat /sys/block/sda/device/model
Virtual disk

Emergency_vs_rescue
---------------------------------------------------------------------------------------------

emergency.target == /bin/sh 
rescue.target == single user mode /S/init 1


The emergency.target has no corresponding sysvinit runlevel, and would just boot your machine to a shell with really nothing started.

rescue.target is like the old single user or runlevel 1 from sysvinit. It 

Here is a short explanation from the developer:

In systemd, "emergency" is little more than an equivalent to init=/bin/sh on the kernel command like. i.e. you get a shell, but almost nothing else 
(except for systemd in the background which you can then ask to do more). No services are started, no mount points mounted, no sockets established, nothing. 
Just a raw, delicious shell and systemd's promise to be around if you need more. In contrast to that "rescue" is equivalent to the old runlevel 1 (or S), 
i.e. sysinit is run, everything is mounted, but no normal services are started yet.

I think emergency mode is kinda nice for debugging purposes, since it allows you to boot bit-by-bit, simply by starting in the emergency mode
 and then starting the various services and other units that are part of the early boot step-by-step. This will become particularly useful when 
 Fedora splits up sysinit into various smaller scripts which could then be started seperately and independently.

Consider it a part of our boot-up debugging tools.

###################################################################################################################################

linux_networking_research
-----------------------------------------------------------------------------------------------------

https://developer.ibm.com/tutorials/l-lpic1-109-2/

TCP/IP host configuration
A Linux system has a name which is called the host name and this name is used within the system (stand alone or connected to a network) to identify it.
 Usually, the same host name will be used to identify the system if it is part of a network. When the system is connected to a network or the Internet, 
 has a more rigorous name as part of the Domain Name System (DNS). A DNS name has two parts, the host name and a domain name. The fully qualified domain
 name (FQDN) consists of the host name followed by a period and then the domain name (for example, myhost.mydomain). Domain names usually consist of
 multiple parts separated by periods (for example, ibm.com or lpi.org).

The kernel sets the host name value during boot, usually from configuration files.


The kernel stores the currently active host name in the virtual /proc file system in the file, /proc/sys/kernel/hostname. The host name and FQDN may also
 be stored in /etc/hosts.

[root@eplnx002 y6e8jjc]# find /proc | grep hostname
/proc/sys/kernel/hostname

[root@eplnx002 y6e8jjc]# cat /proc/sys/kernel/hostname
eplnx002


Changing your host name
You can use the hostname command with root privilege to change your host name. This does not update the value in /etc/hostname. This is illustrated on 
my Fedora 29 system in Listing 2.

Listing 2. Changing host name with the hostname command

[ian@attic5-f29 ~]$ cat /etc/hostname
attic5-f29
[ian@attic5-f29 ~]$ hostname
attic5-f29
[ian@attic5-f29 ~]$ sudo hostname attic5-f29-a
[ian@attic5-f29 ~]$ hostname
attic5-f29-a
[ian@attic5-f29 ~]$ cat /etc/hostname
attic5-f29
[ian@attic5-f29 ~]$ cat /proc/sys/kernel/hostname
attic5-f29-a

Note that /etc/hostname has not been updated while the virtual /proc/sys/kernel/hostname does show the updated value. If you want to make this change 
permanent, you need to update /etc/hostname yourself. You may also need to update /etc/hosts or other files.

If your system uses the systemd system and service manager, use the hostnamectl command. The hostnamectl command has several commands to show status, 
set host names, or set other values. If used with no commands, or with the status command, it displays the current host name status as shown in Listing 3.

Listing 3. Displaying host name using the hostnamectl command

[ian@attic5-f29 ~]$ hostnamectl status
   Static hostname: attic5-f29
Transient hostname: attic5-f29-a
         Icon name: computer-desktop
           Chassis: desktop
        Machine ID: 434ef6f0139941b8bbdeb5b2950278d0
           Boot ID: 3f2201af05364f819287617d8c215ec7
  Operating System: Fedora 29 (Workstation Edition)
       CPE OS Name: cpe:/o:fedoraproject:fedora:29
            Kernel: Linux 5.0.14-200.fc29.x86_64
      Architecture: x86-64

You see that the old host name, attic5-f29, is shown as the static host name while the new name, attic5-f29-a, is shown as the transient host name. 
The hostnamectl command distinguishes a third name called the pretty name which can be a descriptive name such as Ian’s UEFI computer. Use the set-hostname
 command to set one or all of these names. If you do not specify a particular one, all three will be updated to the same new value. Listing 4 shows how to 
 set the host name to attic5-f29-b and verify that the /etc/host6name file has been updated. I also show how to set a pretty host name. The status no longer
 shows a distinct transient name as it is now the same as the static host name.

Listing 4. Setting a host name using the hostnamectl command

[ian@attic5-f29 ~]$ cat /etc/hostname
attic5-f29
[ian@attic5-f29 ~]$ sudo hostnamectl set-hostname attic5-f29-b
[ian@attic5-f29 ~]$ sudo find /etc -type f -mmin -5
/etc/hostname
[ian@attic5-f29 ~]$ cat /etc/hostname
attic5-f29-b
[ian@attic5-f29 ~]$ sudo hostnamectl --pretty set-hostname "Ian's UEFI desktop"
[ian@attic5-f29 ~]$ hostnamectl
   Static hostname: attic5-f29-b
   Pretty hostname: Ian's UEFI desktop
         Icon name: computer-desktop
           Chassis: desktop
        Machine ID: 434ef6f0139941b8bbdeb5b2950278d0
           Boot ID: 3f2201af05364f819287617d8c215ec7
  Operating System: Fedora 29 (Workstation Edition)
       CPE OS Name: cpe:/o:fedoraproject:fedora:29
            Kernel: Linux 5.0.14-200.fc29.x86_64
      Architecture: x86-64
Show less
A third way to change the host name is to use the Network Manager command line interface (nmcli) to interact with the Network Manager daemon. As with 
hostnamectl, the nmcli command has several commands within it. Use the general command with the hostname option to view or change the host name. As you might
 expect, you don’t need any authority to view the host name but you need root authority to change the host name. Listing 5 shows how to view and set the host
 name using the nmcli general command.

Listing 5. Setting a host name using the nmcli command

ian@attic5-f29 ~]$ nmcli general hostname
attic5-f29-b
[ian@attic5-f29 ~]$ sudo nmcli general hostname attic5-f29
[ian@attic5-f29 ~]$ cat /etc/hostname
[ian@attic5-f29 ~]$ hostname
attic5-f29
attic5-f29
[ian@attic5-f29 ~]$ hostnamectl
   Static hostname: attic5-f29
   Pretty hostname: Ian's UEFI desktop
Transient hostname: attic5-f29-b
         Icon name: computer-desktop
           Chassis: desktop
        Machine ID: 434ef6f0139941b8bbdeb5b2950278d0
           Boot ID: 3f2201af05364f819287617d8c215ec7
  Operating System: Fedora 29 (Workstation Edition)
       CPE OS Name: cpe:/o:fedoraproject:fedora:29
            Kernel: Linux 5.0.14-200.fc29.x86_64
      Architecture: x86-64
Show more
Note that the nmcli general command updates /etc/hostname and changes the host name as displayed by hostname and the static host name as displayed by 
hostnamectl. The transient host name as displayed by hostnamectl is not affected immediately, but will shortly change to the same as the new static host
 name. The pretty name stored by hostnamectl is not affected.



--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

If your system uses systemd, the nameserver value in resolv.conf is likely to be 127.0.0.53 which is an internal DNS stub resolver that is part of 
systemd-resolved. If you run systemd-resolved --status on a system similar to the above, you will see that the stub also connects to 192.168.1.1. 
The /etc/resolv.conf file is also likely to be a symbolic link to /run/systemd/resolve/stub-resolv.conf. If you want to create your own resolv.conf, 
you should first break this link.

There are other things you can specify in resolv.conf, including a list of domains to search for names that are not fully qualified. See the man page for 
resolv.conf for additional information.

The Name Service Switch file, /etc/nsswitch.conf, provides additional configuration, including the sources or so-called databases to use for name lookup. 
Listing 10 shows /etc/nsswitch.conf from my Ubuntu 18.04 LTS system. In this example, host names are resolved according to the specification in the hosts line. 
First, search for files (/etc/hosts), then use mdns4_minimal (multicast DNS used for searching small local networks using semantics of regular DNS searches),
 then use DNS, and finally see if the current host name matches the search. Now you see why Fedora might choose not to create an entry for the host name in
 /etc/hosts. Try dig $(hostname) to look up your host name on such a system.



------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

nmcli  

According to the information page for nmcli, Network Manager stores all network configuration as “connections”, which are collections of data (Layer2 details,
 IP addressing, etc.) that describe how to create or connect to a network. A connection is “active” when a device uses that connection’s configuration to create
 or connect to a network. There may be multiple connections that apply to a device, but only one of them can be active on that device at any given time.
 The additional connections can be used to allow quick switching between different networks and configurations.

You can show the connection status for a connection such as enp4s0 using the nmcli connection show enp4s0 command. Indeed nmcli can handle several different
 types of objects or commands as summarized in Table 1. These can be abbreviated to an unambiguous prefix, currently as short as the first letter.


--------------------------------------------------------------------------------------------------------------------------------------------------

Listing 17. Cloning and modifying a connection using nmcli

[ian@attic4-ce7 ~]$ sudo nmcli connection clone enp4s0 enp4s0new
enp4s0 (132a0ea9-e934-4961-b278-0c70dbab84a2) cloned as enp4s0new (ded070be-f416-4313-acfd-f09899c4b27a).
[ian@attic4-ce7 ~]$ sudo nmcli connection modify enp4s0new connection.autoconnect no
[ian@attic4-ce7 ~]$ sudo nmcli connection modify enp4s0new ipv4.addresses 192.168.1.67/24
[ian@attic4-ce7 ~]$ sudo nmcli connection modify enp4s0new -ipv4.dns 1
[ian@attic4-ce7 ~]$ nmcli -f \ipv4.addresses,ipv4.dns,connection.autoconnect,connection.interface-name conn show enp4s0new
[ian@attic4-ce7 ~]$ nmcli -f \
> ipv4.addresses,ipv4.dns,connection.autoconnect,connection.interface-name \
> conn show enp4s0new
ipv4.addresses:                         192.168.1.67/24
ipv4.dns:                               8.8.8.8
connection.autoconnect:                 no
connection.interface-name:              enp4s0
Show less
Listing 18 shows how to use ifup and ifdown to deactivate the original static interface and activate the cloned one.

----------------------------------------------------------------------------------------------------------------------------------------------------

# nmcli con add type ethernet con-name Myhome1 ifname enp0s3   ---> since IP not mentioned it will become a dhcp config. check ifcfg-enp0s3 file 

nmcli conn add type ethernet testnet ifname eth1
nmcli conn add type ethernet con-name testnet ifname eth1
nmcli c s
cat ifcfg-testnet
nmcli d s
nmcli c up testnet
nmcli connection delete testnet
nmcli c s

As you can see it has BOOTPROTO=dhcp, because we didn’t give any static ip address.

Hint: We can modify any connection with the “nmcli con mod“ command. However if you modify a dhcp connection and change it to static don’t forget to change its 
“ipv4.method” from “auto” to “manual”. Otherwise you will end up with two IP addresses: one from dhcp server and the static one.


nmcli con add type ethernet con-name static2 ifname enp0s3 ip4 192.168.1.50/24 gw4 192.168.1.1      ---> To add static IP


Let’s modify the last connection profile and add two dns servers.

# nmcli con mod static2 ipv4.dns “8.8.8.8 8.8.4.4”
Hint: There is something here you must pay attention: the properties for IP address and gateway have different names when you add and when you modify a 
connection. When you add connections you use “ip4” and  “gw4”, while when you modify them you use “ipv4” and “gwv4”.

Now let’s bring up this connection profile:

# nmcli con down static1 ; nmcli con up static2



For example: when you bring down a connection profile, the NetworkManager searches for another connection profile and brings it up automatically. 
(I leave it as exercise to check it). If you don’t want your connection profile to autoconnect:

# nmcli con mod static2 connection.autoconnect no
The last exercise is very usefull: you made a connection profile but you want it to be used by specific users. It’s good to classify your users!

We let only user stella to use this profile:

# nmcli con mod static2 connection.permissions stella
Hint: If you want to give permissions to more than one users, you must type user:user1,user2 without blank space between them:

# nmcli con mod static2 connection.permissions user:stella,john


-----------------------------------------------------------------------------------------------------------------------------------------------------------------


---> next  systemd-networkd   replacing network manager itself !!!

A minimal file to provide a static IPv4 address of 192.168.1.68 instead of the DHCP assigned (192.168.1.25 shown in Listing 21) might be stored in
 /etc/systemd/network/. A sample is shown in Listing 22.

Listing 22. Sample .network file for systemd-networkd

[ian@attic5-f29 ~]$ cat /etc/systemd/network/20-static-enp9s0.network
[Match]
Name=enp9s0

[Network]
Address=192.168.1.68/24
Gateway=192.168.1.1
DNS=8.8.8.8

To switch from Network Manager to systemd-networkd, use the systemctl command to first disable Network manager and then enable and start 
systemd-networkd and systemd-resolved. Note that systemd-resolved will create its own resolv.conf file under the /run/systemd directory. 
Since other system services may depend on the one in /etc, it is better to create a symbolic link to the new one.

-----------------------------------------------------------------------------------------------------------------------------


#######################################################################################################################################

systemctl list-unit-files | grep enabled will list all enabled ones.

If you want which ones are currently running, you need systemctl | grep running.

To check if a network service (here httpd) is enabled at boot, type:

# systemctl is-enabled httpd
disabled

To check if a service starts on boot, run the systemctl status command on your service and check for the “Loaded” line.

$ systemctl status httpd
httpd.service - The Apache HTTP Server
   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled)
...
The last word, either enabled or disabled, will tell you if the service starts on boot. In the case above, the Apache2 webserver “httpd”, it’s Enabled


#################################################################################################################################

1) am I correct in how they work

Yes, although ifupdown prefers ifup@<interface>.service now.

2) should you only have one of these services enabled at any given time

Generally yes, but the important bit is that only one service should manage an interface at any given time. It is quite possible (but probably not recommended) to use different services for different interfaces, e.g. use systemd-networkd for setting up tunnels while still using ifupdown to set up Ethernet.

(For example, I use NetworkManager for my general PC networking, but also have systemd-networkd to create WireGuard links, a "virtual machines" bridge, and such.)

3) once everything is setup, are they both compatible with the ip/ifconfig commands

ip – yes.

ifconfig – partially. Among other problems it has, this tool is incapable of showing multiple IPv4 addresses per interface (unless they're labelled with legacy "aliases"). This is not actually an incompatibility of networkd and ifconfig specifically; rather it's an incompatibility of modern Linux IP stack and ifconfig.

3) once everything is setup, are they both compatible with [...] the ifup and ifdown commands?

The ifup/ifdown commands are exactly the same "ifupdown" which you mentioned. You could say that networking.service just runs ifup <name> for every interface listed as 'auto'. (It's a common misconception that they're low-level tools, or abbreviations for ifconfig up, but they are not.)

Therefore, only interfaces listed in /etc/network/interfaces are compatible with ifup/ifdown.


#########################################################################################################################

Persisting the journal
By default, CentOS/RHEL 7 stores the system journal in /run/log/journal, which is stored on a tmpfs. This implies that on a reboot all stored information will be lost. If the directory /var/log/journal is present the journal will be stored there, thus enabling a persistent journal across reboots.

Enabling a persistent journal can be done by using the following steps:

1. Create the directory /var/log/journal.

# mkdir /var/log/journal
2. Set the group ownership of the new directory to systemd-journal, and the permissions to 2755.

# chown root:systemd-journal /var/log/journal
# chmod 2755 /var/log/journal
3. Inform systemd-journald that the new location should be used by sending a USR1 signal to it. A reboot will also suffice.

# killall -USR1 systemd-journald


journalctl _SYSTEMD_UNIT=sshd.service
This will display all messages generated by the sshd.service systemd unit.

# journalctl _SYSTEMD_UNIT=sshd.service

Boot Messages
Journald tracks each log to a specific system boot. To limit the logs shown to the current boot, use the -b switch.

$ journalctl -b
You can view messages from an earlier boot by passing in its offset from the current boot. For example, the previous boot has an offset of -1, the boot before that is -2, and so on. Here, we are retrieving messages from the last boot:

$ journalctl -b -1
To list the boots of the system, use the following command.

$ journalctl --list-boots


To see messages logged within a specific time window, we can use the --since and --until options. The following command shows journal messages logged within the last hour.

$ journalctl --since "1 hour ago"
To see messages logged in the last two days, the following command can be used.

$ journalctl --since "2 days ago"

journalctl -u nginx.service
The -u switch can be used multiple times to specify more than one unit source. For example, if you want to see log entries for both nginx and mysql, the following command can be used.

$ journalctl -u nginx.service -u mysql.service

this command “follows” the mysql service log.

$ journalctl -u mysql.service -f

journalctl _PID=8088
At other times, you may wish to show all of the entries logged from a specific user or group. This can be done with the _UID or _GID filters. For instance, if your web server runs under the www-data user, you can find the user ID by typing:

id -u www-data
33
Afterwards, you can use the ID that was returned to filter the journal results:

journalctl _UID=33 --since today

One of the impetuses behind the systemd journal is to centralize the management of logs regardless of where the messages are 
originating. Since much of the boot process and service management is handled by the systemd process, it makes sense to standardize 
the way that logs are collected and accessed. The journald daemon collects data from all available sources and stores them in a 
binary format for easy and dynamic manipulation.

This gives us a number of significant advantages. By interacting with the data using a single utility, administrators are able to 
dynamically display log data according to their needs. This can be as simple as viewing the boot data from three boots ago, or
combining the log entries sequentially from two related services to debug a communication issue.

################################################################################################################################

The "number of cores = max load" Rule of Thumb: on a multicore system, your load should not exceed the number of cores available.

The "cores is cores" Rule of Thumb: How the cores are spread out over CPUs doesn't matter. Two quad-cores == four dual-cores == eight single-cores.
 It's all eight cores for these purposes.

Bringing It Home
Let's take a look at the load averages output from uptime:

~ $ uptime
23:05 up 14 days, 6:08, 7 users, load averages: 0.65 0.42 0.36
This is on a dual-core CPU, so we've got lots of headroom. I won't even think about it until load gets and stays above 1.7 or so.

Now, what about those three numbers? 0.65 is the average over the last minute, 0.42 is the average over the last five minutes, and 0.36 is the average 
over the last 15 minutes. Which brings us to the question:

Which average should I be observing? One, five, or 15 minute?

For the numbers we've talked about (1.00 = fix it now, etc), you should be looking at the five or 15-minute averages. Frankly, 
if your box spikes above 
1.0 on the one-minute average, you're still fine. It's when the 15-minute average goes north of 1.0 and stays there that you need
to snap to. 
(obviously, as we've learned, adjust these numbers to the number of processor cores your system has).

###########################################################################################################################################


All processes use memory, of course, but each process doesn't need all its allocated memory all the time. Taking advantage of this 
fact, the kernel 
frees up physical memory by writing some or all of a process' memory to disk until it's needed again.

The kernel uses paging and swapping to perform this memory management. Paging refers to writing portions, termed pages, of a 
process' memory to disk. 
Swapping, strictly speaking, refers to writing the entire process, not just part, to disk. In Linux, true swapping is exceedingly 
rare, but the terms 
paging and swapping often are used interchangeably.

When pages are written to disk, the event is called a page-out, and when pages are returned to physical memory, the event is called 
a page-in. A page 
fault occurs when the kernel needs a page, finds it doesn't exist in physical memory because it has been paged-out, and re-reads it 
in from disk.

Page-ins are common, normal and are not a cause for concern. For example, when an application first starts up, its executable image 
and data are paged-in. 
This is normal behavior.

Page-outs, however, can be a sign of trouble. When the kernel detects that memory is running low, it attempts to free up memory by
paging out. Though this 
may happen briefly from time to time, if page-outs are plentiful and constant, the kernel can reach a point where it's actually 
spending more time managing 
paging activity than running the applications, and system performance suffers. This woeful state is referred to as thrashing.




Using swap space is not inherently bad. Rather, it's intense paging activity that's problematic. For instance, if your 
most-memory-intensive application is
 idle, it's fine for portions of it to be set aside when another large job is active. Memory pages belonging to an idle application
 are better set aside so 
 the kernel can use physical memory for disk buffering.


All fields are explained in the vmstat man page, but the most important columns for this article are free, si and so. The free
column shows the amount 
of free memory, si shows page-ins and so shows page-outs. In this example, the so column is zero consistently, indicating there are
no page-outs.

The abbreviations so and si are used instead of the more accurate po and pi for historical reasons.

It isn't necessarily bad for your system to be using some of its swap space. But if you discover your system is often running low
on physical memory and
 paging is causing performance to suffer, add more memory. If you can't add more memory, run memory-intensive jobs at different 
 times of the day, avoid 
 running nonessential jobs when memory demand is high or distribute jobs across multiple systems if possible.

================================================================================================================================================================

If the %I/O wait is more than zero for a longer period of time then we can consider there is some bottleneck in 
I/O system ( Hard disk or Network )





