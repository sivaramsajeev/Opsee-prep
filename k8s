

That is the output of replicas: 3

Think of one deployment can have many running instances(replica).

//deployment.yaml
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: tomcat-deployment222
spec:
  selector:
    matchLabels:
      app: tomcat
  replicas: 3
  template:
    metadata:
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat
        image: tomcat:9.0
        ports:
        - containerPort: 8080
	
########################################################################################################
FROM alpine as dev 
RUN 
RUN     --out dir

###########STARTING THE NEW STAGE #################################
FROM alpine-trimmed-down as prod

COPY --from=dev /out /app

RUN
CMD 
#########################################################################################################

FROM python:3.7.3-alpine3.9 as base

RUN mkdir /work/
WORKDIR /work/

COPY ./src/requirements.txt /work/requirements.txt
RUN pip install -r requirements.txt

COPY ./src/ /work/
ENV FLASK_APP=server.py

# ---------START NEW IMAGE : DEBUGGER ###################
FROM base as debug
RUN pip install ptvsd

WORKDIR /work/
CMD python -m ptvsd --host 0.0.0.0 --port 5678 --wait --multiprocess -m flask run -h 0.0.0 -p 5000

###########START NEW IMAGE: PRODUCTION ###################
FROM base as prod

CMD flask run -h 0.0.0 -p 5000





################################################################################################

 
FROM node:12.4.0-alpine as dev

RUN mkdir /work/
WORKDIR /work/

COPY ./src/package.json /work/package.json
RUN npm install

COPY ./src/ /work/


###########START NEW IMAGE###################

FROM dev as prod

CMD node .




##############################################################################################

FROM golang:1.12.5-alpine3.9 as debug

# installing git
RUN apk update && apk upgrade && \
    apk add --no-cache git \
        dpkg \
        gcc \
        git \
        musl-dev

ENV GOPATH /go
ENV PATH $GOPATH/bin:/usr/local/go/bin:$PATH

RUN go get github.com/sirupsen/logrus
RUN go get github.com/buaazp/fasthttprouter
RUN go get github.com/valyala/fasthttp
RUN go get github.com/go-delve/delve/cmd/dlv

WORKDIR /go/src/work
COPY ./src /go/src/work/

RUN go build -o app
### Run the Delve debugger ###
COPY ./dlv.sh /
RUN chmod +x /dlv.sh 
ENTRYPOINT [ "/dlv.sh"]

###########START NEW IMAGE###################

FROM alpine:3.9 as prod
COPY --from=debug /go/src/work/app /
CMD ./app
© 2020 GitHub, Inc.


##################################################################################

brctl show or ifconfig -a    -> usual bridge network created by docker engine

but PODs makes use of DNS    http://foo  thats where kubeproxy comes into play it makes use of the underlying 
feature of iptables in linux kernel to make routing possible among different nodes having their own subnets
as far as docker bridges are concerned.

Otherwise they would not be able to communicate since the pods in a node belong to the bridge of that particular node 
alone hence kubeproxy enables iptables based routing. thats why labels are important since it works based on labels.

kubectl describe svc example-svc  -  it should show the required endpoints so that we can confirm svc is working fine

Now ingress is all about serving traffic to the outside world. Companies usually expose it as an endpoint to the application.

When pods talk to each other they make use of service  ie http://foo       or   http://foo.bar    # bar is the ns if in different namespace

an Ingress controller basically replaces a pod and acts as a proxy, you could use HAproxy or nginx, which will make use of 
the service and communicate to pods in deployment via http   http://foo . Now the developer doesnt need to know nginx or HAproxy
he just needs to write a yaml for ingress rules.

So basically separate microservices through a single ingress contoller (~ to an API gateway)

           POD          <------                      |  INGRESS     |     <-------  http://bar/bar-service
	   DEPLOYMENT   <------	|	 |           |              |     <-------  http://foo.com/foo-service
	   DEPLOYMENT   <------	|SERVICE |	     |  CONTROLLER  |     <-------  http://foo-service.foo.com


apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: example-service
  annotations:
    kubernetes.io/ingress.class: "traefik"
    traefik.ingress.kubernetes.io/frontend-entry-points: http,https
    #traefik.ingress.kubernetes.io/redirect-entry-point: https
    #traefik.ingress.kubernetes.io/redirect-permanent: "true"
spec:
  rules:
  - host: marcel.test
    http:
      paths:
      - path: /
        backend:
          serviceName: example-service
          servicePort: 80   
	  













########################################################################################################
	

Pod is a collection of containers and basic object of Kuberntes. All containers of pod lie in same node.

Not suitable for production
No rolling updates
Deployment is a kind of controller in Kubernetes.

Controllers use a Pod Template that you provide to create the Pods for which it is responsible.

Deployment creates a ReplicaSet which in turn make sure that, CurrentReplicas is always same as desiredReplicas .

Advantages :

You can rollout and rollback your changes using deployment
Monitors the state of each pod
Best suitable for production
Supports rolling updates



Try to avoid Pods and implement Deployments instead for managing containers as objects of kind Pod will not be rescheduled (or self healed) in the event of a node failure or pod termination.

A Deployment is generally preferable because it defines a ReplicaSet to ensure that the desired number of Pods is always available and specifies a strategy to replace Pods, such as RollingUpdate.


kubectl logs -f <pod-id>
You can use the -f flag:

-f, --follow=false: Specify if the logs should be streamed.



You can use labels

kubectl logs -l app=elasticsearch


You can get yaml from the kubectl create configmap command and pipe it to kubectl replace, like this:

kubectl create configmap foo --from-file foo.properties -o yaml --dry-run | kubectl replace -f -


apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 2
  selector:
    app: myapp
    deployment: initial
  template:
    metadata:
      labels:
        app: myapp
        deployment: initial
    spec:
      containers:
      - name: myapp
        image: myregistry.com/myapp:5c3dda6b
        ports:
        - containerPort: 80
      imagePullPolicy: Always
      imagePullSecrets:
        - name: myregistry.com-registry-key
	
	
	
	##########################################################################################################################
	
apiVersion: v1
kind: Pod
metadata:
  name: hello-world
spec:  # specification of the pod’s contents
  restartPolicy: Never
  containers:
  - name: hello
    image: "ubuntu:14.04"
    env:
    - name: MESSAGE
      value: "hello world"
    command: ["/bin/sh","-c"]
    args: ["/bin/echo \"${MESSAGE}\""]
If I want to run more than one command, how to do?


command: ["/bin/sh","-c"]
args: ["command one; command two && command three"]
Explanation: The command ["/bin/sh", "-c"] says "run a shell, and execute the following instructions". The args are then passed
as commands to the shell. In shell scripting a semicolon separates commands, and && conditionally runs the following command if
the first succeed. In the above example, it always runs command one followed by command two, and only runs command three if command
two succeeded.

Alternative: In many cases, some of the commands you want to run are probably setting up the final command to run. In this case,
building your own Dockerfile is the way to go. Look at the RUN directive in particular.

Use secrets for things which are actually secret like API keys, credentials, etc
Use config map for not-secret configuration data
In the future there will likely be some differentiators for secrets like rotation or support for backing the secret API w/ HSMs, etc.
In general we like intent-based APIs, and the intent is definitely different for secret data vs. plain old configs

Both, ConfigMaps and Secrets store data as a key value pair. The major difference is, Secrets store data in base64 format meanwhile 
ConfigMaps store data in a plain text.

If you have some critical data like, keys, passwords, service accounts credentials, db connection string, etc then you should
always go for Secrets rather than Configs.

And if you want to do some application configuration using environment variables which you don't want to keep secret/hidden like, 
app theme, base platform url, etc then you can go for ConfigMaps


Containers are meant to run to completion. You need to provide your container with a task that will never finish. Something
like this should work:

apiVersion: v1
kind: Pod
metadata:
  name: ubuntu
spec:
  containers:
  - name: ubuntu
    image: ubuntu:latest
    # Just spin & wait forever
    command: [ "/bin/bash", "-c", "--" ]
    args: [ "while true; do sleep 30; done;" ]
    
    
   
 Use this command inside you Dockerfile to keep the container running in you K8s cluster:

CMD tail -f /dev/null


Service: This directs the traffic to a pod.

TargetPort: This is the actual port on which your application is running inside the container.

Port: Some times your application inside container serves different services on a different port. Ex:- the actual application can run 8080 and health checks for this application can run on 8089 port of the container. So if you hit the service without port it doesn't know to which port of the container it should redirect the request. Service needs to have a mapping so that it can hit the specific port of the container.

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - name: http
      nodePort: 30475
      port: 8089
      protocol: TCP
      targetPort: 8080
    - name: metrics
      nodePort: 31261
      port: 5555
      protocol: TCP
      targetPort: 5555
    - name: health
      nodePort: 30013
      port: 8443
      protocol: TCP
      targetPort: 8085 
if you hit the my-service:8089 the traffic is routed to 8080 of the container(targetPort). Similarly, if you hit my-service:8443 
then it is redirected to 8085 of the container(targetPort).

But this myservice:8089 is internal to the kubernetes cluster and can be used when one application wants to communicate with another
application. So to hit the service from outside the cluster someone needs to expose the port on the host machine on which kubernetes
is running so that the traffic is redirected to a port of the container. In that can use nodePort.

From the above example, you can hit the service from outside the cluster(Postman or any restclient) by host_ip:Nodeport

Say your host machine ip is 10.10.20.20 you can hit the http,metrics,health services by 10.10.20.20:30475,10.10.20.20:31261,
10.10.20.20:30013

You can populate a container's environment variables through the use of Secrets or ConfigMaps. Use Secrets when the data you are working with is sensitive (e.g. passwords), and ConfigMaps when it is not.

In your Pod definition specify that the container should pull values from a Secret:

apiVersion: v1
kind: Pod
metadata: 
  labels: 
    context: docker-k8s-lab
    name: mysql-pod
  name: mysql-pod
spec: 
  containers:
  - image: "mysql:latest"
    name: mysql
    ports: 
    - containerPort: 3306
    envFrom:
      - secretRef:
         name: mysql-secret
Note that this syntax is only available in Kubernetes 1.6 or later. On an earlier version of Kubernetes you will have to specify each value manually, e.g.:

env: 
- name: MYSQL_USER
  valueFrom:
    secretKeyRef:
      name: mysql-secret
      key: MYSQL_USER
      
      
 Deployment - You specify a PersistentVolumeClaim that is shared by all pod replicas. In other words, shared volume.

The backing storage obviously must have ReadWriteMany or ReadOnlyMany accessMode if you have more than one replica pod.

StatefulSet - You specify a volumeClaimTemplates so that each replica pod gets a unique PersistentVolumeClaim associated with it. 
In other words, no shared volume.

Here, the backing storage can have ReadWriteOnce accessMode.

StatefulSet is useful for running things in cluster e.g Hadoop cluster, MySQL cluster, where each node has its own storage.


Use 'StatefulSet' with a distributed application that requires each node to have a persistent state and the ability to configure an arbitrary number of nodes through a configuration (replicas = 'X').

All nodes in a master-master configuration and slave nodes in a master-slave configuration can make use of a StatefulSet along with a Service. Master nodes (like master, master-secondary) may each be a Pod with some persistent volume along with a Service as these nodes have no need to scale up or down. They can as well be a StatefulSet with replicas = 1.

Examples of StatefulSet are:
- Datanodes (slaves) in a Hadoop cluster (master-slave)
- Database nodes (master-master) in a Cassandra cluster

Each Pod (replica) in a StatefulSet has
- A unique and stable network identity
- Kubernetes creates one PersistentVolume for each VolumeClaimTemplate


To pull a private DockerHub hosted image from a Kubernetes YAML:

Run these commands:

DOCKER_REGISTRY_SERVER=docker.io
DOCKER_USER=Type your dockerhub username, same as when you `docker login`
DOCKER_EMAIL=Type your dockerhub email, same as when you `docker login`
DOCKER_PASSWORD=Type your dockerhub pw, same as when you `docker login`

kubectl create secret docker-registry myregistrykey \
  --docker-server=$DOCKER_REGISTRY_SERVER \
  --docker-username=$DOCKER_USER \
  --docker-password=$DOCKER_PASSWORD \
  --docker-email=$DOCKER_EMAIL
If your username on DockerHub is DOCKER_USER, and your private repo is called PRIVATE_REPO_NAME, and the image you want to pull is tagged as latest, create this example.yaml file:

apiVersion: v1
kind: Pod
metadata:
  name: whatever
spec:
  containers:
    - name: whatever
      image: DOCKER_USER/PRIVATE_REPO_NAME:latest
      imagePullPolicy: Always
      command: [ "echo", "SUCCESS" ]
  imagePullSecrets:
    - name: myregistrykey
Then run:

kubectl create -f example.yaml


This should work:

kubectl create secret generic production-tls \
    --from-file=./tls.key --from-file=./tls.crt --dry-run -o yaml | 
  kubectl apply -f 
  
  
  


    
    






############################################################################################################################

Containers on same pod act as if they are on the same machine. You can ping them using localhost:port itself. Every container in a
pod shares the same IP. You can `ping localhost` inside a pod. Two containers in the same pod share an IP and a network namespace
and They are both localhost to each other. Discovery works like this: Component A's pods -> Service Of Component B -> Component B's 
pods and Services have domain names servicename.namespace.svc.cluster.local, the dns search path of pods by default includes that 
stuff, so a pod in namespace Foo can find a Service bar in same namespace Foo by connecting to `bar`


kube-proxy does 2 things

for every Service, open a random port on the node and proxy that port to the Service.
install and maintain iptables rules which capture accesses to a virtual ip:port and redirect those to the port in (1)
The kube-proxy is a component that manages host sub-netting and makes services available to other components.Kubeproxy handles 
network communication and shutting down master does not stop a node from serving the traffic and kubeproxy works, in the same way, 
using a service. The iptables will route the connection to kubeproxy, which will then proxy to one of the pods in the service.
kube-proxy translate the destination address to whatever is in the endpoints.

Container Runtime

Kubelet
kube-proxy
Kubernetes Worker node is a machine where workloads get deployed. The workloads are in the form of containerised applications 
and because of that, every node in the cluster must run the container run time such as docker in order to run those workloads.
You can have multiple masters mapped to multiple worker nodes or a single master having a single worker node. Also, the worker
nodes are not gossiping or doing leader election or anything that would lead to odd-quantities. The role of the container run
time is to start and managed containers. The kubelet is responsible for running the state of each node and it receives commands
and works to do from the master. It also does the health check of the nodes and make sure they are healthy. Kubelet is also
responsible for metric collectins of pods as well. The kube-proxy is a component that manages host subnetting and makes services
available to other components.


A Pod is running 2 containers, when One container stops - another Container is still running, on this event, I want to terminate
this Pod?

You need to add a liveness and readiness probe to query each container,  if the probe fails, the entire pod will be restarted .add 
liveness object that calls any api that returns 200 to you from another container and both liveness and readiness probes run in
infinite loops for example, If X depended to Y So add liveness  in X that check the health of Y.Both readiness/liveness probes
always have to run after the container has been started .kubelet component performs the liveness/readiness checks and set 
initialDelaySeconds and it can be anything from a few seconds to a few minutes depending on app start time. Below is the
configuration spec

livenessProbe spec:
livenessProbe:
httpGet:
path: /path/test/
port: 10000
initialDelaySeconds: 30
timeoutSeconds: 5
readinessProbe spec:
readinessProbe:
httpGet:
path: /path/test/
port: 10000
initialDelaySeconds: 30
timeoutSeconds: 5


An ingress is an object that holds a set of rules for an ingress controller, which is essentially a reverse proxy and is used to
(in the case of nginx-ingress for example) render a configuration file. It allows access to your Kubernetes services from outside 
the Kubernetes cluster. It holds a set of rules. An Ingress Controller is a controller. Typically deployed as a Kubernetes Deployment.
That deployment runs a reverse proxy, the ingress part, and a reconciler, the controller part. the reconciler configures the reverse
proxy according to the rules in the ingress object. Ingress controllers watch the k8s api and update their config on changes. 
The rules help to pass to a controller that is listening for them. You can deploy a bunch of ingress rules, but nothing will 
happen unless you have a controller that can process them.

LoadBalancer service -> Ingress controller pods -> App service (via ingress) -> App pods




How to forward port `8080 (container) -> 8080 (service) -> 8080 (ingress) -> 80 (browser)` how is it done?

The ingress is exposing port 80 externally for the browser to access, and connecting to a service that listens on 8080. The ingress will listen on port 80 by default. An "ingress controller" is a pod that receives external traffic and handles the ingress  and is configured by an ingress resource For this you need to configure ingress selector and if no 'ingress controller selector' is specified then no ingress controller will control the ingress.

simple ingress Config will look like

host: abc.org
http:
paths:
backend:
serviceName: abc-service
servicePort: 8080
Then the service will look like
kind: Service
apiVersion: v1
metadata:
name: abc-service
spec:
ports:
protocol: TCP
port: 8080 # this is the port the service listens on
targetPort: 8080



Are deployments with more than one replica automatically doing rolling updates when a new deployment config is applied?

The Deployment updates Pods in a rolling update fashion when .spec.strategy.type==RollingUpdate .You can specify maxUnavailable
and maxSurge to control the rolling update process. Rolling update is the default deployment strategy.kubectl rolling-update updates
Pods and ReplicationControllers in a similar fashion. But, Deployments are recommended, since they are declarative, and have
additional features, such as rolling back to any previous revision even after the rolling update is done.So for rolling updates
to work as one may expect, a readiness probe is essential. Redeploying deployments is easy but rolling updates will do it nicely
for me without any downtime. The way to make a  rolling update of a Deployment and kubctl apply on it is as below

spec:
minReadySeconds: 180
replicas: 9
revisionHistoryLimit: 20
selector:
matchLabels:
deployment: standard
name: standard-pod
strategy:
rollingUpdate:
maxSurge: 1
maxUnavailable: 1
type: RollingUpdate


Suppose you have to use database with your application but well, if you make a database container-based deployment. how would the 
data persist?

Deployments are for stateless services, you want to use a StatefulSet or just define 3+ pods without a replication controller
at all. If you care about stable pod names and volumes, you should go for StatefulSet.Using statefulsets you can maintain which
pod is attached to which disk.StatefulSets make vanilla k8s capable of keeping Pod state (things like IPs, etc) which makes it 
easy to run clustered databases. A stateful set is a controller that orchestrates pods for the desired state. StatefulSets 
formerly known as PetSets will help for the database if hosting your own. Essentially StatefulSet is for dealing with
applications that inherently don't care about what node they run on, but need unique storage/state.







#######################################################################################################################
docker commit <cid> <new-image>   then find the difference with the base image using docker history/log

Difference between images and container lies in the top writable layer

RUN during build time while CMD on the running container on the image + RUN creates an additional layer while CMD 
writes on existing layer

Leverage NFS to mount same volume over multiple containers




With the advent of microservice architecture, users to individually scale key functions of an application and handle millions of customers. On top of this, technologies like Docker containers emerged in the enterprise, creating a consistent, portable, and easy way for users to quickly build these microservices. While Docker continued to thrive, managing these microservices & containers became a paramount requirement. All you need is a robust orchestration platform which can manage those containers which host your entire application. Kubernetes comes to a rescue.

Kubernetes is a robust orchestration platform which brings a number of features and which can be thought of as:

As a container platform
As a microservices platform
As a portable cloud platform and a lot more.
Kubernetes provides a container-centric management environment. It orchestrates computing, networking, and storage infrastructure on behalf of user workloads. This provides much of the simplicity of Platform as a Service (PaaS) with the flexibility of Infrastructure as a Service (IaaS), and enables portability across infrastructure providers. Below are the list of features which Kubernetes provides -

Service Discovery and load balancing: Kubernetes has a feature which assigns the containers with their own IP addresses and a unique DNS name, which can used to balance the load on them.
Planning: Placement of the containers on the node is a crucial feature on which makes the decision based on the resources it requires and other restrictions.
Auto Scaling: Based on the CPU usage, vertical scaling of applications is automatically triggered using the command line.
Self Repair: This is an unique feature in the Kubernetes which will restart the container automatically when it fails. If the Node dies, then containers are replaced or re-planned on the other Nodes. You can stop the containers, if they don't respond for the health checks.
Storage Orchestration: This feature of Kubernetes enables the user to mount the network storage system as a local file system.
Batch execution: Kubernetes manages both batch and CI workloads along with replacing containers that fail.
Deployments and Automatic Rollbacks: During the configuration changes for the application hosted on the Kubernetes, progressively monitors the health to ensure that it does not terminate all the instances at once, it makes an automatic rollback only in the case of failure.
Configuration Management and Secrets: All classifies information like keys and passwords are stored under module called Secrets in Kubernetes. These Secrets are used specially while configuring the application without having to reconstruct the image.



Docker started as a GITHUB project back in 2013(which is almost 5+ years from now). Slowly it grew massively with HUGE contributors across the world. Today it is a platform which is shipped as both - an open source as well as a commercial product. The orchestration is just a mere feature of Docker Enterprise Edition.

But if we really want to study how K8s is related to Docker, then the most preferred answer would be -

Docker CLI provides the mechanism for managing the life cycle of the containers. Where as the docker image defines the build time framework of runtime containers. CLI commands are there to start, stop, restart and perform lifecycle operations on these containers. Containers can be orchestrated and can be made to run on multiple hosts. The questions that need to be answered are how these containers are coordinated and scheduled? And how will the application running in these containers will communicate each other?

 Kubernetes is the answer. Today, Kubernetes mostly uses Docker to package, instantiate, and run containerized applications. Said that there are various another container runtime available but Docker is the most popular runtime binary used by Kubernetes.

Both Kubernetes and Docker build a comprehensive standard for managing the containerized applications intelligently along with providing powerful capabilities.Docker provides a platform for building running and distributing Docker containers. Docker brings up its own clustering tool which can be used for orchestration. But Kubernetes is a orchestration platform for Docker containers which is more extensive than the Docker clustering tool, and has capacity to scale to the production level. Kubernetes is a container orchestration system for Docker containers that is more extensive than Docker Swarm and is meant to coordinate clusters of nodes at scale in production in an efficient manner.  Kubernetes is a plug and play architecture for the container orchestration which provides features like high availability among the distributed nodes

######################################################################################################################
Config maps ideally stores application configuration in a plain text format whereas Secrets store sensitive data like password in an encrypted format. Both config maps and secrets can be used as volume and mounted inside a pod through a pod definition file.

Config map:

                 kubectl create configmap myconfigmap
 --from-literal=env=dev
Secret:

echo -n ‘admin’ > ./username.txt
echo -n ‘abcd1234’ ./password.txt
kubectl create secret generic mysecret --from-file=./username.txt --from-file=./password.txt



If a node is tainted, is there a way to still schedule the pods to that node?

When a node is tainted, the pods don't get scheduled by default, however, if we have to still schedule a pod to a tainted node we can start applying tolerations to the pod spec.

Apply a taint to a node:

kubectl taint nodes node1 key=value:NoSchedule
Apply toleration to a pod:

spec:
tolerations:
- key: "key"
operator: "Equal"
value: "value"
effect: "NoSchedule"



The mapping between persistentVolume and persistentVolumeClaim is always one to one. Even When you delete the claim, PersistentVolume still remains as we set persistentVolumeReclaimPolicy is set to Retain and It will not be reused by any other claims. Below is the spec to create the Persistent Volume.

apiVersion: v1
kind: PersistentVolume
metadata:
name: mypv
spec:
capacity:
storage: 5Gi
volumeMode: Filesystem
accessModes:
- ReadWriteOnce
persistentVolumeReclaimPolicy: Retain


By default Deployment in Kubernetes using RollingUpdate as a strategy. Let's say we have an example that creates a deployment in Kubernetes

kubectl run nginx --image=nginx # creates a deployment
○ → kubectl get deploy
NAME    DESIRED  CURRENT UP-TO-DATE   AVAILABLE AGE
nginx   1  1 1            0 7s
Now let’s assume we are going to update the nginx image

kubectl set image deployment nginx nginx=nginx:1.15 # updates the image 
Now when we check the replica sets

kubectl get replicasets # get replica sets
NAME               DESIRED CURRENT READY   AGE
nginx-65899c769f   0 0 0       7m
nginx-6c9655f5bb   1 1 1       13s
From the above, we can notice that one more replica set was added and then the other replica set was brought down

kubectl rollout status deployment nginx 
# check the status of a deployment rollout

kubectl rollout history deployment nginx
 # check the revisions in a deployment

○ → kubectl rollout history deployment nginx
deployment.extensions/nginx
REVISION  CHANGE-CAUSE
1         <none>
2         <none>


How to monitor that a Pod is always running?

We can introduce probes. A liveness probe with a Pod is ideal in this scenario.

A liveness probe always checks if an application in a pod is running,  if this check fails the container gets restarted. This is ideal in many scenarios where the container is running but somehow the application inside a container crashes.

spec:
containers:
- name: liveness
image: k8s.gcr.io/liveness
args:
- /server
livenessProbe:
      httpGet:
        path: /healthz
	
	
	

You can use following command to delete the POD forcefully.

kubectl delete pod <PODNAME> --grace-period=0 --force --namespace <NAMESPACE>















How Kubernetes simplify the containerized application deployment process?

A application deployment requires , web tier , application tier and database tier . All these requirements will spawn multiple containers and these containers should communicate among each other . Kubernetes cluster will take care of the whole system and orchestrates the container needs . 

Let us look at a quick WordPress application example. WordPress application consists of frontend(WordPress running on PHP and Apache) and backend(MySQL). The below YAML file can help you specify everything you will need to bring WordPress Application in a single shot:

apiVersion: v1
kind: Service
metadata:
 name: wordpress
 labels:
   app: wordpress
spec:
 ports:
   - port: 80
 selector:
   app: wordpress
   tier: frontend
 type: LoadBalancer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: wp-pv-claim
 labels:
   app: wordpress
spec:
 storageClassName: manual
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
     storage: 2Gi
---
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
 name: wordpress
 labels:
   app: wordpress
spec:
 storageClassName: manual
 selector:
   matchLabels:
     app: wordpress
     tier: frontend
 strategy:
   type: Recreate
 template:
   metadata:
     labels:
       app: wordpress
       tier: frontend
   spec:
     containers:
 
  - image: wordpress:4.8-apache
       name: wordpress
       env:
       - name: WORDPRESS_DB_HOST
         value: wordpress-mysql
       - name: WORDPRESS_DB_PASSWORD
         valueFrom:
           secretKeyRef:
             name: mysql-pass
             key: password
       ports:
       - containerPort: 80
         name: wordpress
       volumeMounts:
       - name: wordpress-persistent-storage
         mountPath: /var/www/html
     volumes:
     - name: wordpress-persistent-storage
       persistentVolumeClaim:
         claimName: wp-pv-clai
I assume that you have n-node Kubernetes cluster running in your infrastructure. All you need is to run the below command:

kubectl create -f wordpress-deployment.yaml   

That’s it. Browse to http://<IP>:80 port to open to see WordPress App up and running. Hence, we saw that how Kubernetes simplifies the application deployment.



How to Install Kubernetes?
Install below packages on all of your machines:

kubeadm: the command to bootstrap the cluster.
kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.
kubectl: the command line util to talk to your cluster.
Note : kubeadm will not install or manage kubelet or kubectl for you, so you will need to ensure they match the version of the Kubernetes control panel you want kubeadm to install for you.

 If you do not, there is a risk of a version skew occurring that can lead to unexpected, buggy behavior. 

However, one minor version skew between the kubelet and the control plane is supported, but the kubelet version may never exceed the API server version. For example, kubelet running 1.7.0 should be fully compatible with a 1.8.0 API server, but not vice versa.

Below is the example for installing in Debian or Ubuntu flavours

# apt-get update && apt-get install -y apt-transport-https curl
# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb http://apt.kubernetes.io/ kubernetes-xenial main
EOF
# apt-get update
# apt-get install -y kubelet kubeadm kubectl
# apt-mark hold kubelet kubeadm kubectl
Configure cgroup driver used by kubelet on Master Node
When using Docker, kubeadm will automatically detect the cgroup driver for the kubelet and set it in the /var/lib/kubelet/kubeadm-flags.env file during runtime.
If you are using a different CRI, you have to modify the file /etc/default/kubelet with your cgroup-driver value, like so:
KUBELET_KUBEADM_EXTRA_ARGS=--cgroup-driver=<value>
This file will be used by kubeadm init and kubeadm join to source extra user-defined arguments for the kubelet.
Please mind, that you only have to do that if the cgroup driver of your CRI is not cgroupfs, because that is the default value in the kubelet already.
Restarting the kubelet is required:

Kubernetes is a combination of multiple parts working together to get the container job done and the most vital part of it is Master node . The node acts as brain to the cluster and manages the whole ecosystem .
Master connects to etcd via HTTP or HTTPS to store the data and also connects flannel to access the container application.
Worker nodes engage with master via HTTP or HTTPS to get a command and report the status.
Overlay network makes connections of their container applications. All of it will be discussed below for more in-depth
Below are the mentioned components :
etcd

The heart of any Kubernetes cluster that implements a distributed key value store where all of the objects in a kubernetes cluster are persisted . 
It works on a algorithm which has replication techniques across servers to maintain the data stored in etcd . 
Optimistic concurrency is also used to compare-and-swap data across etcd server , when a user reads and update a value , the system checks that no other component in the system has updated the same value . This technique removes the locking mechanism that increases the server throughput .
Another technique known as watch protocol , which accounts for changes made in key value pair in etcd directory . Its improves efficiency to the client as it wait for the changes and then react to the change without continuous polling to the server .
kube-apiserver

As the name connects , its a server that provides an HTTP- or HTTPS-based RESTful API that is allowed to have direct access to the Kubernetes cluster .

Its a connector between all the kubernetes components and mediates all interactions between clients and the API objects stored in etcd .
Api server database is external to it , so it is a stateless server which is replicated 3 times to implement fault-tolerance
The APIs are exposed and managed by the server , the characteristics of those API requests must be described so that the client and server know how to communicate .
Define API pattern where the request is defined like api paths or groups.
Internal loops are responsible for background operations like CRD (Custom Resource Definitions) which inherently creates new paths for API requests
kube-controller-manager

The controller manager is a general service that has many responsibilities. 

Controller manager is a collection of control loops rolled up into one binary
Manages Kubernetes nodes
The control loops needed to implement the functionality like replica sets and deployments are run by Manager
Creates and updates the Kubernetes internal information
changes the current status to the desired status 
kube-scheduler

It is a simple algorithm that defines the priority to dispatch and is responsible for scheduling pods into nodes .
is continuously scanning the API server (with watch protocol) for Pods which don’t have a nodeName and are eligible for scheduling
Node affinity provide a simple way to guarantee that a Pod lands on a particular node
Predicates is a concept that helps in making correct resource requirements for the pods
Data locality


In Kubernetes, servers that perform work by running containers are known as nodes. Execution of jobs and reporting the status back to the master are the primary tasks on Node server .

kubelet

The main process on Kubernetes node that performs major container operations . 

 The Kubelet is the node-daemon that communicates with Kubernetes master for all machines that are part of a Kubernetes cluster. 
It periodically access the controller to check and report the status of the cluster
It merges the available CPU, disk and memory for a node into the large Kubernetes cluster.
Communicates the state of containers back up to the api-server for control loops to observe the current state of the containers.
kube-proxy

The kube proxy implements load-balancer networking model on each node. 
It makes the Kubernetes services locally and can do TCP and UDP forwarding.
The kube-proxy programs the network on its node, so that network requests to the virtual IP address of a service, are in-fact routed to the endpoints which implement this service
It finds cluster IPs via environment variables or DNS.
Routes traffic from Pods on the machine to Pods, anywhere in the cluster


Stateful sets: It is a controller in Kubernetes which provides a distinctive identity to its pods and responsible for managing the deployment and scaling of a set of Pods. This controller intend to be used with stateful applications(like database) and distributed systems.
Daemon Sets: It ensure that all the cluster nodes run a copy of a Pod.Whenever you add any nodes to Kubernetes cluster, DaemonSets ensures that Pods get automatically added to the new nodes as needed. Hence, it takes responsibility to manage multitude of replicated Pods.

Replication sets are an iteration on the replication controller design with greater flexibility in how the controller identifies the pods it is meant to manage. Replication sets are much more advanced than ReplicationController as they have greater replica selection capabilities, but they dont have the rolling updates capabilities.

Use Secrets in Pods

To use Secrets inside Pods, choose to expose pods in environment variables or mount the Secrets as volumes.

In terms of accessing Secrets inside a Pod, add env section inside the container spec

// using access-token Secret inside a Pod
# cat 2-7-2_env.yaml
apiVersion: v1
kind: Pod
metadata:
   name: secret-pod-env
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["/bin/sh", "-c", "while : ;do echo $ACCESS_TOKEN; sleep 10; done"]
    env:
        - name: ACCESS_TOKEN
          valueFrom:
            secretKeyRef:
              name: access-token
              key: 2-7-1_access-token
// create a pod
# kubectl create -f 2-7-2_env.yaml
pod "secret-pod-env" created



A Kubernetes volume, on the other hand, the same as the Pod that encloses it. Consequently, a volume outlives any Containers that run within the Pod, and data is preserved across Container restarts. Of course, when a Pod ceases to exist, the volume will cease to exist, too. Perhaps more importantly than this, Kubernetes supports many types of volumes, and a Pod can use any number of them simultaneously.

The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this we introduce two new API resources:PersistentVolume and PersistentVolumeClaim.

A PersistentVolume (PV) is a storage in the cluster that has to be provisioned by an administrator and it is a cluster resource. PVs are volume plugins like Volumes, but have a life cycle independent of any individual pod that uses the PV.

This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

 A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes 
 
 
 Label in Kubernetes is meaningful tag word that can be attached to Kubernetes objects to make them as a part of a group. Labels can be used for working on different instances for management or routing purposes. 

For example, the controller-based objects use labels to mark the pods that they should operate on. Micro Services use labels to understand the structure of backend pods they should route requests to.

Labels are key-value pairs. Each unit can have more than one label, but each unit can only have one entry for each key. Key is used as an identifier, but additionally can classify objects by other criteria based on  development stage, public accessibility, application version, etc activities.

Annotations attach arbitrary key-value information to an Kubernetes object. On the other hand labels should be used for meaningful information to match a pod with selection criteria, annotations contain less structured data. Annotations are a way of adding more metadata to an object that is not helpful for selection purposes.

Whenever master node under kubernetes fails, the cluster still remain in an operational mode. It doesn’t affect pod creation or service member changes. If worker node fails, master stop receiving updates from worker node.



DNS is a built-in service in Kubernetes. It gets  launched automatically when Kubernetes is setup for the first time. Kubernetes
Domain Name Server schedules a DNS Pod and Service on the cluster, and setup the kubelets to inform individual containers to use 
the DNS Service’s IP to resolve DNS names. Every Service which gets defined in the Kubernetes cluster (including the DNS server 
itself) is assigned with a DNS name. By default, a client Pod’s DNS search list will include the Pod’s own namespace and the 
cluster’s default domain. For E.g. if we have a Service named serve1 in the Kubernetes namespace ns1. A Pod running in namespace
ns1 can look up this service by simply doing a DNS query for serve1. A Pod running in namespace collab can look up this service by
doing a DNS query for serve1.ns1.


Add nodes in a HA cluster in kubernetes 

Once the masters are ready, nodes can be added into the system. The node should be finished with the prerequisite configuration as a worker node in the kubeadm cluster. 

Need to start kublet

$ sudo systemctl enable kubelet && sudo systemctl start kubelet
Run  the join command as below . However, please change the master IP to the load balancer one:

// join command 
$ sudo kubeadm join --token <CUSTOM_TOKEN> <LOAD_BALANCER_IP>:6443 --discovery-token-ca-cert-hash sha256:<HEX_STRING>
Then go to the first master or second master to check the nodes' status:

// see the node is added
$ kubectl get nodes
NAME       STATUS ROLES     AGE VERSION
master01   Ready master    4h v1.10.2
master02   Ready master    3h v1.10.2
node01     Ready <none>    22s v1.10.2


Use kubectl get deployment <deployment>. If the DESIRED, CURRENT, UP-TO-DATE are all equal, then the Deployment has completed


Make sure your imagePullPolicy is set to Always(this is the default). That means when a pod is deleted, a new pod will ensure it has the current version of the image. Then refresh all your pods. 

The simplest way to refresh all your pods is to just delete them and they will be recreated with the latest image. This immediately destroys all your pods which will cause a service outage. Do this with kubectl delete pod -l <name>=<value> where name and value are the label selectors your deployment uses. 

A better way is to edit your deployment and modify the deployment pod spec to add or change any annotation. This will cause all your pods to be deleted and rescheduled, but this method will also obey your rollingUpdate strategy, meaning no downtime assuming your rollingUpdate strategy already behaves properly. Setting a timestamp or a version number is convenient, but any change to pod annotations will cause a rolling update. For a deployment named nginx, this can be done with:

PATCH='{"spec":{"template":{"metadata":{"annotations":{"timestamp":"'$(date)'"}}}}}'
kubectl patch deployment nginx -p "$PATCH"
It is considered bad practice to rely on the :latest docker image tag in your deployments, because using :latest there is no way to rollback or specify what version of your image to use. It's better to update the deployment with an exact version of the image and use --record so that you can use kubectl rollout undo deployment <deployment> or other commands to manage rollouts.


debug a Pending pod?
Pending pod cannot be scheduled onto a node. 

Performing command  kubectl describe pod <pod_name> will help you undestand the problem.  

kubectl logs <pod> can also be helpful.  

Common reasons for pods getting stuck in Pending State are: 

1) When the pod requesting more resources than are available, for example a pod has set a request for an amount of CPU or  that is not available 

anywhere on any node. eg. requesting a 8 CPU cores when all your nodes only have 4 CPU cores. 

Doing a kubectl describe node <node> on each node will also show already requested resources. 

2) There are taints that prevent a pod from scheduling on your nodes. 

3) The nodes have been marked unschedulable with kubectl cordon 

4) There are no Ready nodes. kubectl get nodes will display the status of all nodes.


debug a Pending pod?
A Pending pod is one that cannot be scheduled onto a node. Doing a kubectl describe pod <pod> will usually tell you why. kubectl logs <pod> can also be helpful. There are several common reasons for pods stuck in Pending:

** The pod is requesting more resources than are available, a pod has set a request for an amount of CPU or memory that is not available anywhere on any node. eg. requesting a 8 CPU cores when all your nodes only have 4 CPU cores. Doing a kubectl describe node <node> on each node will also show already requested resources. ** There are taints that prevent a pod from scheduling on your nodes. ** The nodes have been marked unschedulable with kubectl cordon ** There are no Ready nodes. kubectl get nodes will display the status of all nodes.



I rollback the Deployment?
Applying changes to a Deployment process with the --record flag then Kubernetes by default saves the previous Deployment activities in its history. 

The below command will display all the prior Deployments,

                             kubectl rollout history deployment <deployment> 

The last Deployment can be restored with the command, 

                             kubectl rollout undo deployment <deployment> 

The Deployments which are in progress can also be paused and resumed.

The moment new Deployment is applied, during this process a new ReplicaSet object is created which is slowly scaled up while the old ReplicaSet is scaled down.  

We can get the ReplicaSet that has been rolled out with command 

                             kubectl get replicaset 

 Each ReplicaSet is named with the format -, 

                          kubectl describe replicaset <replicaset>a
			  

An Ingress Controller is a pod that can act as an inbound traffic handler.  Ingress Controller implemented as a HTTP reverse proxy. Prominent features are HTTP path and service based routing and SSL termination
			  
			  
			  






################################################################################################################

The fundamental behind Kubernetes is that we can enforce the desired state management, by which I mean that we can feed the cluster
services of a specific configuration, and it will be up to the cluster services to go out and run that configuration in the
infrastructur

So, as you can see in the above diagram, the deployment file will have all the configurations required to be fed into the cluster
services. Now, the deployment file will be fed to the API and then it will be up to the cluster services to figure out how to schedule 
these pods in the environment and make sure that the right number of pods are running.

So, the API which sits in front of services, the worker nodes & the Kubelet process that the nodes run, all together make up the 
Kubernetes Cluster.

What is Heapster?
Heapster is a cluster-wide aggregator of data provided by Kubelet running on each node. This container management tool is supported
natively on Kubernetes cluster and runs as a pod, just like any other pod in the cluster. So, it basically discovers all nodes in
the cluster and queries usage information from the Kubernetes nodes in the cluster, via on-machine Kubernetes agent.

What is Kubectl?
Kubectl is the platform using which you can pass commands to the cluster. So, it basically provides the CLI to run commands
against the Kubernetes cluster with various ways to create and manage the Kubernetes component.

Q14.  What is Kubelet?
This is an agent service which runs on each node and enables the slave to communicate with the master. So, Kubelet works on the
description of containers provided to it in the PodSpec and makes sure that the containers described in the PodSpec are healthy and 
running.

The master node has the kube-controller-manager, kube-apiserver, kube-scheduler, etcd. Whereas the worker node has kubelet 
and kube-proxy running on each node.

Kube-proxy can run on each and every node and can do simple TCP/UDP packet forwarding across backend network service. So basically,
it is a network proxy which reflects the services as configured in Kubernetes API on each node. So, the Docker-linkable compatible
environment variables provide the cluster IPs and ports which are opened by proxy.

Kubernetes master controls the nodes and inside the nodes the containers are present. Now, these individual containers are 
contained inside pods and inside each pod, you can have a various number of containers based upon the configuration and
requirements. So, if the pods have to be deployed, then they can either be deployed using user interface or command line
interface. Then, these pods are scheduled on the nodes and based on the resource requirements, the pods are allocated to
these nodes. The kube-apiserver makes sure that there is communication established between the Kubernetes node and the master 
components.


The kube – apiserver follows the scale-out architecture and, is the front-end of the master node control panel. This exposes 
all the APIs of the Kubernetes Master node components and is responsible for establishing communication between Kubernetes
Node and the Kubernetes master components.

The kube-scheduler is responsible for distribution and management of workload on the worker nodes. So, it selects the most
suitable node to run the unscheduled pod based on resource requirement and keeps a track of resource utilization. It makes
sure that the workload is not scheduled on nodes which are already full

Multiple controller processes run on the master node but are compiled together to run as a single process which is the
Kubernetes Controller Manager. So, Controller Manager is a daemon that embeds controllers and does namespace creation and garbage
collection. It owns the responsibility and communicates with the API server to manage the end-points.

Etcd is written in Go programming language and is a distributed key-value store used for coordinating between distributed work.
So, Etcd stores the configuration data of the Kubernetes cluster, representing the state of the cluster at any given point
in time.

services - cluster ip(internal for the cluster) , nodeport & loadbalancer

A load balancer is one of the most common and standard ways of exposing service. There are two types of load balancer used based
on the working environment i.e. either the Internal Load Balancer or the External Load Balancer. The Internal Load Balancer
automatically balances load and allocates the pods with the required configuration whereas the External Load Balancer directs
the traffic from the external load to the backend pods

Ingress network is a collection of rules that acts as an entry point to the Kubernetes cluster. This allows inbound connections, which can be configured to give services externally through reachable URLs, load balance traffic, or by offering name-based virtual hosting. So, Ingress is an API object that manages external access to the services in a cluster, usually by HTTP and is the most powerful way of exposing service.

Now, let me explain to you the working of Ingress network with an example.

There are 2 nodes having the pod and root network namespaces with a Linux bridge. In addition to this, there is also a new virtual ethernet device called flannel0(network plugin) added to the root network.

Now, suppose we want the packet to flow from pod1 to pod 4. Refer to the below diagram

So, the packet leaves pod1’s network at eth0 and enters the root network at veth0.
Then it is passed on to cbr0, which makes the ARP request to find the destination and it is found out that nobody on this node has the
destination IP address.
So, the bridge sends the packet to flannel0 as the node’s route table is configured with flannel0.
Now, the flannel daemon talks to the API server of Kubernetes to know all the pod IPs and their respective nodes to create mappings
for pods IPs to node IPs.
The network plugin wraps this packet in a UDP packet with extra headers changing the source and destination IP’s to their respective
nodes and sends this packet out via eth0.
Now, since the route table already knows how to route traffic between nodes, it sends the packet to the destination node2.
The packet arrives at eth0 of node2 and goes back to flannel0 to de-capsulate and emits it back in the root network namespace.
Again, the packet is forwarded to the Linux bridge to make an ARP request to find out the IP that belongs to veth1.
The packet finally crosses the root network and reaches the destination Pod4.


The Cloud Controller Manager is responsible for persistent storage, network routing, abstracting the cloud-specific code from the 
core Kubernetes specific code, and managing the communication with the underlying cloud services. It might be split out into several
different containers depending on which cloud platform you are running on and then it enables the cloud vendors and Kubernetes code 
to be developed without any inter-dependency. So, the cloud vendor develops their code and connects with the Kubernetes
cloud-controller-manager while running the Kubernetes.

As for users, it is really important to understand the performance of the application and resource utilization at all the different 
abstraction layer, Kubernetes factored the management of the cluster by creating abstraction at different levels like container, pods, 
services and whole cluster. Now, each level can be monitored and this is nothing but Container resource monitoring.

tools - heapster, influxdb, prometheus,grafana

Replica Set and Replication Controller do almost the same thing. Both of them ensure that a specified number of pod replicas are running at any given time. The difference comes with the usage of selectors to replicate pods. Replica Set use Set-Based selectors while replication controllers use Equity-Based selectors.

Equity-Based Selectors: This type of selector allows filtering by label key and values. So, in layman terms, the equity-based selector
will only look for the pods which will have the exact same phrase as that of the label.
Example: Suppose your label key says app=nginx, then, with this selector, you can only look for those pods with label app equal to
nginx.
Selector-Based Selectors: This type of selector allows filtering keys according to a set of values. So, in other words, the selector 
based selector will look for pods whose label has been mentioned in the set.
Example: Say your label key says app in (nginx, NPS, Apache). Then, with this selector, if your app is equal to any of nginx, NPS, 
or Apache, then the selector will take it as a true result.

Multiple Kubernetes clusters can be managed as a single cluster with the help of federated clusters. So, you can create multiple
Kubernetes clusters within a data center/cloud and use federation to control/manage them all at one place.

The company can implement the DevOps methodology, by building a CI/CD pipeline, but one problem that may occur here is the 
configurations may take time to go up and running. So, after implementing the CI/CD pipeline the company’s next step should
be to work in the cloud environment. Once they start working on the cloud environment, they can schedule containers on a cluster
and can orchestrate with the help of Kubernetes. This kind of approach will help the company reduce their deployment time, and
also get faster across various environments.


In order to give millions of clients the digital experience they would expect, the company needs a platform that is scalable,
and responsive, so that they could quickly get data to the client website. Now, to do this the company should move from their
private data centers (if they are using any) to any cloud environment such as AWS. Not only this, but they should also implement
the microservice architecture so that they can start using Docker containers. Once they have the base framework ready, then they
can start using the best orchestration platform available i.e. Kubernetes. This would enable the teams to be autonomous in building
applications and delivering them very quickly.

 to solve the problem, they can shift their monolithic code base to a microservice design and then each and every microservices
 can be considered as a container. So, all these containers can be deployed and orchestrated with the help of Kubernetes.
 
 Kubernetes Controller = Both ReplicaSet and Deployment
 
 











###################################################################################################################
gcloud command line installation (req python 2) --> download the tarball & follow doc 

APIs -> enable APIs & services for gce & k8s

gcloud components list --> gcloud components install kubectl 
gcloud config list    , gclud compute regions list , gcloud compute zones list 
gcloud config set compute/zone europe-west-1c

For GKE demo, from console with micro instance & default K8s version 

gcloud container clusters create "demo-clutser" --zone "us-central1-a" --machine-type "f1-micro"

-> connect via terminal or cloud shell 

kubectl cluster-info

Think of name spaces as partitions in harddisk. So we can partition cluster by name spaces. No 2 resources with same names in same namespace.
So partition using name space for using same names.

scp master:/etc/kubernetes/admin.conf  ~/.kube/config

kubectl get namespaces   --> by default in default ns

kubectl create namespace demo

kubectl config view --> parsing the .kube/config values 

kubectl config get-contexts

kubectl config set-context kubesys --namespace=kube-system --user=kubernetes-admin --cluster=kubernetes

kubectl config use-context kubesys  --> to switch context & namespace 

Look in the kube system namespace, kubectl -n(for name space) kube-system get pods  --> Gives you the cluster related pods
kubectl -n kube-system get pod kube-proxy-dfjj -o yaml > /tmp/mypod.yaml

watch kubectl get all -o wide   --> in one terminal 

kubectl run nginx-deploy --image nginx --replicas 2   --> deployment 

then edit possible from console since it shows yaml 

kubectl describe nginx-deploy

kubectl scale deploy nginx-deploy --replicas=4  --> 3 microinstances wont allow 4 pods

shell into container from console by kubectl -> exec == kubectl exec -it <pod> -- /bin/sh

creating service from console itself by using 'expose' 

For service , kubectl expose deployment hello-node --type=LoadBalancer --port=8080

#####################################################################################################################

Kubernetes uses etcd as its database.

The only nuance is that etcd is a distributed database – because Kubernetes is a distributed system.

etcd manages a lot of the tricky problems in running a distributed database – like race conditions and networking – and saves 
Kubernetes from worrying about it.

How does Kubernetes use etcd?
Kubernetes uses etcd as a key-value database store. It stores the configuration of the Kubernetes cluster in etcd.

It also stores the actual state of the system and the desired state of the system in etcd.

It then uses etcd’s watch functionality to monitor changes to either of these two things. If they diverge, Kubernetes makes changes to reconcile the actual state and the desired state.

A Kubernetes cluster stores all its data in etcd.

Anything you might read from a kubectl get xyz command is stored in etcd.

Any change you make via kubectl create will cause an entry in etcd to be updated.

Any node crashing or process dying causes values in etcd to be changed.

The set of processes that make up Kubernetes use etcd to store data and notify each other of changes.

#########################################################################################################################
you can see token value on master node using command:

cat /etc/kubernetes/pki/tokens.csv

The commands I used are:

kubeadm token generate
kubeadm token create <generated-token> --print-join-command --ttl=0

kubectl -n kube-system get secret clusterinfo -o yaml | grep token-map | awk '{print $2}'



#######################################################################################################################

Checking Pod Logs with kubectl logs
The first thing I normally do if a Pod is having problems is check the logs for any errors. This is very similar to docker logs.

kubectl logs [pod-name]
If the Pod contains more than one container you can use the -c switch to define a specific container. Use the container name defined in the Pod or Deployment YAML.

kubectl logs [pod-name] -c [container-name]
Note: Run kubectl get pod [pod-name] -o yaml or kubectl get deployment [deployment-name] -o yaml if you’re not sure about the name of the container. The -o yaml switch is useful for getting additional information about the Pod by the way – more information on that technique will be provided a little later.

To get logs for all containers in a Pod (if you have more than 1) you can run the following:

kubectl logs [pod-name] --all-containers=true
If you want to get logs for a previously running Pod add the -p flag:

kubectl logs -p [pod-name]
Finally, to stream the logs for a Pod use the -f flag:

kubectl logs -f [pod-name]

You can run the kubectl describe command to see information about the Pod as well as events that have run (look at the bottom of the output for the events). This is really helpful to see if the image for a container was pulled correctly, if the container started in the Pod, any Pod reschedule events, and much more.

kubectl describe pod [pod-name]
In some cases describe events may lead to the discovery that the troubled Pod has been rescheduled frequently by Kubernetes. It’s great that this happens (when setup properly with a Deployment for example), but it’s also good to get to the bottom of “why” a Pod is being rescheduled to determine if there’s a bug in the code that’s running, a memory leak, or another issue.


Viewing the Pod YAML with -o yaml
Finally, you can run kubectl get on a troubled Pod but display the YAML (or JSON) instead of just the basic Pod information. In many scenarios this may yield some useful information.

kubectl get pods [pod-name] -o yaml
You can do the same thing for a specific Deployment as well:

kubectl get deployment [deployment-name] -o yaml
kubectl get documentation

Shelling into a Pod Container with kubectl exec
In some cases you may need to get into a Pod’s container to discover what is wrong. With Docker you would use the docker exec command. Kubernetes is similar:

kubectl exec [pod-name] -it sh


Kubernetes OOM problems
When any Unix based system runs out of memory, OOM safeguard kicks in and kills certain processes based on obscure rules only accessible to level 12 dark sysadmins (chaotic neutral). Kubernetes OOM management tries to avoid the system running behind trigger its own. When the node is low on memory, Kubernetes eviction policy enters the game and stops pods as failed. These pods are scheduled in a different node if they are managed by a ReplicaSet. This frees memory to relieve the memory pressure.



OOM kill due to container limit reached
This is by far the most simple memory error you can have in a pod. You set a memory limit, one container tries to allocate more memory than that allowed,and it gets an error. This usually ends up with a container dying, one pod unhealthy and Kubernetes restarting that pod.

test          frontend        0/1     Terminating         0          9m21s
Describe pods output would show something like this:

   State:          Running
      Started:      Thu, 10 Oct 2019 11:14:13 +0200
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Thu, 10 Oct 2019 11:04:03 +0200
      Finished:     Thu, 10 Oct 2019 11:14:11 +0200
…

Events:
  Type    Reason          Age                    From                                                  Message
  ----    ------          ----                   ----                                                  -------
  Normal  Scheduled       6m39s                  default-scheduler                                     Successfully assigned test/frontend to gke-lab-kube-gke-default-pool-02126501-7nqc
  Normal  SandboxChanged  2m57s                  kubelet, gke-lab-kube-gke-default-pool-02126501-7nqc  Pod sandbox changed, it will be killed and re-created.
  Normal  Killing         2m56s                  kubelet, gke-lab-kube-gke-default-pool-02126501-7nqc  Killing container with id docker://db:Need to kill Pod
The Exit code 137 is important because it means that the system terminated the container as it tried to use more memory than its limit.

In order to monitor this, you always have to look at the use of memory compared to the limit. Percentage of the node memory used by a pod is usually a bad indicator as it gives no indication on how close to the limit the memory usage is. In Kubernetes, limits are applied to containers, not pods, so monitor the memory usage of a container vs. the limit of that container.



Kubernetes OOM kill due to limit overcommit
Memory requested is granted to the containers so they can always use that memory, right? Well, it’s complicated. Kubernetes will not allocate pods that sum to more memory requested than memory available in a node. But limits can be higher than requests, so the sum of all limits can be higher than node capacity. This is called overcommit and it is very common. In practice, if all containers use more memory than requested, it can exhaust the memory in the node. This usually causes the death of some pods in order to free some memory


Memory management in Kubernetes is complex, as it has many facets. Many parameters enter the equation at the same time:

Memory request of the container.
Memory limit of the container.
Lack of those settings.
Free memory in the system.
Memory used by the different containers.
With these parameters, a blender and some maths, Kubernetes elaborates a score. Last in the table is killed or evicted. The pod can be restarted depending on the policy, so that doesn’t mean the pod will be removed entirely.

Despite this mechanism, we can still finish up with system OOM kills as Kubernetes memory management runs only every several seconds. If the system memory fills too quickly, the system can kill Kubernetes control processes, making the node unstable.



This scenario should be avoided as it will probably require a complicated troubleshooting, ending with an RCA based on hypothesis and a node restart.

In day-to-day operation, this means that in case of overcommitting resources, pods without limits will likely be killed, containers using more resources than requested have some chances to die and guaranteed containers will most likely be fine


CPU throttling due to CPU limit
There are many differences on how CPU and memory requests and limits are treated in Kubernetes. A container using more memory than the limit will most likely die, but using CPU can never be the reason of Kubernetes killing a container. CPU management is delegated to the system scheduler, and it uses two different mechanisms for the requests and the limits enforcement.

CPU requests are managed using the shares system. This means that the resources in the CPU are prioritized depending on the value of shares. Each CPU core is divided into 1,024 shares and the resources with more shares have more CPU time reserved. Be careful, in moments of CPU starvation, shares won’t ensure your app has enough resources, as it can be affected by bottlenecks and general collapse.



Tip: If a container requests 100m, the container will have 102 shares. These values are only used for pod allocation. Monitoring the shares in a pod does not give any idea of a problem related to CPU throttling.

On the other hand, limits are treated differently. Limits are managed with the CPU quota system. This works by dividing the CPU time in 100ms periods and assigning a limit on the containers with the same percentage that the limit represents to the total CPU in the node.

If you want to know if your pod is suffering from CPU throttling, you have to look at the percentage of the quota assigned that is being used. Absolute CPU use can be treacherous, as you can see in the following graphs. CPU use of the pod is around 25%, but as that is the quota assigned, it is using 100% and consequently suffering CPU throttling.

sts & containers → Container limits

There is a great difference between CPU and memory quota management. Regarding memory, a pod without requests and limits is considered burstable and is the first of the list to OOM kill. With the CPU, this is not the case. A pod without CPU limits is free to use all the CPU resources in the node. Well, truth is, the CPU is there to be used, but if you can’t control which process is using your resources, you can end up with a lot of problems due to CPU starvation of key processes.


First, there are two reasons why your Pods can fail.
Errors in the configuration of your Kubernetes resources like deployment and services.
Problems in your code.
In the former case, containers do not start. In the latter instance, the application code fails after the container starts up. We’ll address each of these situations systematically


Meanwhile, here are other error codes that occur when a container fails to start.
ImagePullBackoff— Docker image registry not accessible, image name/version specified in deployment incorrect. Make sure the image name is correct, and the registry is accessible and authenticated (docker login…).
RunContainerError— One possible cause: ConfigMap/Secrets missing.
ContainerCreating— Something not available immediately, persistent volume?

Next, here are a few errors that can occur after the container has started.
CrashLoopBackOff — Pod liveness check has failed or the Docker image is faulty. E.g., The Docker CMD is exiting immediately. See tip number three to check logs. Note: The RESTARTS column in the screenshot shows the number of restarts. In this case, you should expect to see some restarts because K8S attempts to start Pods repeatedly when errors occur.
If the Pod is in status Running and your app is still not working correctly, proceed to tips three and four


Tip 2: Check Events Related to Pods
If you see one of the error codes on the Pod status, you can get more information with the describe command. This is helpful in situations where the container itself did not start.
kubectl describe frontend-65c58c957d-f4cqn


Check Your Logs
Now that the container has started, see if the application is functioning correctly by checking logs. E.g. for Pod frontend-65c58c957d-bzbg2:
kubectl logs --tail=10 frontend-65c58c957d-bzbg2

K8S fires events whenever the state of the resources it manages changes (Normal, Warning, etc). They help us understand what happened behind the scenes. The get events command provides an aggregate perspective of events.
# all events sorted by time. 
kubectl get events --sort-by=.metadata.creationTimestamp
# warnings only
kubectl get events --field-selector type=Warning
# events related to Nodes 
kubectl get events --field-selector involvedObject.kind=Node


ext, try the command shown here to grep debug commands.
kubectl | grep -i -A 10 debugging


When you wish to deploy an application in Kubernetes, you usually define three components:

a Deployment — which is a recipe for creating copies of your application called Pods
a Service — an internal load balancer that routes the traffic to Pods
an Ingress — a description of how the traffic should flow from outside the cluster to your Service.


Assuming you wish to deploy a simple Hello World application, the YAML for such application should look similar to this:

hello-world.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deployment
  labels:
    track: canary
spec:
  selector:
    matchLabels:
      any-name: my-app
  template:
    metadata:
      labels:
        any-name: my-app
    spec:
      containers:
      - name: cont1
        image: learnk8s/app:1.0.0
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    name: app
---
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - http:
    paths:
    - backend:
        serviceName: app
        servicePort: 80
      path: /

	  
	  
The surprising news is that Service and Deployment aren't connected at all.

Instead, the Service points to the Pods directly and skips the Deployment altogether.

So what you should pay attention to is how the Pods and the Service are related to each other.

You should remember three things:

The Service selector should match at least one label of the Pod
The Service targetPort should match the containerPort of the container inside the Pod
The Service port can be any number. Multiple Services can use the same port because they have different IP addresses assigned.



What about the track: canary label at the top of the Deployment?

Should that match too?

That label belongs to the deployment, and it's not used by the Service's selector to route traffic.

In other words, you can safely remove it or assign it a different value.

And what about the matchLabels selector?

It always has to match the Pod labels and it's used by the Deployment to track the Pods.

Assuming that you made the correct change, how do you test it?

You can check if the Pods have the right label with the following command:

bash
kubectl get pods --show-labels


You can also connect to the Pod!

You can use the port-forward command in kubectl to connect to the Service and test the connection.

bash
kubectl port-forward service/<service name> 3000:80
Where:

service/<service name> is the name of the service — in the current YAML is "my-service"
3000 is the port that you wish to open on your computer
80 is the port exposed by the Service in the port field
If you can connect, the setup is correctly

The Ingress retrieves the right Service by name and port exposed.

Two things should match in the Ingress and Service:

The servicePort of the Ingress should match the port of the Service
The serviceName of the Ingress should match the name of the Service


Since there are three components in every deployment, you should debug all of them in order, starting from the bottom.

You should make sure that your Pods are running, then
Focus on getting the Service to route traffic to the Pods and then
Check that the Ingress is correctly configured

How do you investigate on what went wrong?

There are four useful commands to troubleshoot Pods:

kubectl logs <pod name> is helpful to retrieve the logs of the containers of the Pod
kubectl describe pod <pod name> is useful to retrieve a list of events associated with the Pod
kubectl get pod <pod name> is useful to extract the YAML definition of the Pod as stored in Kubernetes
kubectl exec -ti <pod name> bash is useful to run an interactive command within one of the containers of the Pod


Common Pods errors
Pods can have startup and runtime errors.

Startup errors include:

ImagePullBackoff
ImageInspectError
ErrImagePull
ErrImageNeverPull
RegistryUnavailable
InvalidImageName
Runtime errors include:

CrashLoopBackOff
RunContainerError
KillContainerError
VerifyNonRootError
RunInitContainerError
CreatePodSandboxError
ConfigPodSandboxError
KillPodSandboxError
SetupNetworkError
TeardownNetworkError
Some errors are more common than others.

The following is a list of the most common error and how you can fix them.

ImagePullBackOff
This error appears when Kubernetes isn't able to retrieve the image for one of the containers of the Pod.

There are three common culprits:

The image name is invalid — as an example, you misspelt the name, or the image does not exist
You specified a non-existing tag for the image
The image that you're trying to retrieve belongs to a private registry, and Kubernetes doesn't have credentials to access it
The first two cases can be solved by correcting the image name and tag.

For the last, you should add the credentials to your private registry in a Secret and reference it in your Pods.

The official documentation has an example about how you could to that.

CrashLoopBackOff
If the container can't start, then Kubernetes shows the CrashLoopBackOff message as a status.

Usually, a container can't start when:

There's an error in the application that prevents it from starting
You misconfigured the container
The Liveness probe failed too many times
You should try and retrieve the logs from that container to investigate why it failed.

If you can't see the logs because your container is restarting too quickly, you can use the following command


kubectl logs <pod-name> --previous
Which prints the error messages from the previous container.


RunContainerError
The error appears when the container is unable to start.

That's even before the application inside the container starts.

The issue is usually due to misconfiguration such as:

mounting a not-existent volume such as ConfigMap or Secrets
mounting a read-only volume as read-write
You should use kubectl describe pod <pod-name> to collect and analyse the error.

Pods in a Pending state
When you create a Pod, the Pod stays in the Pending state.

Why?

Assuming that your scheduler component is running fine, here are the causes:

The cluster doesn't have enough resources such as CPU and memory to run the Pod
The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota
The Pod is bound to a Pending PersistentVolumeClaim
Your best option is to inspect the Events section in the kubectl describe command:

bash
kubectl describe pod <pod name>
For errors that are created as a result of ResourceQuotas, you can inspect the logs of the cluster with:

bash
kubectl get events --sort-by=.metadata.creationTimestamp
Pods in a not Ready state
If a Pod is Running but not Ready it means that the Readiness probe is failing.

When the Readiness probe is failing, the Pod isn't attached to the Service, and no traffic is forwarded to that instance.

A failing Readiness probe is an application-specific error, so you should inspect the Events section in kubectl describe to identify the error.

2. Troubleshooting Services
If your Pods are Running and Ready, but you're still unable to receive a response from your app, you should check if the Service is configured correctly.

Services are designed to route the traffic to Pods based on their labels.

So the first thing that you should check is how many Pods are targeted by the Service.

You can do so by checking the Endpoints in the Service:

bash
kubectl describe service <service-name> | grep Endpoints
An endpoint is a pair of <ip address:port>, and there should be at least one — when the Service targets (at least) a Pod.

If the "Endpoints" section is empty, there are two explanations:

you don't have any Pod running with the correct label (hint: you should check if you are in the right namespace)
You have a typo in the selector labels of the Service
If you see a list of endpoints, but still can't access your application, then the targetPort in your service is the likely culprit.

How do you test the Service?

Regardless of the type of Service, you can use kubectl port-forward to connect to it:

bash
kubectl port-forward service/<service-name> 3000:80
Where:

<service-name> is the name of the Service
3000 is the port that you wish to open on your computer
80 is the port exposed by the Service
3. Troubleshooting Ingress
If you've reached this section, then:

the Pods are Running and Ready
the Service distributes the traffic to the Pod
But you still can't see a response from your app.

It means that most likely, the Ingress is misconfigured.

Since the Ingress controller being used is a third-party component in the cluster, there are different debugging techniques depending on the type of Ingress controller.

But before diving into Ingress specific tools, there's something straightforward that you could check.

The Ingress uses the serviceName and servicePort to connect to the Service.

You should check that those are correctly configured.

You can inspect that the Ingress is correctly configured with:

bash
kubectl describe ingress <ingress-name>
If the Backend column is empty, then there must be an error in the configuration.

If you can see the endpoints in the Backend column, but still can't access the application, the issue is likely to be:

how you exposed your Ingress to the public internet
how you exposed your cluster to the public internet
You can isolate infrastructure issues from Ingress by connecting to the Ingress Pod directly.

First, retrieve the Pod for your Ingress controller (which could be located in a different namespace):

bash
kubectl get pods --all-namespaces
NAMESPACE   NAME                              READY STATUS
kube-system coredns-5644d7b6d9-jn7cq          1/1   Running
kube-system etcd-minikube                     1/1   Running
kube-system kube-apiserver-minikube           1/1   Running
kube-system kube-controller-manager-minikube  1/1   Running
kube-system kube-proxy-zvf2h                  1/1   Running
kube-system kube-scheduler-minikube           1/1   Running
kube-system nginx-ingress-controller-6fc5bcc  1/1   Running
Describe it to retrieve the port:

bash
kubectl describe pod nginx-ingress-controller-6fc5bcc
 --namespace kube-system \
 | grep Ports
Finally, connect to the Pod:

bash
kubectl port-forward nginx-ingress-controller-6fc5bcc 3000:80 --namespace kube-system
At this point, every time you visit port 3000 on your computer, the request is forwarded to port 80 on the Pod.

Does it works now?

If it works, the issue is in the infrastructure. You should investigate how the traffic is routed to your cluster.
If it doesn't work, the problem is in the Ingress controller. You should debug the Ingress.
If you still can't get the Ingress controller to work, you should start debugging it.

There are many different versions of Ingress controllers.

Popular options include Nginx, HAProxy, Traefik, etc.

You should consult the documentation of your Ingress controller to find a troubleshooting guide.

Since Ingress Nginx is the most popular Ingress controller, we included a few tips for it in the next section.

Debugging Ingress Nginx
The Ingress-nginx project has an official plugin for Kubectl.

You can use kubectl ingress-nginx to:

inspect logs, backends, certs, etc.
connect to the Ingress
examine the current configuration
The three commands that you should try are:

kubectl ingress-nginx lint, which checks the nginx.conf
kubectl ingress-nginx backend, to inspect the backend (similar to kubectl describe ingress <ingress-name>)
kubectl ingress-nginx logs, to check the logs
Please notice that you might need to specify the correct namespace for your Ingress controller with --namespace <name>.

Summary
Troubleshooting in Kubernetes can be a daunting task if you don't know where to start.

You should always remember to approach the problem bottom-up: start with the Pods and move up the stack with Service and Ingress.

The same debugging techniques that you learnt in this article can be applied to other objects such as:

failing Jobs and CronJobs
StatefulSets and DaemonSets


#########################################################################################################################

My pod is crashing or otherwise unhealthy
First, take a look at the logs of the current container:

kubectl logs ${POD_NAME} ${CONTAINER_NAME}
If your container has previously crashed, you can access the previous container’s crash log with:

kubectl logs --previous ${POD_NAME} ${CONTAINER_NAME}
Alternately, you can run commands inside that container with exec:

kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN}
Note: -c ${CONTAINER_NAME} is optional. You can omit it for Pods that only contain a single container.
As an example, to look at the logs from a running Cassandra pod, you might run

kubectl exec cassandra -- cat /var/log/cassandra/system.log




The first thing to do is to delete your pod and try creating it again with the --validate option. For example, run kubectl apply --validate -f mypod.yaml. If you misspelled command as commnd then will give an error like this:

I0805 10:43:25.129850   46757 schema.go:126] unknown field: commnd
I0805 10:43:25.129973   46757 schema.go:129] this may be a false alarm, see https://github.com/kubernetes/kubernetes/issues/6842
pods/mypod
The next thing to check is whether the pod on the apiserver matches the pod you meant to create (e.g. in a yaml file on your local machine). For example, run kubectl get pods/mypod -o yaml > mypod-on-apiserver.yaml and then manually compare the original pod description, mypod.yaml with the one you got back from apiserver, mypod-on-apiserver.yaml. There will typically be some lines on the “apiserver” version that are not on the original version. This is expected










